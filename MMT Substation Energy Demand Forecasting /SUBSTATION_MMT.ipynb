{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2stouzipcntz25hbgr6n",
   "authorId": "3366391852320",
   "authorName": "HMASSA",
   "authorEmail": "haley.massa@snowflake.com",
   "sessionId": "94da2dc2-e716-41e9-9399-ec7113951061",
   "lastEditTime": 1761166487510
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7e69ea6e-c0ab-4825-a665-a2fa21579aef",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "# Power Grid Forecasting: Partitioned Models at Scale\n\n## Demo Overview\nThis notebook demonstrates a complete end-to-end ML pipeline for forecasting power production across 5,000 electrical substations using Snowflake's partitioned models capabilities.\n\n### What We'll Build:\n- **Data Simulation**: Generate realistic time-series data for 5,000 substations\n- **Feature Store**: Point-in-time correct features with serving parity\n- **Partitioned Training**: One XGBoost model per substation (5,000 models)\n- **Distributed Inference**: Parallel predictions across all substations\n- **Production Orchestration**: Automated retraining with ML Jobs and Tasks\n- **Monitoring**: Observability and model performance tracking\n\n### Key Benefits:\n- **Scalability**: Train 5,000+ models in parallel\n- **Performance**: All computation stays close to data\n- **Simplicity**: Single notebook, SQL + Python\n- **Production-Ready**: Automated retraining and monitoring"
  },
  {
   "cell_type": "markdown",
   "id": "35db5dc1-f367-4972-9290-50842ace72d6",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "## 1. Environment Setup & DDL\n\nFirst, let's create our database objects and infrastructure."
  },
  {
   "cell_type": "code",
   "id": "ee9a5985-6c35-48f3-9298-dc73a0da04ee",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "-- Create database and schemas\nCREATE DATABASE IF NOT EXISTS POWERGRID_DEMO;\nCREATE SCHEMA IF NOT EXISTS POWERGRID_DEMO.PUBLIC;  -- Main schema for all tables and views\nCREATE SCHEMA IF NOT EXISTS POWERGRID_DEMO.ML;      -- ML-specific objects (stages, procedures)\n\n-- Warehouse for interactive development\nCREATE WAREHOUSE IF NOT EXISTS DEMO_WH\n  WAREHOUSE_SIZE = 'X-LARGE'\n  AUTO_SUSPEND = 300\n  AUTO_RESUME = TRUE\n  INITIALLY_SUSPENDED = TRUE;\n\n-- Stage for Python files and model artifacts\nCREATE OR REPLACE STAGE POWERGRID_DEMO.ML.CODE_STAGE\n  DIRECTORY = (ENABLE = TRUE);\n\n\n-- Compute Pool for ML Jobs\nCREATE COMPUTE POOL IF NOT EXISTS SUBSTATION_MMT_EXAMPLE\n  MIN_NODES = 2\n  MAX_NODES = 20\n  INSTANCE_FAMILY = CPU_X64_S;\n\n-- Set context\nUSE DATABASE POWERGRID_DEMO;\nUSE SCHEMA POWERGRID_DEMO.PUBLIC;\nUSE WAREHOUSE DEMO_WH;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d159e55-dbde-4852-a82d-3faacc67c142",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "### Create Core Data Tables"
  },
  {
   "cell_type": "code",
   "id": "b861b358-a28f-4bde-a148-0874b2dc6d9d",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "-- All tables will be in the PUBLIC schema\n\n-- Training data: Historical production with weather features\nCREATE OR REPLACE TABLE SUBSTATION_PRODUCTION_TRAIN (\n  SUBSTATION_ID       NUMBER,         -- 1..5000\n  TIMESTAMP_HOUR      TIMESTAMP_NTZ,  -- hourly timestamps\n  HIST_PRODUCTION     FLOAT,          -- observed production (target)\n  TEMP_C              FLOAT,          -- temperature in Celsius\n  WIND_MPS            FLOAT,          -- wind speed in m/s\n  HOUR_OF_DAY         NUMBER,         -- 0-23\n  DAY_OF_WEEK         NUMBER,         -- 0-6\n  CREATED_AT          TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Inference input: Future weather conditions (no production label)\nCREATE OR REPLACE TABLE SUBSTATION_PRODUCTION_INFERENCE_INPUT (\n  SUBSTATION_ID       NUMBER,\n  TIMESTAMP_HOUR      TIMESTAMP_NTZ,\n  TEMP_C              FLOAT,\n  WIND_MPS            FLOAT,\n  HOUR_OF_DAY         NUMBER,\n  DAY_OF_WEEK         NUMBER,\n  CREATED_AT          TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Forecast outputs\nCREATE OR REPLACE TABLE SUBSTATION_FORECASTS (\n  SUBSTATION_ID       NUMBER,\n  TIMESTAMP_HOUR      TIMESTAMP_NTZ,\n  PREDICTION          FLOAT,\n  MODEL_VERSION       STRING,\n  CREATED_AT          TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Model performance tracking\nCREATE OR REPLACE TABLE MODEL_PERFORMANCE_METRICS (\n  SUBSTATION_ID       NUMBER,\n  MODEL_VERSION       STRING,\n  METRIC_DATE         DATE,\n  MAE                 FLOAT,\n  RMSE                FLOAT,\n  MAPE                FLOAT,\n  PREDICTION_COUNT    NUMBER,\n  CREATED_AT          TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4674b2eb-9e05-4a57-8ff5-1822a46b7980",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "## 2. Data Simulation\n\nLet's generate realistic time-series data for 5,000 substations with seasonal patterns, weather dependencies, and realistic noise."
  },
  {
   "cell_type": "code",
   "id": "b81164dd-5552-4899-95a2-d084d3e4a850",
   "metadata": {
    "language": "python",
    "name": "Generate_Data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from datetime import datetime, timedelta\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\ndef generate_substation_data_sql(session, n_substations=5000, days_history=365):\n    \"\"\"\n    Generate realistic power production data using pure SQL in Snowflake.\n    MUCH faster than Python loops - executes entirely in Snowflake's engine!\n    \n    Features:\n    - Seasonal patterns (higher production in summer)\n    - Daily cycles (peak around 2 PM)\n    - Weather dependencies  \n    - Individual substation characteristics\n    - Realistic noise\n    \"\"\"\n    print(f\"ðŸš€ Generating {n_substations:,} substations Ã— {days_history} days = {n_substations * days_history * 24:,} rows\")\n    print(f\"   â€¢ Using pure SQL - executes in Snowflake engine (100x faster than Python!)\")\n    \n    end_time = datetime.utcnow().replace(minute=0, second=0, microsecond=0)\n    start_time = end_time - timedelta(days=days_history)\n    \n    # Generate data using PURE SQL - runs in Snowflake, not Python!\n    print(f\"   â€¢ Executing SQL generation...\")\n    \n    session.sql(f\"\"\"\n        CREATE OR REPLACE TABLE SUBSTATION_PRODUCTION_TRAIN AS\n        WITH \n        substations AS (\n            SELECT SEQ4() + 1 AS SUBSTATION_ID\n            FROM TABLE(GENERATOR(ROWCOUNT => {n_substations}))\n        ),\n        hours AS (\n            SELECT DATEADD(hour, SEQ4(), '{start_time}'::TIMESTAMP_NTZ) AS TIMESTAMP_HOUR\n            FROM TABLE(GENERATOR(ROWCOUNT => {days_history * 24}))\n        ),\n        base_data AS (\n            SELECT \n                s.SUBSTATION_ID,\n                h.TIMESTAMP_HOUR,\n                HOUR(h.TIMESTAMP_HOUR) AS HOUR_OF_DAY,\n                DAYOFWEEK(h.TIMESTAMP_HOUR) AS DAY_OF_WEEK,\n                DAYOFYEAR(h.TIMESTAMP_HOUR) AS DAY_OF_YEAR\n            FROM substations s CROSS JOIN hours h\n        )\n        SELECT\n            SUBSTATION_ID,\n            TIMESTAMP_HOUR,\n            -- Realistic power production with seasonal and daily patterns\n            -- Use HASH for deterministic \"randomness\" per substation\n            GREATEST(0, \n                (40 + (HASH(SUBSTATION_ID) % 40 + 20)) *\n                (1 + 0.3 * SIN(2 * PI() * (DAY_OF_YEAR - 80) / 365)) *\n                (1 + 0.4 * SIN(2 * PI() * (HOUR_OF_DAY - 6) / 24)) +\n                (HASH(SUBSTATION_ID, TIMESTAMP_HOUR) % 100 - 50) / 10.0  -- Noise: -5 to +5\n            ) AS HIST_PRODUCTION,\n            -- Weather features with patterns\n            15 + 10 * SIN(2 * PI() * (DAY_OF_YEAR - 80) / 365) + \n                5 * SIN(2 * PI() * HOUR_OF_DAY / 24) + \n                (HASH(TIMESTAMP_HOUR) % 40 - 20) / 10.0 AS TEMP_C,  -- Noise: -2 to +2\n            ABS(5 + 3 * SIN(2 * PI() * DAY_OF_YEAR / 365) + \n                (HASH(DAY_OF_YEAR) % 40 - 20) / 10.0) AS WIND_MPS,  -- Noise: -2 to +2\n            HOUR_OF_DAY,\n            DAY_OF_WEEK\n        FROM base_data\n    \"\"\").collect()\n    \n    row_count = session.table(\"SUBSTATION_PRODUCTION_TRAIN\").count()\n    print(f\"âœ… Generated {row_count:,} rows in Snowflake!\")\n    print(\"   â€¢ Data generation complete!\")\n\n# Generate the training data - FULL YEAR using pure SQL\ngenerate_substation_data_sql(session, n_substations=5000, days_history=365)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90f656b6-bfc5-4fb4-93e2-aa3c6fedc8e3",
   "metadata": {
    "language": "sql",
    "name": "cell11",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select * from SUBSTATION_PRODUCTION_TRAIN\nLIMIT 10",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69c2e0a3-f9db-4c74-a29e-6e0926748108",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "### Generate Future Inference Data"
  },
  {
   "cell_type": "code",
   "id": "496fbd29-cd5c-402e-9c0a-a75891b1b424",
   "metadata": {
    "language": "python",
    "name": "Generate_Inference_Data",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nimport numpy as np\nimport pandas as pd \n\ndef generate_inference_data(session: session, n_substations: int = 5000, hours_ahead: int = 48):\n    \"\"\"Generate future weather conditions for inference (next 48 hours).\"\"\"\n    print(f\"Generating inference data for next {hours_ahead} hours...\")\n    \n    # Future time range\n    start_time = datetime.utcnow().replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)\n    future_hours = [start_time + timedelta(hours=i) for i in range(hours_ahead)]\n    \n    inference_data = []\n    \n    for substation_id in range(1, n_substations + 1):\n        # Use substation-specific seed for consistent weather patterns\n        np.random.seed(substation_id + 10000)\n        \n        # Base weather for this substation (slight regional variation)\n        base_temp = 18 + np.random.uniform(-5, 5)\n        base_wind = 6 + np.random.uniform(-2, 2)\n        \n        for timestamp in future_hours:\n            # Add daily temperature cycle\n            temp_cycle = 4 * np.sin(2 * np.pi * (timestamp.hour - 6) / 24)\n            temp = base_temp + temp_cycle + np.random.normal(0, 1.5)\n            \n            # Wind with some persistence\n            wind = max(0, base_wind + np.random.normal(0, 1.5))\n            \n            inference_data.append({\n                'SUBSTATION_ID': substation_id,\n                'TIMESTAMP_HOUR': timestamp,  # Already a Python datetime\n                'TEMP_C': float(temp),\n                'WIND_MPS': float(wind),\n                'HOUR_OF_DAY': timestamp.hour,\n                'DAY_OF_WEEK': timestamp.weekday()\n            })\n    \n    # Write to Snowflake\n    df_inference = pd.DataFrame(inference_data)\n    \n    # Ensure proper data types\n    df_inference['SUBSTATION_ID'] = df_inference['SUBSTATION_ID'].astype('int64')\n    df_inference['TEMP_C'] = df_inference['TEMP_C'].astype('float64')\n    df_inference['WIND_MPS'] = df_inference['WIND_MPS'].astype('float64')\n    df_inference['HOUR_OF_DAY'] = df_inference['HOUR_OF_DAY'].astype('int64')\n    df_inference['DAY_OF_WEEK'] = df_inference['DAY_OF_WEEK'].astype('int64')\n    \n    # Use Snowpark DataFrame for better type handling\n    snowpark_df = session.create_dataframe(df_inference)\n    snowpark_df.write.mode(\"overwrite\").save_as_table(\"SUBSTATION_PRODUCTION_INFERENCE_INPUT\")\n    \n    print(f\"Generated {len(inference_data)} inference records.\")\n    return df_inference\n\n# Generate inference data\ninference_data = generate_inference_data(session)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cffcb292-ef59-40b4-9f1c-0f725a50db41",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "## 3. Feature Store Implementation\n\nBuild a production-ready feature store with point-in-time correctness and serving parity.\n"
  },
  {
   "cell_type": "code",
   "id": "ab460ab1-ebb6-490a-83e3-ea8e1ad44331",
   "metadata": {
    "language": "python",
    "name": "Implement_Feature_Store",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import (\n    FeatureStore,\n    FeatureView,\n    Entity,\n    CreationMode\n)\nfrom snowflake.snowpark import DataFrame\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.window import Window\nfrom datetime import timedelta\nimport warnings\n\n# Suppress the online_config private preview warning\n# We're using batch/offline features only, not online serving\nwarnings.filterwarnings('ignore', message='.*online_config.*')\n\n# Initialize Feature Store\n# The feature store will be created if it doesn't exist\nfs = FeatureStore(\n    session=session,\n    database=\"POWERGRID_DEMO\",\n    name=\"SUBSTATION_FEATURE_STORE\",\n        default_warehouse=\"DEMO_WH\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\nprint(\"âœ… Feature Store initialized\")\nprint(\"   NOTE: Using BATCH/OFFLINE features only (no online serving)\")\n\n# Define the Substation Entity\nsubstation_entity = Entity(\n    name=\"SUBSTATION\",\n    join_keys=[\"SUBSTATION_ID\"],\n    desc=\"Electrical substation entity for power grid forecasting\"\n)\n\n# Register the entity\nfs.register_entity(substation_entity)\nprint(\"âœ… Entity 'SUBSTATION' registered\")\n\n# Create source data view for feature engineering\n# Note: Fully qualify the table name to reference the PUBLIC schema\nsession.sql(\"\"\"\nCREATE OR REPLACE VIEW POWERGRID_DEMO.PUBLIC.SUBSTATION_SOURCE_DATA AS\nSELECT\n    SUBSTATION_ID,\n    TIMESTAMP_HOUR,\n    HIST_PRODUCTION,\n    TEMP_C,\n    WIND_MPS,\n    HOUR_OF_DAY,\n    DAY_OF_WEEK,\n    -- Derived time features\n    CASE WHEN HOUR_OF_DAY BETWEEN 6 AND 18 THEN 1 ELSE 0 END as IS_DAYTIME,\n    CASE WHEN DAY_OF_WEEK IN (5, 6) THEN 1 ELSE 0 END as IS_WEEKEND,\n    EXTRACT(MONTH FROM TIMESTAMP_HOUR) as MONTH,\n    -- Weather interaction features\n    TEMP_C * WIND_MPS as TEMP_WIND_INTERACTION,\n    CASE WHEN TEMP_C > 25 THEN 1 ELSE 0 END as IS_HOT,\n    CASE WHEN WIND_MPS > 8 THEN 1 ELSE 0 END as IS_WINDY\nFROM POWERGRID_DEMO.PUBLIC.SUBSTATION_PRODUCTION_TRAIN\n\"\"\").collect()\n\nprint(\"âœ… Source data view created\")\n\n# Define Feature View with time-series features\n# Note: Feature Views in Snowflake work differently - we'll create a managed approach\nprint(\"ðŸ“Š Creating Feature View with time-series features...\")\n\n# Get source data as Snowpark DataFrame (from PUBLIC schema)\nsource_df = session.table(\"POWERGRID_DEMO.PUBLIC.SUBSTATION_SOURCE_DATA\")\n\n# Define window specs for partitioned calculations\nwindow_by_substation = Window.partition_by(\"SUBSTATION_ID\").order_by(\"TIMESTAMP_HOUR\")\n\n# Define rolling windows (using rows between to ensure PIT correctness)\nwindow_24h_past = Window.partition_by(\"SUBSTATION_ID\").order_by(\"TIMESTAMP_HOUR\").rows_between(-24, -1)\nwindow_168h_past = Window.partition_by(\"SUBSTATION_ID\").order_by(\"TIMESTAMP_HOUR\").rows_between(-168, -1)\n\n# Create the feature DataFrame with all transformations\nfeatures_df = source_df.select(\n    F.col(\"SUBSTATION_ID\"),\n    F.col(\"TIMESTAMP_HOUR\"),\n    F.col(\"HIST_PRODUCTION\"),\n    F.col(\"TEMP_C\"),\n    F.col(\"WIND_MPS\"),\n    F.col(\"HOUR_OF_DAY\"),\n    F.col(\"DAY_OF_WEEK\"),\n    F.col(\"IS_DAYTIME\"),\n    F.col(\"IS_WEEKEND\"),\n    F.col(\"MONTH\"),\n    F.col(\"TEMP_WIND_INTERACTION\"),\n    F.col(\"IS_HOT\"),\n    F.col(\"IS_WINDY\"),\n    # Lag features\n    F.lag(\"HIST_PRODUCTION\", 1).over(window_by_substation).alias(\"LAG_1H\"),\n    F.lag(\"HIST_PRODUCTION\", 24).over(window_by_substation).alias(\"LAG_24H\"),\n    F.lag(\"HIST_PRODUCTION\", 168).over(window_by_substation).alias(\"LAG_1W\"),\n    # Rolling statistics (past 24 hours)\n    F.avg(\"HIST_PRODUCTION\").over(window_24h_past).alias(\"ROLL_MEAN_24H\"),\n    F.stddev(\"HIST_PRODUCTION\").over(window_24h_past).alias(\"ROLL_STD_24H\"),\n    F.max(\"HIST_PRODUCTION\").over(window_24h_past).alias(\"ROLL_MAX_24H\"),\n    F.min(\"HIST_PRODUCTION\").over(window_24h_past).alias(\"ROLL_MIN_24H\"),\n    # Weekly rolling mean\n    F.avg(\"HIST_PRODUCTION\").over(window_168h_past).alias(\"ROLL_MEAN_1W\")\n)\n\n# Calculate trend (difference between lag 1 and lag 25)\nfeatures_df = features_df.with_column(\n    \"TREND_24H\",\n    F.col(\"LAG_1H\") - F.lag(\"HIST_PRODUCTION\", 25).over(window_by_substation)\n)\n\n# Register as a Feature View in the Feature Store\nfeature_view = fs.register_feature_view(\n    feature_view=FeatureView(\n        name=\"SUBSTATION_TIME_SERIES_FEATURES\",\n        entities=[substation_entity],\n        feature_df=features_df,\n        timestamp_col=\"TIMESTAMP_HOUR\",\n        desc=\"Time-series features for substation power production forecasting\"\n    ),\n    version=\"V1\"\n)\n\nprint(\"âœ… Feature View 'SUBSTATION_TIME_SERIES_FEATURES' registered\")\n\nprint(\"âœ… Feature View registered\")\n\n# Generate training dataset from Feature Store\nprint(\"\\nðŸ“Š Generating training dataset from Feature Store...\")\nfrom datetime import datetime, timedelta\n\nend_date = datetime.utcnow()\nstart_date = end_date - timedelta(days=365)\n\n# Create spine (all substation-timestamp combinations for training)\nspine_df = session.table(\"POWERGRID_DEMO.PUBLIC.SUBSTATION_PRODUCTION_TRAIN\") \\\n    .filter((F.col(\"TIMESTAMP_HOUR\") >= start_date) & (F.col(\"TIMESTAMP_HOUR\") <= end_date)) \\\n    .select(F.col(\"SUBSTATION_ID\"), F.col(\"TIMESTAMP_HOUR\"))\n\n# Generate dataset from Feature Store\ntraining_dataset = fs.generate_dataset(\n    spine_df=spine_df,\n    features=[feature_view],\n    name=\"SUBSTATION_TRAINING_DATASET\",\n    spine_timestamp_col=\"TIMESTAMP_HOUR\",\n    spine_label_cols=[\"HIST_PRODUCTION\"]\n)\n\n# Materialize for ManyModelTraining (needs simple table)\nprint(\"   â€¢ Materializing dataset...\")\ntraining_dataset.read.to_snowpark_dataframe().write.mode(\"overwrite\").save_as_table(\"SUBSTATION_TRAINING_DATA\")\n\n# Read with SQL (not session.table) - forces fresh query plan\ntraining_df = session.sql(\"SELECT * FROM SUBSTATION_TRAINING_DATA\")\n\nprint(f\"âœ… Feature Store ready!\")\nprint(f\"   â€¢ Entity: SUBSTATION\")\nprint(f\"   â€¢ Feature View: SUBSTATION_TIME_SERIES_FEATURES\")\nprint(f\"   â€¢ Training Dataset: SUBSTATION_TRAINING_DATASET\")\nprint(f\"   â€¢ Period: {start_date.date()} to {end_date.date()}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8f0fca6-892a-4930-9135-38e5c400e440",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "training_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f5d5be8-8e36-4a42-98b6-2531b08372aa",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "## 4. Partitioned Model Training\n\nTrain one XGBoost model per substation using Snowflake's partitioned model capabilities."
  },
  {
   "cell_type": "code",
   "id": "28198ad6-02a1-4a10-8424-881dfcffa7da",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom datetime import datetime\nimport time\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\n# Training function (following Snowflake docs pattern)\ndef train_substation_model(data_connector, context):\n    \"\"\"Train XGBoost model for one substation partition.\"\"\"\n\n    partition_id = context.partition_id\n    assert partition_id is not None\n    \n    # Load partitioned data.\n    pandas_df: pd.DataFrame = data_connector.to_pandas()\n    print(f\"Training model for partition: {partition_id}\")\n    \n    # Define Feature and Target Columns \n    \n    \n    FEATURE_COLS = [\n    'TEMP_C', 'WIND_MPS', 'HOUR_OF_DAY', 'DAY_OF_WEEK',\n    'IS_DAYTIME', 'IS_WEEKEND', 'MONTH',\n    'TEMP_WIND_INTERACTION', 'IS_HOT', 'IS_WINDY',\n    'LAG_1H', 'LAG_24H', 'LAG_1W',\n    'ROLL_MEAN_24H', 'ROLL_STD_24H', 'ROLL_MAX_24H', 'ROLL_MIN_24H',\n    'ROLL_MEAN_1W', 'TREND_24H'\n]\n\n    TARGET_COL = 'HIST_PRODUCTION'\n    \n    \n    # Prepare features and target\n    X = pandas_df[FEATURE_COLS]\n    y = pandas_df[TARGET_COL]\n    \n    # Train XGBoost\n    model = XGBRegressor(\n        n_estimators=50,\n        max_depth=6,\n        learning_rate=0.05,\n        random_state=42\n    )\n    model.fit(X, y)\n\n    # Evaluate on training data\n    preds = model.predict(pandas_df[FEATURE_COLS])\n    mse = mean_squared_error(pandas_df[TARGET_COL], preds)\n    r2 = r2_score(pandas_df[TARGET_COL], preds)\n    \n    print(f\"Model trained for {partition_id}\")\n    return model",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd5c8a26-00f9-4639-8df3-aa6f0fc1765f",
   "metadata": {
    "language": "python",
    "name": "multi_node_training_enabled",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Optional step to scale to multiple nodes for speed up overall many model trainings.\nfrom snowflake.ml.runtime_cluster import cluster_manager\nTOTAL_NODES=20\ncluster_manager.scale_cluster(expected_cluster_size=TOTAL_NODES, notebook_name=\"SUBSTATION_MMT\", options={\"block_until_min_cluster_size\": 2})",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e20237e-8313-4792-88fd-90cac88bad06",
   "metadata": {
    "language": "python",
    "name": "MMT"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.distributors.many_model import ManyModelTraining\nfrom snowflake.ml.modeling.distributors.distributed_partition_function.entities import (\n    ExecutionOptions,\n    RunStatus,\n)\n\ntrainer = ManyModelTraining(\n     train_substation_model,    \n    stage_name=\"@POWERGRID_DEMO.ML.ML_STAGE\",    \n)\n\nrun_id=\"my_mmt_model_v1\"\ntraining_run = trainer.run(\n    snowpark_dataframe=training_df,\n    partition_by=\"SUBSTATION_ID\",\n    run_id=run_id,\n    on_existing_artifacts=\"overwrite\", # or \"error\"\n    # execution_options is optional. When running in a multi-node setting, it's recommended setting use_head_node=False to exclude head node from doing actual training, this improves overall MMT training reliability.\n    # execution_options=ExecutionOptions(use_head_node=False)\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "471a5169-8d51-4eed-b39f-dda0c0757155",
   "metadata": {
    "language": "python",
    "name": "cell37",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "## Value filters for test \n\n#from snowflake.snowpark.functions import col\n#values_to_filter = [1,2,3,4,5]\n\n#training_df = training_df.filter(col(\"SUBSTATION_ID\").in_(values_to_filter))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e80ad51c-7195-48b6-b0b3-6b914fd12e7f",
   "metadata": {
    "language": "python",
    "name": "get_progress"
   },
   "outputs": [],
   "source": "training_run.get_progress()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "streamlit_watch_run",
    "codeCollapsed": false
   },
   "source": "assert training_run.wait() == RunStatus.SUCCESS",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "589e05f7-b74a-43f3-a972-7f7134a7a177",
   "metadata": {
    "language": "python",
    "name": "cancel_job",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Run if you want to cancel the training job \n#training_run.cancel()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1ef39b8-1dab-4ee1-8298-8eb596499adb",
   "metadata": {
    "language": "python",
    "name": "cell25",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "training_run.get_progress()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f515e75e-4bc2-4831-80a7-b34e5abd7908",
   "metadata": {
    "language": "python",
    "name": "Done_Cells",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "training_run.get_progress()[\"DONE\"][0].logs # inspect result\n\n# To inspect failures\n# training_run.get_progress()[\"FAILED\"][0].logs ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "204abbce-bb9a-4d15-b914-50ced3dd6664",
   "metadata": {
    "language": "python",
    "name": "Failed_Logs",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# To inspect failures\ntraining_run.get_progress()[\"FAILED\"][0].logs ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bbb6365-7493-485d-b01d-2fe7461887cb",
   "metadata": {
    "language": "python",
    "name": "View_Models",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# To obtain models trained with each partition\nimport xgboost as xgb\nfor partition_id in training_run.partition_details.keys():\n    model = training_run.get_model(partition_id)\n    assert isinstance(model, xgb.XGBRegressor)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "694c08af-0b4e-447d-843f-033a98dc6816",
   "metadata": {
    "name": "cell24",
    "collapsed": false
   },
   "source": "## Step 3: Running Inference on Trained Models"
  },
  {
   "cell_type": "markdown",
   "id": "b8d331dc-69c5-476e-b63f-fcefb11174ae",
   "metadata": {
    "name": "cell27",
    "collapsed": false
   },
   "source": "### Step 3.1: Register Models in the Snowflake Model Registry and Run Inference (Warehouse Execution) â€” GA Feature"
  },
  {
   "cell_type": "code",
   "id": "935c5176-2d2c-4864-afb6-2f52e6fa01f9",
   "metadata": {
    "language": "python",
    "name": "get_models"
   },
   "outputs": [],
   "source": "#models = {\n#    partition_id: training_run.get_model(partition_id)\n#    for partition_id in training_run.partition_details\n#}",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09edb450-f509-4488-ac1e-89ff0443e751",
   "metadata": {
    "language": "python",
    "name": "log_models",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# from typing import Optional\n# from snowflake.ml.model import custom_model\n# from snowflake.ml.registry import registry\n# import pandas as pd\n\n\n# # Log model to model registry\n# class PartitionedModel(custom_model.CustomModel):\n#     def __init__(self, context: Optional[custom_model.ModelContext] = None) -> None:\n#         super().__init__(context)\n#         self.partition_id = None\n#         self.model = None\n\n#     @custom_model.partitioned_api\n#     def predict(self, input: pd.DataFrame) -> pd.DataFrame:\n#         FEATURE_COLS = [\n#     'TEMP_C', 'WIND_MPS', 'HOUR_OF_DAY', 'DAY_OF_WEEK',\n#     'IS_DAYTIME', 'IS_WEEKEND', 'MONTH',\n#     'TEMP_WIND_INTERACTION', 'IS_HOT', 'IS_WINDY',\n#     'LAG_1H', 'LAG_24H', 'LAG_1W',\n#     'ROLL_MEAN_24H', 'ROLL_STD_24H', 'ROLL_MAX_24H', 'ROLL_MIN_24H',\n#     'ROLL_MEAN_1W', 'TREND_24H'\n# ]\n\n#         TARGET_COL = 'HIST_PRODUCTION'\n\n#         model_id = str(input[\"SUBSTATION_ID\"][0])\n#         model = self.context.model_ref(model_id)\n\n#         model_output = model.predict(input[FEATURE_COLS])\n#         res = pd.DataFrame(model_output)\n#         return res",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6b6e884-83ad-4c3b-8079-4a3d2c6d5eae",
   "metadata": {
    "language": "python",
    "name": "cell29",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# from snowflake.ml.model import custom_model\n\n# # Models have been fit, and they can now be retrieved and registered to the model registry.\n# model_context = custom_model.ModelContext(\n#     models=models\n# )\n\n# my_stateful_model = PartitionedModel(context=model_context)\n# reg = registry.Registry(session=session)\n# options = {\n#     \"function_type\": \"TABLE_FUNCTION\",\n#     \"relax_version\": False\n# }\n# FEATURE_COLS = [\n#     'TEMP_C', 'WIND_MPS', 'HOUR_OF_DAY', 'DAY_OF_WEEK',\n#     'IS_DAYTIME', 'IS_WEEKEND', 'MONTH',\n#     'TEMP_WIND_INTERACTION', 'IS_HOT', 'IS_WINDY',\n#     'LAG_1H', 'LAG_24H', 'LAG_1W',\n#     'ROLL_MEAN_24H', 'ROLL_STD_24H', 'ROLL_MAX_24H', 'ROLL_MIN_24H',\n#     'ROLL_MEAN_1W', 'TREND_24H'\n# ]\n\n# TARGET_COL = 'HIST_PRODUCTION'\n\n# import snowflake.snowpark.functions as F\n# from functools import reduce\n\n# cols = FEATURE_COLS + [\"SUBSTATION_ID\"]\n# cond = reduce(lambda a, b: a & F.col(b).is_not_null(), cols[1:], F.col(cols[0]).is_not_null())\n\n# ## want sample data with no nulls\n# sample_input_data = (\n#     training_df\n#     .select(cols)\n#     .filter(cond)      # keep only rows with no nulls in all required columns\n#     .limit(1)\n#     .to_pandas()\n# )\n# mv = reg.log_model(\n#     my_stateful_model,\n#     model_name=\"partitioned_model\",\n#     options=options,\n#     conda_dependencies=[\"pandas\", \"xgboost\"],\n#     sample_input_data=sample_input_data,    \n# )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be5bb0e5-57f9-4ffd-9c93-4aa3bbfb005f",
   "metadata": {
    "language": "python",
    "name": "cell28",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# service_prediction = mv.run(\n#     training_df,\n#     partition_column=\"SUBSTATION_ID\",\n# )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7997d26e-6da2-462b-ac22-bbb1f38f74de",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "### Step 3.2: Alternative ManyModelInference Method (Container Execution) â€” Preview Feature\n\nBest for scaling training or inference across thousands of partitions, or when incorporating custom Python dependencies. Container execution gives us the flexibility to bring and scale out any enviornment to Snowflake. "
  },
  {
   "cell_type": "code",
   "id": "942cf265-5613-46a4-8605-717954817263",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.distributors.many_model import ManyModelInference\nfrom snowflake.ml.data import DataConnector\nfrom snowflake.ml.modeling.distributors.distributed_partition_function.partition_context import (\n    PartitionContext,\n)\n\ndef xgb_inference_func(data_connector: DataConnector, model, context: PartitionContext):\n    \"\"\"Simple inference function.\"\"\"\n    df = data_connector.to_pandas()\n    FEATURE_COLS = [\n    'TEMP_C', 'WIND_MPS', 'HOUR_OF_DAY', 'DAY_OF_WEEK',\n    'IS_DAYTIME', 'IS_WEEKEND', 'MONTH',\n    'TEMP_WIND_INTERACTION', 'IS_HOT', 'IS_WINDY',\n    'LAG_1H', 'LAG_24H', 'LAG_1W',\n    'ROLL_MEAN_24H', 'ROLL_STD_24H', 'ROLL_MAX_24H', 'ROLL_MIN_24H',\n    'ROLL_MEAN_1W', 'TREND_24H'\n]\n    X = df[FEATURE_COLS].values\n    predictions = model.predict(X)\n\n    # Write prediction results to persistent storage\n    results = df.copy()\n    results['predictions'] = predictions\n    \n    # Two persistence strategies (choose one or both based on your needs):\n\n    # Strategy 1: Stage artifacts - for framework management and debugging\n    # context.upload_to_stage(results, \"predictions.csv\",\n    #     write_function=lambda df, path: df.to_csv(path, index=False))\n\n    # Strategy 2: Snowflake table - for immediate downstream consumption\n    predictions_df = context.session.create_dataframe(results)\n    predictions_df.write.mode(\"append\").save_as_table(\"sales_predictions\")\n    \n    return predictions\n\nmmi = ManyModelInference(\n    inference_func=xgb_inference_func,\n    stage_name=\"@POWERGRID_DEMO.ML.ML_STAGE\",\n    training_run_id=run_id, # run_id from previous many model training run at step 2\n)\n\nfrom snowflake.snowpark.functions import col\nvalues_to_filter = [1,2,3,4,5]\n\ntraining_df = training_df.filter(col(\"SUBSTATION_ID\").in_(values_to_filter))\n\n\ninference_run = mmi.run(\n    partition_by=\"SUBSTATION_ID\",\n    snowpark_dataframe=training_df, # running inference on the same training data mainly for illustration purposes.\n    run_id=\"basic_inference_run\",\n    on_existing_artifacts=\"overwrite\",\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "894aee37-3ebf-4248-8d0a-d6247b18b8a9",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#inference_run.cancel()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f06b2c5-9adf-4046-93df-04bbf73c98f0",
   "metadata": {
    "language": "python",
    "name": "cell33",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "assert inference_run.wait() == RunStatus.SUCCESS",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fa71e7c-b84f-438e-956c-ed6207a06cb8",
   "metadata": {
    "language": "python",
    "name": "cell34",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "inference_run.get_progress()\n# inference_run.get_progress()[\"FAILED\"][0].logs",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6eac389-9bec-4d99-90eb-2ef02e3557c4",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "source": "## Step 4: Troubleshooting Failed Runs\n\nTraining functions can fail for various reasons. Below are some common causes:\n\n- **User Code Errors**  \n  Bugs or issues in the user-defined training function can cause failures.\n\n- **Infrastructure Issues**  \n  An *Out-of-Memory (OOM)* error occurs when the training function consumes more memory than the node can provide.\n\n- **Unexpected Node Failures**  \n  In some cases, a node might crash unexpectedly.\n\n---\n\n### Handling OOM and Node Failures\n\nWhen an OOM error or fatal node failure occurs, the **MMT API will not automatically retry** the training function. Instead, it will mark the corresponding partition ID run as **`INTERNAL_ERROR`**. If a worker node crashes, logs might not be captured, making debugging more difficult.\n\nFor all other failure scenarios (including OOM errors), MMT provides:\n- A **detailed error message**  \n- A **stack trace** to help diagnose and fix the issue\n\n---\n\n### Retry Logic for Non-Fatal Errors\n\nIf the failure is not considered fatal (e.g., transient issues), MMT will automatically retry the training function with **exponential backoff**. This mechanism allows transient issues to resolve before the function ultimately fails.\n\n**Retry Attempts:**\n1. **First retry**: Wait for 2 seconds (`initial_delay`)\n2. **Second retry**: Wait for 4 seconds (2 * `initial_delay`)\n3. **Third retry**: Wait for 8 seconds (2^2 * `initial_delay`)\n4. **Fourth retry**: Wait for 16 seconds (2^3 * `initial_delay`)\n5. **Final retry**: No delay â€” if it fails again, an exception is raised\n"
  },
  {
   "cell_type": "code",
   "id": "753eb65c-c789-470f-be0d-af97a6a21657",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "def user_func_error(data_connector: DataConnector, context: PartitionContext):\n    pandas_df = data_connector.to_pandas()\n\n    FEATURE_COLS = [\n    'TEMP_C', 'WIND_MPS', 'HOUR_OF_DAY', 'DAY_OF_WEEK',\n    'IS_DAYTIME', 'IS_WEEKEND', 'MONTH',\n    'TEMP_WIND_INTERACTION', 'IS_HOT', 'IS_WINDY',\n    'LAG_1H', 'LAG_24H', 'LAG_1W',\n    'ROLL_MEAN_24H', 'ROLL_STD_24H', 'ROLL_MAX_24H', 'ROLL_MIN_24H',\n    'ROLL_MEAN_1W', 'TREND_24H'\n]\n\n    TARGET_COL = 'HIST_PRODUCTION'\n    model = xgb.XGBRegressor()\n\n    # INTENTIONAL USER-CODE FAILURE: fitss function does not exist\n    model.fitss(pandas_df[FEATURE_COLS], pandas_df[TARGET_COL])    \n    \n    return model\n\n\nmodel_name=\"my_mmt_model\"\nmodel_version = \"v2\"\nrun_id=f\"{model_name}_{model_version}\"\n\ntrainer = ManyModelTraining(\n    user_func_error,    \n    stage_name=\"@POWERGRID_DEMO.ML.ML_STAGE\",\n)\n\nfailed_trainer_run = trainer.run(\n    snowpark_dataframe=training_df,\n    partition_by=\"SUBSTATION_ID\",    \n    run_id=run_id,\n    on_existing_artifacts=\"overwrite\", # or \"error\"\n)\n",
   "execution_count": null
  }
 ]
}