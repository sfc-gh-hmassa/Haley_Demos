{
 "metadata": {
  "kernelspec": {
   "display_name": "Snowflake",
   "language": "sql",
   "name": "snowflake"
  },
  "language_info": {
   "codemirror_mode": "sql",
   "file_extension": ".sql",
   "mimetype": "application/x-sql",
   "name": "sql"
  },
  "lastEditStatus": {
   "notebookId": "2tmfmuhlwozbboy2pesm",
   "authorId": "3366391852320",
   "authorName": "HMASSA",
   "authorEmail": "haley.massa@snowflake.com",
   "sessionId": "3e9168af-b899-4acb-9768-b64553ce6366",
   "lastEditTime": 1755223361373
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70163286-6eb1-4dea-89dd-c9752fc44c74",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Enterprise Work Order Batch Processing Pipeline\n",
    "\n",
    "This notebook demonstrates an **automated batch processing pipeline** that processes high-value work orders using:\n",
    "- **Snowflake Streams** to capture work order changes\n",
    "- **Snowflake Tasks** for automated batch processing weekly\n",
    "- **Cortex AI** for intelligent summarization\n",
    "\n",
    "## 🎯 Business Focus\n",
    "Only work orders **≥ $25,000** trigger AI summarization for executive attention.\n",
    "\n",
    "**✅ TESTED AND WORKING** - This pipeline has been validated end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459dd65e-1ff3-4ab6-bc17-ac72c28c0f16",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Ensure your role has the necessary privileges:\n",
    "\n",
    "```sql\n",
    "GRANT EXECUTE TASK ON ACCOUNT TO ROLE <YOUR_ROLE>;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d3769-eadd-43fe-b847-8f1af80bd92c",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "-- Set up the environment\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "USE DATABASE DEMODB;\n",
    "USE SCHEMA WORK_ORDER_MANAGEMENT;\n",
    "\n",
    "-- 🎯 BUSINESS CONFIGURATION\n",
    "-- Pipeline processes work orders >= $25,000\n",
    "-- Threshold is hardcoded in stored procedure for reliability\n",
    "\n",
    "SELECT \n",
    "    'PIPELINE CONFIGURATION' as STATUS,\n",
    "    'High-value threshold: $25,000' as COST_THRESHOLD,\n",
    "    'Batch processing: Weekly on Mondays at 9 AM EST' as SCHEDULE,\n",
    "    'Active statuses: Open, In Progress, Pending Approval' as STATUSES;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e5c8d-9919-4339-a5e5-75f42d8aa1e1",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": [
    "## Step 1: Create Stream\n",
    "\n",
    "Create a Snowflake Stream to capture changes to the `ENTERPRISE_WORK_ORDERS` table. The stream provides an audit trail of all changes, while business filtering for batch processing happens in the stored procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1df8fc-e600-4e05-9d63-c246e6f1597f",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "-- Create stream to monitor work orders table\n",
    "CREATE OR REPLACE STREAM ENTERPRISE_WORK_ORDERS_STREAM\n",
    "ON TABLE ENTERPRISE_WORK_ORDERS\n",
    "COMMENT = 'Stream to capture new work orders for batch processing';\n",
    "\n",
    "-- Verify stream creation\n",
    "SELECT \n",
    "    'Stream created successfully' AS STATUS,\n",
    "    SYSTEM$STREAM_HAS_DATA('ENTERPRISE_WORK_ORDERS_STREAM') AS HAS_DATA,\n",
    "    CURRENT_TIMESTAMP() AS CREATED_AT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa9ab91-b907-48cc-b206-e977af3e4699",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## Step 2: Create Processing Stored Procedure\n",
    "\n",
    "**Key Business Logic:**\n",
    "- Only processes NEW work orders (INSERT operations)\n",
    "- Cost threshold: **≥ $25,000** (hardcoded for task reliability)\n",
    "- Status filter: Active work orders only\n",
    "- Duplicate prevention: Won't reprocess existing summaries\n",
    "\n",
    "**✅ TESTED AND WORKING** - This is the validated version that successfully processes high-value work orders in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c8357-20cf-4e5b-a767-320d45d09743",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE PROCEDURE PROCESS_HIGH_COST_WORK_ORDERS()\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "EXECUTE AS CALLER\n",
    "AS\n",
    "$$\n",
    "DECLARE\n",
    "    min_cost_threshold NUMBER DEFAULT 25000;  -- Hardcoded for task reliability\n",
    "    result_message STRING;\n",
    "BEGIN\n",
    "    -- Process new high-cost work orders from stream in batch\n",
    "    INSERT INTO WORK_ORDER_SUMMARIES (WORK_ORDER_ID, SUMMARY, CREATED_TIMESTAMP)\n",
    "    WITH new_high_cost_orders AS (\n",
    "        -- 🎯 BUSINESS LOGIC APPLIED HERE:\n",
    "        -- 1. Only INSERT operations (new work orders)\n",
    "        -- 2. Cost >= $25,000 (hardcoded threshold)\n",
    "        -- 3. Active status only (Open, In Progress, Pending Approval)\n",
    "        -- 4. No existing summary (avoid duplicates)\n",
    "        SELECT \n",
    "            s.WORK_ORDER_ID,\n",
    "            s.WORK_ORDER_NOTES\n",
    "        FROM ENTERPRISE_WORK_ORDERS_STREAM s\n",
    "        WHERE s.METADATA$ACTION = 'INSERT'  -- Only new inserts\n",
    "          AND s.TOTAL_COST >= :min_cost_threshold  -- HIGH-VALUE THRESHOLD\n",
    "          AND s.STATUS IN ('Open', 'In Progress', 'Pending Approval')  -- ACTIVE STATUSES ONLY\n",
    "          -- Ensure we don't already have a summary\n",
    "          AND NOT EXISTS (\n",
    "              SELECT 1 FROM WORK_ORDER_SUMMARIES ws \n",
    "              WHERE ws.WORK_ORDER_ID = s.WORK_ORDER_ID\n",
    "          )\n",
    "    ),\n",
    "    ai_summaries AS (\n",
    "        -- Generate AI summaries for qualifying work orders\n",
    "        SELECT \n",
    "            n.WORK_ORDER_ID,\n",
    "            SNOWFLAKE.CORTEX.COMPLETE(\n",
    "                'claude-3-5-sonnet',\n",
    "                ARRAY_CONSTRUCT(\n",
    "                    OBJECT_CONSTRUCT('role', 'user', 'content', 'Summarize these technician notes: ' || n.WORK_ORDER_NOTES)\n",
    "                ),\n",
    "                OBJECT_CONSTRUCT('max_tokens', 256)\n",
    "            ):choices[0]:messages::STRING AS ai_summary\n",
    "        FROM new_high_cost_orders n\n",
    "    )\n",
    "    SELECT \n",
    "        s.WORK_ORDER_ID,\n",
    "        s.ai_summary,\n",
    "        CURRENT_TIMESTAMP()\n",
    "    FROM ai_summaries s\n",
    "    WHERE s.ai_summary IS NOT NULL;\n",
    "\n",
    "    -- Create result message\n",
    "    result_message := 'Processed high-cost work orders from stream. Cost threshold: $' || min_cost_threshold;\n",
    "\n",
    "    RETURN result_message;\n",
    "\n",
    "END;\n",
    "$$;\n",
    "\n",
    "SELECT 'Stored procedure created successfully' AS STATUS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac2d1e-7178-457c-9715-7f59a49f9025",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "## 📋 Stored Procedure Logic Explained\n",
    "\n",
    "The `PROCESS_HIGH_COST_WORK_ORDERS()` procedure implements a **three-stage filtering and processing pattern** that you can adapt for similar use cases:\n",
    "\n",
    "### **Stage 1: Business Rule Filtering (`new_high_cost_orders` CTE)**\n",
    "```sql\n",
    "-- Applies 4 key business filters:\n",
    "WHERE s.METADATA$ACTION = 'INSERT'           -- Only new records\n",
    "  AND s.TOTAL_COST >= :min_cost_threshold    -- Cost threshold ($25K)\n",
    "  AND s.STATUS IN ('Open', 'In Progress', 'Pending Approval')  -- Active only\n",
    "  AND NOT EXISTS (SELECT 1 FROM WORK_ORDER_SUMMARIES...)       -- No duplicates\n",
    "```\n",
    "\n",
    "**💡 Implementation Pattern**: Use this approach when you need to:\n",
    "- Filter stream data by business criteria\n",
    "- Prevent duplicate processing\n",
    "- Apply multiple conditions efficiently\n",
    "\n",
    "### **Stage 2: AI Processing (`ai_summaries` CTE)**\n",
    "```sql\n",
    "-- Generates AI summaries using Snowflake Cortex\n",
    "SNOWFLAKE.CORTEX.COMPLETE(\n",
    "    'claude-3-5-sonnet',                    -- Model selection\n",
    "    ARRAY_CONSTRUCT(OBJECT_CONSTRUCT(...)), -- Conversation format\n",
    "    OBJECT_CONSTRUCT('max_tokens', 256)     -- Response limits\n",
    "):choices[0]:messages::STRING               -- Extract clean text\n",
    "```\n",
    "\n",
    "**💡 Implementation Pattern**: Use this approach when you need to:\n",
    "- Process data through AI models in batch\n",
    "- Handle AI response formatting\n",
    "- Control AI output parameters\n",
    "\n",
    "### **Stage 3: Data Persistence**\n",
    "```sql\n",
    "-- Inserts results with timestamp\n",
    "INSERT INTO WORK_ORDER_SUMMARIES (WORK_ORDER_ID, SUMMARY, CREATED_TIMESTAMP)\n",
    "SELECT s.WORK_ORDER_ID, s.ai_summary, CURRENT_TIMESTAMP()\n",
    "FROM ai_summaries s\n",
    "WHERE s.ai_summary IS NOT NULL;  -- Only successful AI responses\n",
    "```\n",
    "\n",
    "**💡 Implementation Pattern**: Use this approach when you need to:\n",
    "- Store processed results with audit trails\n",
    "- Handle potential AI processing failures gracefully\n",
    "- Maintain data quality (NULL filtering)\n",
    "\n",
    "### **�� Key Design Decisions**\n",
    "\n",
    "1. **Hardcoded Threshold**: `25000` instead of session variables for task reliability\n",
    "2. **Stream Metadata**: Uses `METADATA$ACTION = 'INSERT'` to process only new records\n",
    "3. **CTE Pattern**: Separates filtering, processing, and persistence for clarity\n",
    "4. **Error Handling**: `WHERE s.ai_summary IS NOT NULL` prevents storing failed AI calls\n",
    "5. **Idempotent Design**: `NOT EXISTS` check prevents duplicate processing\n",
    "\n",
    "**✅ This pattern can be adapted for any stream-based AI processing pipeline!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40a7e8-9200-4c4e-9167-c383eb0be61e",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "## Step 3: Create Automated Task\n",
    "\n",
    "**Weekly Batch Processing**: Task runs every Monday at 9 AM EST but only when the stream has new data, processing work orders in batches to save compute costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24883602-a0cb-416e-babe-08dc0f2eec17",
   "metadata": {
    "language": "sql",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TASK PROCESS_HIGH_COST_WORK_ORDERS_TASK\n",
    "    WAREHOUSE = COMPUTE_WH\n",
    "    SCHEDULE = 'USING CRON 0 9 * * MON America/New_York'\n",
    "    COMMENT = 'Automatically process high-cost work orders (>=$25K) for AI summarization - runs weekly on Mondays at 9 AM EST'\n",
    "    WHEN (\n",
    "        -- Only execute when stream has data\n",
    "        -- The stored procedure handles filtering for high-value work orders\n",
    "        SYSTEM$STREAM_HAS_DATA('ENTERPRISE_WORK_ORDERS_STREAM')\n",
    "    )\n",
    "AS\n",
    "    CALL PROCESS_HIGH_COST_WORK_ORDERS();\n",
    "\n",
    "-- Resume the task to start processing\n",
    "ALTER TASK PROCESS_HIGH_COST_WORK_ORDERS_TASK RESUME;\n",
    "\n",
    "SELECT 'Task created and resumed successfully' AS STATUS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f3917-3549-4288-8eac-31eabbcb09a1",
   "metadata": {
    "name": "cell11"
   },
   "source": [
    "## Step 4: Clean Setup for Testing\n",
    "\n",
    "**Important**: Remove any existing test data first to ensure clean demonstration of the batch processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261cda7-f3a6-46fb-a0d8-6c78eaab058f",
   "metadata": {
    "language": "sql",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "-- Clean up any existing test data to ensure clean demo\n",
    "DELETE FROM WORK_ORDER_SUMMARIES WHERE WORK_ORDER_ID LIKE 'DEMO-%';\n",
    "DELETE FROM ENTERPRISE_WORK_ORDERS WHERE WORK_ORDER_ID LIKE 'DEMO-%';\n",
    "\n",
    "-- Verify cleanup\n",
    "SELECT \n",
    "    'Cleanup completed' AS STATUS,\n",
    "    (SELECT COUNT(*) FROM ENTERPRISE_WORK_ORDERS WHERE WORK_ORDER_ID LIKE 'DEMO-%') AS REMAINING_WORK_ORDERS,\n",
    "    (SELECT COUNT(*) FROM WORK_ORDER_SUMMARIES WHERE WORK_ORDER_ID LIKE 'DEMO-%') AS REMAINING_SUMMARIES;\n",
    "\n",
    "-- Show stream status after cleanup\n",
    "SELECT \n",
    "    'Stream Status After Cleanup' AS CHECK_TYPE,\n",
    "    SYSTEM$STREAM_HAS_DATA('ENTERPRISE_WORK_ORDERS_STREAM') AS HAS_DATA,\n",
    "    (SELECT COUNT(*) FROM ENTERPRISE_WORK_ORDERS_STREAM) AS STREAM_RECORD_COUNT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7206ea4-352d-4bf0-a6e5-bcdc753b5a0a",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": [
    "## Step 5: Insert Test Data\n",
    "\n",
    "Now insert fresh test work orders to validate that only high-cost ones (≥$25K) get processed in the next batch run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cea8c2-2b56-47ac-9f1a-8e406c9ac2c7",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "-- Insert HIGH-COST test work order (should trigger processing)\n",
    "INSERT INTO ENTERPRISE_WORK_ORDERS (\n",
    "    WORK_ORDER_ID, EQUIPMENT_TYPE, FACILITY, STATUS, PRIORITY, URGENCY,\n",
    "    TOTAL_COST, LABOR_COST, PARTS_COST, LABOR_HOURS,\n",
    "    CREATED_DATE, SCHEDULED_DATE, WORK_ORDER_NOTES,\n",
    "    FAILURE_MODE, ESTIMATED_DOWNTIME_HOURS, BUSINESS_IMPACT_SCORE,\n",
    "    SAFETY_RISK_LEVEL, COMPLIANCE_REQUIRED, SPECIALIZED_SKILLS_REQUIRED, CONTRACTOR_COST\n",
    ") VALUES (\n",
    "    'DEMO-HIGH-001',\n",
    "    'Critical Production Line',\n",
    "    'Manufacturing Plant A',\n",
    "    'Open',\n",
    "    'Critical',\n",
    "    'High',\n",
    "    50000.00,  -- HIGH COST: Above $25K threshold ✅\n",
    "    20000.00,\n",
    "    25000.00,\n",
    "    80.0,\n",
    "    CURRENT_DATE(),\n",
    "    CURRENT_DATE() + 3,\n",
    "    'CRITICAL: Main production line failure requiring immediate repair. High-cost emergency parts needed.',\n",
    "    'Mechanical Failure',\n",
    "    24.0,\n",
    "    9,\n",
    "    'High',\n",
    "    TRUE,\n",
    "    TRUE,\n",
    "    5000.00\n",
    ");\n",
    "\n",
    "-- Insert LOW-COST test work order (should NOT trigger processing)\n",
    "INSERT INTO ENTERPRISE_WORK_ORDERS (\n",
    "    WORK_ORDER_ID, EQUIPMENT_TYPE, FACILITY, STATUS, PRIORITY, URGENCY,\n",
    "    TOTAL_COST, LABOR_COST, PARTS_COST, LABOR_HOURS,\n",
    "    CREATED_DATE, SCHEDULED_DATE, WORK_ORDER_NOTES,\n",
    "    FAILURE_MODE, ESTIMATED_DOWNTIME_HOURS, BUSINESS_IMPACT_SCORE,\n",
    "    SAFETY_RISK_LEVEL, COMPLIANCE_REQUIRED, SPECIALIZED_SKILLS_REQUIRED, CONTRACTOR_COST\n",
    ") VALUES (\n",
    "    'DEMO-LOW-001',\n",
    "    'Office Equipment',\n",
    "    'Admin Building',\n",
    "    'Open',\n",
    "    'Low',\n",
    "    'Low',\n",
    "    2000.00,  -- LOW COST: Below $25K threshold ❌\n",
    "    1000.00,\n",
    "    800.00,\n",
    "    4.0,\n",
    "    CURRENT_DATE(),\n",
    "    CURRENT_DATE() + 7,\n",
    "    'Routine maintenance on office printers and workstations.',\n",
    "    'Routine Maintenance',\n",
    "    1.0,\n",
    "    2,\n",
    "    'Low',\n",
    "    FALSE,\n",
    "    FALSE,\n",
    "    200.00\n",
    ");\n",
    "\n",
    "SELECT 'Fresh test data inserted' AS STATUS;\n",
    "\n",
    "-- Verify stream captured the new inserts\n",
    "SELECT \n",
    "    'Stream Data Check' AS CHECK_TYPE,\n",
    "    COUNT(*) AS NEW_RECORDS_IN_STREAM,\n",
    "    COUNT(CASE WHEN TOTAL_COST >= 25000 THEN 1 END) AS HIGH_VALUE_IN_STREAM,\n",
    "    COUNT(CASE WHEN TOTAL_COST < 25000 THEN 1 END) AS LOW_VALUE_IN_STREAM\n",
    "FROM ENTERPRISE_WORK_ORDERS_STREAM\n",
    "WHERE METADATA$ACTION = 'INSERT' AND WORK_ORDER_ID LIKE 'DEMO-%';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f709f-c4e0-4ede-a192-187c8e4655c2",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": [
    "## Step 6: Trigger Batch Processing\n",
    "\n",
    "Manually trigger the task to demonstrate immediate batch processing (normally runs weekly on Mondays at 9 AM EST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167a620-fd11-44b8-b030-ca7f99756cd1",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "-- Manually trigger the task for immediate testing\n",
    "EXECUTE TASK PROCESS_HIGH_COST_WORK_ORDERS_TASK;\n",
    "\n",
    "SELECT 'Batch processing task executed - checking results...' AS STATUS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5dc6b8-4930-4294-ba4d-4e901218ea41",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "## Wait 2 minutes for the task to run \n"
  },
  {
   "cell_type": "markdown",
   "id": "870ad094-ad62-481e-bcfd-1f4f35625ea2",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": [
    "## Step 7: Validate Results\n",
    "\n",
    "Verify that the batch processing worked correctly - only high-cost work orders should have summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0a3d4-60d4-4ab5-b007-f686d38ba983",
   "metadata": {
    "language": "sql",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "-- Validate business logic: Only high-cost work orders should have summaries\n",
    "SELECT \n",
    "    '🧪 BATCH PROCESSING VALIDATION' AS TEST_TYPE,\n",
    "    w.WORK_ORDER_ID,\n",
    "    w.TOTAL_COST,\n",
    "    CASE \n",
    "        WHEN w.TOTAL_COST >= 25000 THEN 'HIGH (should have summary)'\n",
    "        ELSE 'LOW (should NOT have summary)'\n",
    "    END AS EXPECTED_CATEGORY,\n",
    "    CASE \n",
    "        WHEN s.WORK_ORDER_ID IS NOT NULL THEN 'HAS SUMMARY ✅' \n",
    "        ELSE 'NO SUMMARY ❌' \n",
    "    END AS ACTUAL_RESULT,\n",
    "    CASE \n",
    "        WHEN w.TOTAL_COST >= 25000 AND s.WORK_ORDER_ID IS NOT NULL THEN '✅ CORRECT'\n",
    "        WHEN w.TOTAL_COST < 25000 AND s.WORK_ORDER_ID IS NULL THEN '✅ CORRECT'\n",
    "        ELSE '❌ ERROR'\n",
    "    END AS VALIDATION_STATUS\n",
    "FROM ENTERPRISE_WORK_ORDERS w\n",
    "LEFT JOIN WORK_ORDER_SUMMARIES s ON w.WORK_ORDER_ID = s.WORK_ORDER_ID\n",
    "WHERE w.WORK_ORDER_ID LIKE 'DEMO-%'\n",
    "ORDER BY w.TOTAL_COST DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d07a3-ddba-4db3-a84a-7c9d951b1557",
   "metadata": {
    "language": "sql",
    "name": "cell19",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "-- Show the generated AI summary for validation\n",
    "SELECT \n",
    "    '🤖 AI GENERATED SUMMARY' AS CONTENT_TYPE,\n",
    "    s.WORK_ORDER_ID,\n",
    "    w.TOTAL_COST,\n",
    "    s.SUMMARY,\n",
    "    s.CREATED_TIMESTAMP\n",
    "FROM WORK_ORDER_SUMMARIES s\n",
    "JOIN ENTERPRISE_WORK_ORDERS w ON s.WORK_ORDER_ID = w.WORK_ORDER_ID\n",
    "WHERE s.WORK_ORDER_ID LIKE 'DEMO-%'\n",
    "ORDER BY s.CREATED_TIMESTAMP DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82ab6d-7d0a-4a43-a929-444dcd8ebb47",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": [
    "## Step 8: Final Clean Up\n",
    "\n",
    "Remove test data after validation to keep the environment clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cece3-6183-4577-bd53-b6f4724ffbc3",
   "metadata": {
    "language": "sql",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "-- Clean up test data after demonstration\n",
    "DELETE FROM WORK_ORDER_SUMMARIES WHERE WORK_ORDER_ID LIKE 'DEMO-%';\n",
    "DELETE FROM ENTERPRISE_WORK_ORDERS WHERE WORK_ORDER_ID LIKE 'DEMO-%';\n",
    "\n",
    "SELECT 'Test data cleaned up successfully' AS STATUS;\n",
    "\n",
    "-- Final verification\n",
    "SELECT \n",
    "    'Final Cleanup Verification' AS CHECK_TYPE,\n",
    "    (SELECT COUNT(*) FROM ENTERPRISE_WORK_ORDERS WHERE WORK_ORDER_ID LIKE 'DEMO-%') AS REMAINING_WORK_ORDERS,\n",
    "    (SELECT COUNT(*) FROM WORK_ORDER_SUMMARIES WHERE WORK_ORDER_ID LIKE 'DEMO-%') AS REMAINING_SUMMARIES;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83095829-81f6-41a3-9f86-ba3a347a6f12",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": [
    "## 🎯 Weekly Batch Processing Pipeline Summary\n",
    "\n",
    "### **✅ VALIDATED BUSINESS LOGIC:**\n",
    "\n",
    "**High-Value Work Order Criteria:**\n",
    "- **Cost Threshold**: ≥ $25,000 (hardcoded for reliability)\n",
    "- **Status Filter**: Active work orders only ('Open', 'In Progress', 'Pending Approval')\n",
    "- **Duplicate Prevention**: Won't reprocess work orders with existing summaries\n",
    "\n",
    "### **🏗️ Architecture Components:**\n",
    "\n",
    "1. **📊 Stream**: Captures work order changes for audit trail\n",
    "2. **🧠 Stored Procedure**: Applies business filtering and generates AI summaries\n",
    "3. **⚡ Task**: Executes batch processing weekly on Mondays at 9 AM EST when new data is available\n",
    "4. **📈 Testing**: Available via `test_pipeline_end_to_end.sql` script\n",
    "\n",
    "### **🤖 AI Integration:**\n",
    "- **Model**: Claude-3.5-Sonnet via Cortex Complete\n",
    "- **Focus**: Concise summaries for executive attention\n",
    "- **Output**: Automatically stored in `WORK_ORDER_SUMMARIES` table\n",
    "\n",
    "### **💡 Key Benefits:**\n",
    "- **Cost Efficient**: Only processes high-value work orders\n",
    "- **Weekly Batch Processing**: Automated processing every Monday at 9 AM EST\n",
    "- **Reliable**: Hardcoded thresholds prevent session variable issues\n",
    "- **Auditable**: Complete stream history for compliance and analysis\n",
    "- **Clean Testing**: Proper cleanup ensures repeatable demonstrations\n",
    "\n",
    "**🚀 The weekly batch processing pipeline is now ready for production use!**\n",
    "\n",
    "**For comprehensive testing**, use the `test_pipeline_end_to_end.sql` script which validates the entire pipeline with detailed business logic verification."
   ]
  }
 ]
}