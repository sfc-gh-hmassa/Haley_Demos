{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Clustering Models with Snowflake Model Registry\n",
    "\n",
    "Transform unstructured text into actionable insights using AI-powered clustering.\n",
    "\n",
    "**What you'll build:**\n",
    "\n",
    "| Model | Algorithm | Runtime | Use Case |\n",
    "|-------|-----------|---------|----------|\n",
    "| `AI_CLUSTER_KMEANS` | K-Means | Warehouse | Group into **specified number** of clusters |\n",
    "| `AI_CLUSTER_DEEP` | HDBSCAN | SPCS | **Auto-discover** clusters + outliers |\n",
    "\n",
    "**Three Steps:**\n",
    "1. **Build** - Define custom model classes\n",
    "2. **Register** - Log to Model Registry with versioning\n",
    "3. **Use** - Run inference via SQL or SPCS batch jobs"
   ],
   "id": "cell-0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade snowflake-ml-python hdbscan -q\n",
    "print(\"Packages installed\")"
   ],
   "id": "cell-1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ],
   "id": "cell-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.model import custom_model\n",
    "\n",
    "# Initialize\n",
    "session = get_active_session()\n",
    "reg = Registry(session=session)\n",
    "\n",
    "# Configuration\n",
    "DATABASE = \"HALEY_DEMOS\"\n",
    "SCHEMA = \"CLUSTERING\"\n",
    "COMPUTE_POOL = \"MLOPS_COMPUTE_POOL\"\n",
    "\n",
    "print(f\"Connected: {session.get_current_account()}\")\n",
    "print(f\"Location: {DATABASE}.{SCHEMA}\")"
   ],
   "id": "cell-3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sample Data"
   ],
   "id": "cell-4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample text data across distinct topics\n",
    "sample_texts = [\n",
    "    # Technology\n",
    "    \"Machine learning models require large datasets for training\",\n",
    "    \"Neural networks can recognize patterns in complex data\",\n",
    "    \"Cloud computing enables scalable data processing\",\n",
    "    \"APIs allow different software systems to communicate\",\n",
    "    \n",
    "    # Finance\n",
    "    \"Stock market volatility affects investment portfolios\",\n",
    "    \"Interest rates influence borrowing costs for businesses\",\n",
    "    \"Diversification reduces risk in investment strategies\",\n",
    "    \"Quarterly earnings reports drive stock price movements\",\n",
    "    \n",
    "    # Healthcare\n",
    "    \"Clinical trials test the efficacy of new medications\",\n",
    "    \"Patient outcomes improve with early disease detection\",\n",
    "    \"Electronic health records streamline medical documentation\",\n",
    "    \"Preventive care reduces long-term healthcare costs\",\n",
    "    \n",
    "    # Environment\n",
    "    \"Renewable energy sources reduce carbon emissions\",\n",
    "    \"Climate change impacts agricultural productivity\",\n",
    "    \"Sustainable practices minimize environmental footprint\",\n",
    "    \"Conservation efforts protect endangered species\"\n",
    "]\n",
    "\n",
    "# Create table\n",
    "session.sql(f\"CREATE SCHEMA IF NOT EXISTS {DATABASE}.{SCHEMA}\").collect()\n",
    "df = session.create_dataframe([[t] for t in sample_texts], schema=[\"TEXT_CONTENT\"])\n",
    "df.write.mode(\"overwrite\").save_as_table(f\"{DATABASE}.{SCHEMA}.SAMPLE_TEXTS\")\n",
    "\n",
    "print(f\"Created {len(sample_texts)} sample records\")\n",
    "session.table(f\"{DATABASE}.{SCHEMA}.SAMPLE_TEXTS\").show(5)"
   ],
   "id": "cell-5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for model registration\n",
    "embeddings_df = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        TEXT_CONTENT,\n",
    "        AI_EMBED('snowflake-arctic-embed-l-v2.0', TEXT_CONTENT)::ARRAY AS EMBEDDING\n",
    "    FROM {DATABASE}.{SCHEMA}.SAMPLE_TEXTS\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "print(f\"Generated {len(embeddings_df)} embeddings (dim={len(embeddings_df['EMBEDDING'].iloc[0])})\")"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 1: BUILD MODELS\n",
    "\n",
    "Define custom model classes using `snowflake.ml.model.custom_model`."
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: K-Means\n",
    "\n",
    "**Use when:** You know how many groups you want.\n",
    "\n",
    "- Partitions data into exactly K clusters\n",
    "- Uses `@partitioned_inference_api` for batch processing\n",
    "- Runs on **Warehouse** (TABLE_FUNCTION)"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIClusterKMeans(custom_model.CustomModel):\n",
    "    \"\"\"K-Means clustering for embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "        self.default_n_clusters = 4\n",
    "    \n",
    "    def _parse_embedding(self, emb):\n",
    "        if isinstance(emb, str):\n",
    "            import json\n",
    "            return json.loads(emb)\n",
    "        return list(emb)\n",
    "    \n",
    "    @custom_model.partitioned_inference_api\n",
    "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Cluster embeddings into K groups.\n",
    "        \n",
    "        Input: ROW_INDEX, EMBEDDING (JSON string), N_CLUSTERS (optional)\n",
    "        Output: ROW_INDEX, CLUSTER_ID, DISTANCE_TO_CENTER\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        import numpy as np\n",
    "        \n",
    "        row_index = input_df['ROW_INDEX'].values if 'ROW_INDEX' in input_df.columns else range(len(input_df))\n",
    "        embeddings = np.array([self._parse_embedding(e) for e in input_df['EMBEDDING']])\n",
    "        n_clusters = int(input_df['N_CLUSTERS'].iloc[0]) if 'N_CLUSTERS' in input_df.columns else self.default_n_clusters\n",
    "        n_clusters = min(n_clusters, len(embeddings))\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        distances = np.min(kmeans.transform(embeddings), axis=1)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'ROW_INDEX': row_index,\n",
    "            'CLUSTER_ID': labels,\n",
    "            'DISTANCE_TO_CENTER': np.round(distances, 4)\n",
    "        })\n",
    "\n",
    "print(\"Defined: AIClusterKMeans\")"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: HDBSCAN\n",
    "\n",
    "**Use when:** You want the algorithm to discover natural groupings.\n",
    "\n",
    "- Automatically finds clusters + identifies outliers\n",
    "- Uses `@inference_api` for standard inference\n",
    "- Runs on **SPCS** (requires `hdbscan` pip package)"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIClusterDeep(custom_model.CustomModel):\n",
    "    \"\"\"HDBSCAN clustering - auto-discovers clusters and outliers.\"\"\"\n",
    "    \n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "        self.default_min_cluster_size = 3\n",
    "    \n",
    "    def _parse_embedding(self, emb):\n",
    "        if isinstance(emb, str):\n",
    "            import json\n",
    "            return json.loads(emb)\n",
    "        return list(emb)\n",
    "    \n",
    "    @custom_model.inference_api\n",
    "    def predict(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Discover clusters in embedding data.\n",
    "        \n",
    "        Input: EMBEDDING, MIN_CLUSTER_SIZE (optional)\n",
    "        Output: CLUSTER_ID (-1=outlier), IS_OUTLIER, PROBABILITY\n",
    "        \"\"\"\n",
    "        import hdbscan\n",
    "        import numpy as np\n",
    "        \n",
    "        embeddings = np.array([self._parse_embedding(e) for e in input_df['EMBEDDING']])\n",
    "        min_size = int(input_df['MIN_CLUSTER_SIZE'].iloc[0]) if 'MIN_CLUSTER_SIZE' in input_df.columns else self.default_min_cluster_size\n",
    "        \n",
    "        clusterer = hdbscan.HDBSCAN(\n",
    "            min_cluster_size=min_size,\n",
    "            metric='euclidean',\n",
    "            prediction_data=True\n",
    "        )\n",
    "        clusterer.fit(embeddings)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'CLUSTER_ID': clusterer.labels_,\n",
    "            'IS_OUTLIER': (clusterer.labels_ == -1),\n",
    "            'PROBABILITY': np.round(clusterer.probabilities_, 4)\n",
    "        })\n",
    "\n",
    "print(\"Defined: AIClusterDeep\")"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 2: REGISTER MODELS\n",
    "\n",
    "Log models to Snowflake Model Registry with version control."
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample inputs for model validation\n",
    "embeddings_list = embeddings_df['EMBEDDING'].tolist()[:10]\n",
    "\n",
    "sample_input_kmeans = pd.DataFrame({\n",
    "    'ROW_INDEX': list(range(10)),\n",
    "    'EMBEDDING': [str(e) for e in embeddings_list],\n",
    "    'N_CLUSTERS': [4] * 10\n",
    "})\n",
    "\n",
    "sample_input_deep = pd.DataFrame({\n",
    "    'EMBEDDING': embeddings_list,\n",
    "    'MIN_CLUSTER_SIZE': [3] * 10\n",
    "})\n",
    "\n",
    "print(\"Sample inputs ready\")"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register K-Means (Warehouse runtime)\n",
    "print(\"Registering AI_CLUSTER_KMEANS...\")\n",
    "\n",
    "mv_kmeans = reg.log_model(\n",
    "    AIClusterKMeans(custom_model.ModelContext()),\n",
    "    model_name=\"AI_CLUSTER_KMEANS\",\n",
    "    version_name=\"V1\",\n",
    "    sample_input_data=sample_input_kmeans,\n",
    "    options={\"function_type\": \"TABLE_FUNCTION\"},\n",
    "    conda_dependencies=[\"scikit-learn\", \"numpy\", \"pandas\"],\n",
    "    comment=\"K-Means clustering for AI_EMBED embeddings\"\n",
    ")\n",
    "\n",
    "session.sql(f\"ALTER MODEL {DATABASE}.{SCHEMA}.AI_CLUSTER_KMEANS SET DEFAULT_VERSION = V1\").collect()\n",
    "print(f\"Registered: {mv_kmeans.model_name} V1 (Warehouse)\")"
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register HDBSCAN (SPCS runtime - pip package required)\n",
    "print(\"Registering AI_CLUSTER_DEEP...\")\n",
    "\n",
    "mv_deep = reg.log_model(\n",
    "    AIClusterDeep(custom_model.ModelContext()),\n",
    "    model_name=\"AI_CLUSTER_DEEP\",\n",
    "    version_name=\"V1\",\n",
    "    sample_input_data=sample_input_deep,\n",
    "    pip_requirements=[\"hdbscan\", \"scikit-learn\", \"numpy\", \"pandas\"],\n",
    "    target_platforms=[\"SNOWPARK_CONTAINER_SERVICES\"],\n",
    "    comment=\"HDBSCAN clustering - auto-discovers clusters and outliers\"\n",
    ")\n",
    "\n",
    "print(f\"Registered: {mv_deep.model_name} V1 (SPCS)\")"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify registered models\n",
    "print(\"Registered Models:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in [\"AI_CLUSTER_KMEANS\", \"AI_CLUSTER_DEEP\"]:\n",
    "    try:\n",
    "        model = reg.get_model(model_name)\n",
    "        versions = model.show_versions()\n",
    "        print(f\"\\n{model_name}\")\n",
    "        print(f\"  Versions: {', '.join(versions['name'].tolist())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model_name}: Not found\")"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STEP 3: USE MODELS\n",
    "\n",
    "Run inference using the registered models."
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. K-Means (SQL - Instant)\n",
    "\n",
    "Runs on warehouse via SQL TABLE function. Fast and simple."
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test K-Means clustering via SQL\n",
    "print(\"Running K-Means clustering...\\n\")\n",
    "\n",
    "results_kmeans = session.sql(f\"\"\"\n",
    "    WITH input_data AS (\n",
    "        SELECT \n",
    "            ROW_NUMBER() OVER (ORDER BY TEXT_CONTENT) AS ROW_INDEX,\n",
    "            TEXT_CONTENT\n",
    "        FROM {DATABASE}.{SCHEMA}.SAMPLE_TEXTS\n",
    "    ),\n",
    "    cluster_results AS (\n",
    "        SELECT r.*\n",
    "        FROM (\n",
    "            SELECT \n",
    "                ROW_NUMBER() OVER (ORDER BY TEXT_CONTENT) AS ROW_INDEX,\n",
    "                1 AS BATCH_ID,\n",
    "                TO_JSON(AI_EMBED('snowflake-arctic-embed-l-v2.0', TEXT_CONTENT)::ARRAY) AS EMBEDDING,\n",
    "                4 AS N_CLUSTERS\n",
    "            FROM {DATABASE}.{SCHEMA}.SAMPLE_TEXTS\n",
    "        ) src,\n",
    "        TABLE(MODEL({DATABASE}.{SCHEMA}.AI_CLUSTER_KMEANS, V1)!PREDICT(\n",
    "              src.ROW_INDEX, src.EMBEDDING, src.N_CLUSTERS)\n",
    "              OVER (PARTITION BY src.BATCH_ID)) r\n",
    "    )\n",
    "    SELECT \n",
    "        c.CLUSTER_ID,\n",
    "        i.TEXT_CONTENT,\n",
    "        c.DISTANCE_TO_CENTER\n",
    "    FROM input_data i\n",
    "    JOIN cluster_results c ON i.ROW_INDEX = c.ROW_INDEX\n",
    "    ORDER BY c.CLUSTER_ID, c.DISTANCE_TO_CENTER\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "# Display by cluster\n",
    "for cluster_id in sorted(results_kmeans['CLUSTER_ID'].unique()):\n",
    "    cluster_data = results_kmeans[results_kmeans['CLUSTER_ID'] == cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_data)} items)\")\n",
    "    for _, row in cluster_data.iterrows():\n",
    "        print(f\"  - {row['TEXT_CONTENT'][:60]}...\")"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. HDBSCAN - Build Container (Takes ~2-3 min)\n",
    "\n",
    "SPCS models require building a container image first. This is a one-time setup per version."
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data with embeddings\n",
    "print(\"Preparing input data...\")\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {DATABASE}.{SCHEMA}.HDBSCAN_INPUT AS\n",
    "    SELECT \n",
    "        TEXT_CONTENT,\n",
    "        TO_JSON(AI_EMBED('snowflake-arctic-embed-l-v2.0', TEXT_CONTENT)::ARRAY) AS EMBEDDING,\n",
    "        2 AS MIN_CLUSTER_SIZE\n",
    "    FROM {DATABASE}.{SCHEMA}.SAMPLE_TEXTS\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"Created: {DATABASE}.{SCHEMA}.HDBSCAN_INPUT\")"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model version reference\n",
    "mv_deep = reg.get_model(\"AI_CLUSTER_DEEP\").version(\"V1\")\n",
    "print(f\"Model: {mv_deep.model_name} V1\")"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. HDBSCAN - Run Batch Inference\n",
    "\n",
    "Submit a batch job to the SPCS compute pool. Container builds on first run, then reuses."
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import batch inference specs\n",
    "from snowflake.ml.model._client.model.batch_inference_specs import OutputSpec, SaveMode\n",
    "\n",
    "# Input data\n",
    "input_df = session.table(f\"{DATABASE}.{SCHEMA}.HDBSCAN_INPUT\").select(\"EMBEDDING\", \"MIN_CLUSTER_SIZE\")\n",
    "\n",
    "# Output location\n",
    "output_stage = f\"@{DATABASE}.{SCHEMA}.NOTEBOOKS/hdbscan_results/\"\n",
    "\n",
    "print(f\"Submitting batch job to {COMPUTE_POOL}...\")\n",
    "print(\"(First run builds container image - takes ~2-3 min)\")\n",
    "print(\"(Subsequent runs are faster)\\n\")\n",
    "\n",
    "job = mv_deep.run_batch(\n",
    "    compute_pool=COMPUTE_POOL,\n",
    "    X=input_df,\n",
    "    output_spec=OutputSpec(\n",
    "        stage_location=output_stage,\n",
    "        mode=SaveMode.OVERWRITE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Job ID: {job.id}\")\n",
    "print(\"Waiting for completion...\")"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for job completion\n",
    "job.wait()\n",
    "print(\"Batch job complete!\")"
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "session.sql(f\"LIST {output_stage}\").show()"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read results - get exact filename from LIST output\n",
    "files = session.sql(f\"LIST {output_stage}\").collect()\n",
    "parquet_file = [f['name'] for f in files if f['name'].endswith('.parquet')][0]\n",
    "\n",
    "results_df = session.read.parquet(f\"@{DATABASE}.{SCHEMA}.{parquet_file}\")\n",
    "results_hdbscan = results_df.to_pandas()\n",
    "\n",
    "# Summary\n",
    "n_clusters = len([c for c in results_hdbscan['CLUSTER_ID'].unique() if c != -1])\n",
    "n_outliers = (results_hdbscan['CLUSTER_ID'] == -1).sum()\n",
    "print(f\"\\nResults: {n_clusters} clusters, {n_outliers} outliers\")\n",
    "print(results_hdbscan[['CLUSTER_ID', 'IS_OUTLIER', 'PROBABILITY']])"
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. Cleanup - Cancel/View Jobs\n",
    "\n",
    "Batch jobs auto-terminate when complete. Use these commands to manage jobs."
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View recent batch jobs\n",
    "from snowflake.ml.jobs import list_jobs\n",
    "\n",
    "print(\"Recent batch jobs:\")\n",
    "print(list_jobs())"
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel a running job (if needed)\n",
    "# job.cancel()\n",
    "\n",
    "# View job logs (for debugging)\n",
    "# print(job.get_logs())\n",
    "\n",
    "print(\"Batch jobs auto-terminate - no manual cleanup needed!\")"
   ],
   "id": "cell-30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Usage Reference\n",
    "\n",
    "### K-Means (SQL)\n",
    "\n",
    "```sql\n",
    "WITH input_data AS (\n",
    "    SELECT \n",
    "        ROW_NUMBER() OVER (ORDER BY id) AS ROW_INDEX,\n",
    "        1 AS BATCH_ID,\n",
    "        TO_JSON(AI_EMBED('snowflake-arctic-embed-l-v2.0', your_text)::ARRAY) AS EMBEDDING,\n",
    "        5 AS N_CLUSTERS\n",
    "    FROM your_table\n",
    ")\n",
    "SELECT r.*\n",
    "FROM input_data src,\n",
    "TABLE(MODEL(db.schema.AI_CLUSTER_KMEANS, V1)!PREDICT(\n",
    "    src.ROW_INDEX, src.EMBEDDING, src.N_CLUSTERS)\n",
    "    OVER (PARTITION BY src.BATCH_ID)) r\n",
    "```\n",
    "\n",
    "### HDBSCAN (Python - run_batch)\n",
    "\n",
    "```python\n",
    "from snowflake.ml.model._client.model.batch_inference_specs import OutputSpec, SaveMode\n",
    "\n",
    "mv = reg.get_model(\"AI_CLUSTER_DEEP\").version(\"V1\")\n",
    "\n",
    "job = mv.run_batch(\n",
    "    compute_pool=\"YOUR_COMPUTE_POOL\",\n",
    "    X=input_dataframe,  # Must have EMBEDDING column\n",
    "    output_spec=OutputSpec(\n",
    "        stage_location=\"@your_stage/output/\",\n",
    "        mode=SaveMode.OVERWRITE\n",
    "    )\n",
    ")\n",
    "\n",
    "job.wait()\n",
    "results = session.read.parquet(\"@your_stage/output/\")\n",
    "```"
   ],
   "id": "cell-31"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Model | Runtime | Invocation | Packages |\n",
    "|-------|---------|------------|----------|\n",
    "| `AI_CLUSTER_KMEANS` | Warehouse | SQL TABLE function | Anaconda |\n",
    "| `AI_CLUSTER_DEEP` | SPCS | `mv.run_batch()` | pip |\n",
    "\n",
    "**Model Registry Benefits:**\n",
    "- Version control for iterations\n",
    "- Lineage tracking\n",
    "- Centralized access control\n",
    "- Usage monitoring via `ACCOUNT_USAGE`"
   ],
   "id": "cell-32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}