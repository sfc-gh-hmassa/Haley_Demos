{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "lastEditStatus": {
   "notebookId": "qa4tf6asb6pygmcj664d",
   "authorId": "3366391852320",
   "authorName": "HMASSA",
   "authorEmail": "haley.massa@snowflake.com",
   "sessionId": "24e9f17f-b90a-4bec-8b0b-cdb0452bb701",
   "lastEditTime": 1771551966611
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Fine-Tuning Embeddings for JIRA Search\n",
    "\n",
    "This demo shows how to **fine-tune a custom embedding model** using Snowflake ML Jobs and use it with **Cortex Search BYO Embedding**.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502   ML Job #1     \u2502     \u2502   ML Job #2     \u2502     \u2502  Cortex Search  \u2502\n",
    "\u2502   Fine-tune     \u2502 \u2500\u2500> \u2502   Log Model     \u2502 \u2500\u2500> \u2502  BYO Embedding  \u2502\n",
    "\u2502   Embedder      \u2502     \u2502   to Registry   \u2502     \u2502                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \u2502                       \u2502                       \u2502\n",
    "        \u25bc                       \u25bc                       \u25bc\n",
    "   Job Stage              Model Registry          Search Service\n",
    "   (artifacts)            (versioned model)      (semantic search)\n",
    "```\n",
    "\n",
    "## Why Fine-Tune Embeddings?\n",
    "\n",
    "Pre-trained embedding models work well on general text, but fine-tuning on **your domain data** improves search relevance:\n",
    "\n",
    "- Learn domain terminology (\"P0 bug\" = critical issue)\n",
    "- Understand relationships (auth issues \u2194 SSO issues)\n",
    "- Define similarity for your use case\n",
    "\n",
    "## What We'll Use\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| [ML Jobs](https://docs.snowflake.com/developer-guide/snowflake-ml/ml-jobs/overview) | Run training on GPU compute pools |\n",
    "| [Model Registry](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview) | Version and deploy the model |\n",
    "| [Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) | Semantic search with BYO embedding |"
   ],
   "id": "3033f8e1-0adf-4d1d-b66b-c99b79d52275"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "Before running this demo, ensure you have:\n",
    "\n",
    "1. **GPU Compute Pool** - For training\n",
    "2. **External Access Integration** - For PyPI and HuggingFace access\n",
    "3. **Warehouse** - For data operations"
   ],
   "id": "0b72a7c3-6ca7-4659-8d6b-31abed4e1206"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Configuration - update these for your environment\n",
    "DATABASE = \"JIRA_EMBEDDING_DEMO\"\n",
    "SCHEMA = \"PUBLIC\"\n",
    "COMPUTE_POOL = \"JIRA_TRAINING_POOL\"  # GPU compute pool\n",
    "WAREHOUSE = \"JIRA_DEMO_WH\"\n",
    "EAI = \"ALLOW_ALL_EAI\"  # External access integration\n",
    "\n",
    "session.use_database(DATABASE)\n",
    "session.use_schema(SCHEMA)\n",
    "print(f\"Using {DATABASE}.{SCHEMA}\")"
   ],
   "id": "3ca4e00f-4a61-45ba-9c11-42ddff3cf622"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "---\n",
    "## Step 1: Prepare Training Data\n",
    "\n",
    "We have 100 synthetic JIRA tickets. For fine-tuning, we create **training pairs** of similar tickets.\n",
    "\n",
    "### Why Training Pairs?\n",
    "\n",
    "The model learns via **contrastive learning** using `MultipleNegativesRankingLoss`:\n",
    "- **Anchor**: A ticket summary\n",
    "- **Positive**: Summary + description of a *similar* ticket (same issue type)\n",
    "- **Negatives**: Other tickets in the batch (implicitly)\n",
    "\n",
    "### Training Logic\n",
    "\n",
    "We pair tickets by **issue type** (BUG, FEATURE, TASK, etc.):\n",
    "- Tickets of the same type should have **similar** embeddings\n",
    "- Different types should be **further apart** in embedding space\n",
    "\n",
    "This teaches the model domain-specific similarity:\n",
    "- \"Login timeout\" \u2194 \"SSO authentication failure\" (both AUTH bugs)\n",
    "- \"Add dark mode\" \u2194 \"Implement theme toggle\" (both UI features)\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "Pre-trained models treat all text equally. Fine-tuning teaches:\n",
    "- Domain vocabulary (\"P0\" = critical, \"SSO\" = auth)\n",
    "- Business similarity (bugs similar to bugs, features to features)\n",
    "- Search relevance for *your* use case"
   ],
   "id": "d6cbd64b-a18f-455f-9bfa-c1f32305d51a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "-- View ticket distribution\n",
    "SELECT ISSUE_TYPE, COUNT(*) as COUNT \n",
    "FROM JIRA_TICKETS \n",
    "GROUP BY ISSUE_TYPE \n",
    "ORDER BY COUNT DESC"
   ],
   "id": "5d7bb918-2fb1-448f-8eaf-c6e98869077b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "-- Create training pairs: tickets of same type should be similar\n",
    "CREATE OR REPLACE TABLE TRAINING_PAIRS AS\n",
    "SELECT \n",
    "    a.SUMMARY AS ANCHOR,\n",
    "    b.SUMMARY || ' ' || b.DESCRIPTION AS POSITIVE\n",
    "FROM JIRA_TICKETS a\n",
    "JOIN JIRA_TICKETS b \n",
    "    ON a.ISSUE_TYPE = b.ISSUE_TYPE \n",
    "    AND a.ISSUE_KEY != b.ISSUE_KEY\n",
    "ORDER BY RANDOM()\n",
    "LIMIT 500;\n",
    "\n",
    "SELECT COUNT(*) AS NUM_TRAINING_PAIRS FROM TRAINING_PAIRS;"
   ],
   "id": "f0769b6a-dbeb-44c5-8bac-7e02db945bf9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "---\n",
    "## Step 2: Submit Training Job\n",
    "\n",
    "### What is ML Jobs?\n",
    "\n",
    "[ML Jobs](https://docs.snowflake.com/developer-guide/snowflake-ml/ml-jobs/overview) runs Python workloads on Snowflake's managed GPU compute pools:\n",
    "\n",
    "- **`submit_directory()`** - Upload code directory and run\n",
    "- **Automatic Snowflake session** - Scripts can query data directly\n",
    "- **Artifact storage** - Outputs saved to job stage (`job._stage_path`)\n",
    "\n",
    "### Training Script Overview\n",
    "\n",
    "Our `src/train.py` script:\n",
    "1. Loads training pairs from `TRAINING_PAIRS` table\n",
    "2. Fine-tunes `all-MiniLM-L6-v2` with `MultipleNegativesRankingLoss`\n",
    "3. Saves model to `MLRS_STAGE_RESULT_PATH` (job output stage)"
   ],
   "id": "0b27ecc0-01cb-449b-bf94-acf9e81aeff4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml import jobs\n",
    "import os\n",
    "\n",
    "# Path to training code\n",
    "# In a real setup, this would be a local path to your src/ directory\n",
    "# For this demo, we'll create it inline\n",
    "import tempfile\n",
    "\n",
    "TRAIN_SCRIPT = '''\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "session = Session.builder.getOrCreate()\n",
    "df = session.sql(\"SELECT * FROM TRAINING_PAIRS\").to_pandas()\n",
    "print(f\"Loaded {len(df)} training pairs\")\n",
    "\n",
    "examples = [\n",
    "    InputExample(texts=[row[\"ANCHOR\"], row[\"POSITIVE\"]])\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"Base model dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "train_dataloader = DataLoader(examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "output_path = os.environ.get(\"MLRS_STAGE_RESULT_PATH\", \"/tmp/output\")\n",
    "model_path = os.path.join(output_path, \"model\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=10,\n",
    "    output_path=model_path,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(f\"Training complete! Model saved to {model_path}\")\n",
    "'''\n",
    "\n",
    "REQUIREMENTS = 'sentence-transformers>=2.2.0\\ntorch\\n'\n",
    "\n",
    "# Create temp directory with training code\n",
    "payload_dir = tempfile.mkdtemp()\n",
    "with open(os.path.join(payload_dir, \"train.py\"), \"w\") as f:\n",
    "    f.write(TRAIN_SCRIPT)\n",
    "with open(os.path.join(payload_dir, \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(REQUIREMENTS)\n",
    "\n",
    "print(f\"Training code prepared at {payload_dir}\")"
   ],
   "id": "2a6d1518-b07f-48a1-a66c-da3951cdfef6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# Submit the training job\n",
    "train_job = jobs.submit_directory(\n",
    "    payload_dir,\n",
    "    entrypoint=\"train.py\",\n",
    "    compute_pool=COMPUTE_POOL,\n",
    "    stage_name=\"TRAINING_STAGE\",\n",
    "    external_access_integrations=[EAI],\n",
    "    session=session,\n",
    ")\n",
    "\n",
    "print(f\"Job submitted!\")\n",
    "print(f\"  Job ID: {train_job.id}\")\n",
    "print(f\"  Stage path: {train_job._stage_path}\")"
   ],
   "id": "fed8de28-52a4-4c51-8163-5baca29df188"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "# Wait for training to complete (typically 5-10 minutes)\n",
    "print(\"Waiting for training job to complete...\")\n",
    "status = train_job.wait()\n",
    "print(f\"\\nJob finished with status: {status}\")\n",
    "\n",
    "if status == \"FAILED\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(train_job.get_logs())\n",
    "else:\n",
    "    print(f\"\\nModel saved to: {train_job._stage_path}/output/\")\n",
    "    # Show last part of logs\n",
    "    logs = train_job.get_logs()\n",
    "    print(f\"\\nTraining logs (last 1000 chars):\\n{logs[-1000:]}\")"
   ],
   "id": "89b4aada-0664-4ee6-8ac0-ce1298ecf2f1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell11"
   },
   "source": [
    "---\n",
    "## Step 3: Log Model to Registry\n",
    "\n",
    "### What is Model Registry?\n",
    "\n",
    "[Model Registry](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview) provides:\n",
    "- **Versioning** - Track model iterations\n",
    "- **Lineage** - Know what data produced each model\n",
    "- **Inference** - Run predictions via `mv.run()` or deploy as service\n",
    "\n",
    "### CustomModel for Embeddings\n",
    "\n",
    "Since `SentenceTransformer` isn't a natively supported model type, we wrap it in a `CustomModel` with an `encode()` inference API. This exposes the embedding function for use."
   ],
   "id": "52947368-676d-419c-90fa-a7e6a0d0f134"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml import jobs\n",
    "\n",
    "# Log model script\n",
    "LOG_SCRIPT = '''\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.fileset.sfcfs import SFFileSystem\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.model import custom_model\n",
    "\n",
    "class JiraEmbedder(custom_model.CustomModel):\n",
    "    \"\"\"Custom model that exposes encode() for generating embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "        self.model = SentenceTransformer(context.path(\"model\"))\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    @custom_model.inference_api\n",
    "    def encode(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        texts = input_df[\"text\"].tolist()\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=False)\n",
    "        return pd.DataFrame({\"embedding\": [emb.tolist() for emb in embeddings]})\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"model_stage_path\")\n",
    "parser.add_argument(\"--model-name\", required=True)\n",
    "parser.add_argument(\"--version\", default=\"v1\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "session = Session.builder.getOrCreate()\n",
    "\n",
    "# Download model from stage\n",
    "local_dir = \"/tmp/model_download\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "fs = SFFileSystem(snowpark_session=session)\n",
    "fs.get(args.model_stage_path.rstrip(\"/\") + \"/\", local_dir, recursive=True)\n",
    "\n",
    "# Find model directory\n",
    "model_path = local_dir\n",
    "for root, dirs, files in os.walk(local_dir):\n",
    "    if \"config.json\" in files:\n",
    "        model_path = root\n",
    "        break\n",
    "\n",
    "print(f\"Model found at: {model_path}\")\n",
    "\n",
    "# Create and test custom model\n",
    "context = custom_model.ModelContext(artifacts={\"model\": model_path})\n",
    "embedder = JiraEmbedder(context)\n",
    "\n",
    "test_result = embedder.encode(pd.DataFrame({\"text\": [\"test\"]}))\n",
    "print(f\"Embedding dimension: {len(test_result.iloc[0]['embedding'])}\")\n",
    "\n",
    "# Log to registry\n",
    "registry = Registry(session=session)\n",
    "mv = registry.log_model(\n",
    "    embedder,\n",
    "    model_name=args.model_name,\n",
    "    version_name=args.version,\n",
    "    sample_input_data=pd.DataFrame({\"text\": [\"sample text\"]}),\n",
    "    pip_requirements=[\"sentence-transformers>=2.2.0\", \"torch\"],\n",
    "    target_platforms=[\"SNOWPARK_CONTAINER_SERVICES\"],\n",
    "    comment=\"Fine-tuned JIRA embedder\"\n",
    ")\n",
    "print(f\"Model logged: {mv.model_name} version {mv.version_name}\")\n",
    "print(f\"Functions: {mv.show_functions()}\")\n",
    "'''\n",
    "\n",
    "# Create payload\n",
    "log_payload_dir = tempfile.mkdtemp()\n",
    "with open(os.path.join(log_payload_dir, \"log_model.py\"), \"w\") as f:\n",
    "    f.write(LOG_SCRIPT)\n",
    "\n",
    "print(f\"Log model script prepared\")"
   ],
   "id": "0b1917db-2985-4b95-b932-584b2036d02a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# Get model path from training job (pass output/ dir, script finds model inside)\nmodel_stage_path = f\"{train_job._stage_path}/output/\"\nprint(f\"Model stage path: {model_stage_path}\")\n\n# Submit logging job\nlog_job = jobs.submit_directory(\n    log_payload_dir,\n    entrypoint=\"log_model.py\",\n    args=[model_stage_path, \"--model-name\", \"JIRA_EMBEDDER\", \"--version\", \"v3\"],\n    compute_pool=COMPUTE_POOL,\n    stage_name=\"LOG_MODEL_STAGE\",\n    external_access_integrations=[EAI],\n    pip_requirements=[\"sentence-transformers>=2.2.0\", \"torch\", \"snowflake-ml-python>=1.6.0\"],\n    session=session,\n)\n\nprint(f\"Log job submitted: {log_job.id}\")",
   "id": "8bd05a03-a0d3-402f-9b8f-a905b3167434"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell14",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# Wait for logging to complete\n",
    "print(\"Waiting for model logging job...\")\n",
    "status = log_job.wait()\n",
    "print(f\"\\nJob finished with status: {status}\")\n",
    "print(f\"\\nLogs:\\n{log_job.get_logs()[-2000:]}\")"
   ],
   "id": "3ffb7d64-5302-4af1-98c6-97ea8605c4cc"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": [
    "---\n",
    "## Step 4: Deploy Embedding Service\n",
    "\n",
    "For search workloads, we deploy the model as a **service** on SPCS:\n",
    "\n",
    "- **`create_service()`** - Deploys model to compute pool\n",
    "- **`mv.run(..., service_name=...)`** - Runs inference via the service\n",
    "- **Auto-suspend** - Service suspends when idle (saves cost)\n",
    "\n",
    "This is better than `run_batch` for search because we get results directly as a DataFrame."
   ],
   "id": "e011dcba-c926-40a4-8eb2-b2ca185bf715"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\nimport pandas as pd\n\n# Load the model from registry\nregistry = Registry(session=session)\nmv = registry.get_model(\"JIRA_EMBEDDER\").version(\"v1\")\n\nprint(\"Model loaded from registry\")\nprint(f\"Available functions: {mv.show_functions()}\")\n\n# Create inference service (reuse if exists)\nSERVICE_NAME = \"JIRA_EMBEDDER_SVC\"\n\n# Check if service already exists (handle both column name cases)\nexisting = mv.list_services()\nservice_exists = False\nif not existing.empty:\n    # Column might be 'service_name' or 'SERVICE_NAME'\n    cols = existing.columns.str.upper()\n    if 'SERVICE_NAME' in cols:\n        col_name = existing.columns[cols.tolist().index('SERVICE_NAME')]\n        service_exists = SERVICE_NAME in existing[col_name].values\n\nif service_exists:\n    print(f\"Service {SERVICE_NAME} already exists - reusing it\")\nelse:\n    # min_instances=0 enables auto-suspend (default: 30 min idle)\n    # To change suspend time: ALTER SERVICE <name> SET AUTO_SUSPEND_SECS = 300\n    print(f\"Creating service {SERVICE_NAME}... (first run builds image ~10-20 min)\")\n    mv.create_service(\n        service_name=SERVICE_NAME,\n        service_compute_pool=COMPUTE_POOL,\n        image_build_compute_pool=COMPUTE_POOL,\n        min_instances=0,  # Enables auto-suspend after 30 min idle\n        max_instances=1,\n    )\n    print(\"Service created! (auto-suspends after 30 min idle)\")",
   "id": "1e9a478e-b794-4257-83e1-bb77f0d069a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for all JIRA tickets\n",
    "tickets_df = session.table(\"JIRA_TICKETS\").to_pandas()\n",
    "\n",
    "# Combine summary and description for embedding\n",
    "# Column name indicates what was embedded\n",
    "texts_df = pd.DataFrame({\n",
    "    \"text\": tickets_df[\"SUMMARY\"] + \" \" + tickets_df[\"DESCRIPTION\"]\n",
    "})\n",
    "\n",
    "print(f\"Generating embeddings for {len(texts_df)} tickets...\")\n",
    "embeddings_df = mv.run(\n",
    "    texts_df, \n",
    "    function_name=\"encode\",\n",
    "    service_name=SERVICE_NAME\n",
    ")\n",
    "print(f\"Generated {len(embeddings_df)} embeddings\")\n",
    "\n",
    "# Name column to indicate source fields\n",
    "EMBEDDING_COL = \"EMBEDDING_SUMMARY_DESC\"\n",
    "dim = len(embeddings_df.iloc[0]['embedding'])\n",
    "print(f\"Embedding dimension: {dim}\")"
   ],
   "id": "cd22e019-ad1d-4537-8b97-2bbe02e8986e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "# Add embeddings to tickets and store\n",
    "tickets_df[EMBEDDING_COL] = embeddings_df[\"embedding\"]\n",
    "\n",
    "# Create table with VECTOR column\n",
    "session.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE JIRA_WITH_EMBEDDINGS (\n",
    "        ISSUE_KEY VARCHAR,\n",
    "        ISSUE_TYPE VARCHAR,\n",
    "        PRIORITY VARCHAR,\n",
    "        STATUS VARCHAR,\n",
    "        COMPONENT VARCHAR,\n",
    "        SUMMARY VARCHAR,\n",
    "        DESCRIPTION VARCHAR,\n",
    "        {EMBEDDING_COL} VECTOR(FLOAT, {dim})\n",
    "    )\n",
    "\"\"\").collect()\n",
    "\n",
    "# Insert data using SELECT (VALUES doesn't support VECTOR)\n",
    "print(f\"Inserting {len(tickets_df)} tickets with embeddings...\")\n",
    "for idx, row in tickets_df.iterrows():\n",
    "    emb_str = \",\".join(str(x) for x in row[EMBEDDING_COL])\n",
    "    summary = row['SUMMARY'].replace(\"'\", \"''\")\n",
    "    desc = row['DESCRIPTION'].replace(\"'\", \"''\")\n",
    "    session.sql(f\"\"\"\n",
    "        INSERT INTO JIRA_WITH_EMBEDDINGS \n",
    "        SELECT \n",
    "            '{row['ISSUE_KEY']}',\n",
    "            '{row['ISSUE_TYPE']}',\n",
    "            '{row['PRIORITY']}',\n",
    "            '{row['STATUS']}',\n",
    "            '{row['COMPONENT']}',\n",
    "            '{summary}',\n",
    "            '{desc}',\n",
    "            [{emb_str}]::VECTOR(FLOAT, {dim})\n",
    "    \"\"\").collect()\n",
    "\n",
    "print(f\"Done! Table JIRA_WITH_EMBEDDINGS ready.\")"
   ],
   "id": "046bf7b1-2478-496b-8e70-42aafb0c0d4a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell19"
   },
   "source": [
    "---\n",
    "## Step 5: Create Cortex Search with BYO Embedding\n",
    "\n",
    "[Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview) is Snowflake's managed search service. The **BYO Embedding** feature lets you use pre-computed embeddings instead of Snowflake's built-in models.\n",
    "\n",
    "The `EMBEDDING` clause tells Cortex Search to use our custom embeddings."
   ],
   "id": "0fb3f83c-7cea-4022-a0ec-86423cfdda76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "sql",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "-- Create Cortex Search service with BYO embedding\n",
    "-- TEXT INDEXES: columns for keyword search\n",
    "-- VECTOR INDEXES: our pre-computed embedding column\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE JIRA_SEARCH\n",
    "    TEXT INDEXES (SUMMARY)\n",
    "    VECTOR INDEXES (EMBEDDING_SUMMARY_DESC)\n",
    "    ATTRIBUTES ISSUE_TYPE, PRIORITY, COMPONENT\n",
    "    WAREHOUSE = JIRA_DEMO_WH\n",
    "    TARGET_LAG = '1 hour'\n",
    "AS SELECT \n",
    "    ISSUE_KEY,\n",
    "    ISSUE_TYPE,\n",
    "    PRIORITY,\n",
    "    COMPONENT,\n",
    "    SUMMARY,\n",
    "    DESCRIPTION,\n",
    "    EMBEDDING_SUMMARY_DESC\n",
    "FROM JIRA_WITH_EMBEDDINGS"
   ],
   "id": "c5e18263-472b-405f-b1bc-29ee22a7caf2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "---\n",
    "## Step 6: Search!\n",
    "\n",
    "To search, we embed the query with the **same model** and find similar tickets."
   ],
   "id": "6e4d0cad-ecbe-47e3-a29c-7ab1d17dcb8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "def search_jira(query: str, top_k: int = 5):\n",
    "    \"\"\"Search JIRA tickets using our fine-tuned embedder.\"\"\"\n",
    "    \n",
    "    # Embed query with our model (via service)\n",
    "    query_df = pd.DataFrame({\"text\": [query]})\n",
    "    query_emb = mv.run(\n",
    "        query_df, \n",
    "        function_name=\"encode\",\n",
    "        service_name=SERVICE_NAME\n",
    "    ).iloc[0][\"embedding\"]\n",
    "    emb_str = \",\".join(str(x) for x in query_emb)\n",
    "    \n",
    "    # Vector similarity search\n",
    "    results = session.sql(f\"\"\"\n",
    "        SELECT \n",
    "            ISSUE_KEY,\n",
    "            ISSUE_TYPE,\n",
    "            PRIORITY,\n",
    "            SUMMARY,\n",
    "            ROUND(VECTOR_COSINE_SIMILARITY(\n",
    "                EMBEDDING_SUMMARY_DESC, \n",
    "                [{emb_str}]::VECTOR(FLOAT, {len(query_emb)})\n",
    "            ), 3) AS SCORE\n",
    "        FROM JIRA_WITH_EMBEDDINGS\n",
    "        ORDER BY SCORE DESC\n",
    "        LIMIT {top_k}\n",
    "    \"\"\").to_pandas()\n",
    "    \n",
    "    return results"
   ],
   "id": "e8460fae-a0a9-4197-94e2-3a5ac0127d6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "# Try some searches\n",
    "print(\"Query: 'authentication login SSO error'\\n\")\n",
    "print(search_jira(\"authentication login SSO error\").to_string(index=False))"
   ],
   "id": "4adcb24b-91e7-4b08-a799-3a30456e0bbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "print(\"Query: 'payment checkout failure'\\n\")\n",
    "print(search_jira(\"payment checkout failure\").to_string(index=False))"
   ],
   "id": "43b767fb-9b70-47c0-93a6-3ecff8b99fe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "print(\"Query: 'performance slow loading'\\n\")\n",
    "print(search_jira(\"performance slow loading\").to_string(index=False))"
   ],
   "id": "2a7ab426-3378-44e1-9c6f-9d4af55f1b3b"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell26"
   },
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "We built an end-to-end pipeline following the [official ML Jobs pattern](https://github.com/Snowflake-Labs/sf-samples/tree/main/samples/ml/ml_jobs/llm_finetune):\n",
    "\n",
    "| Step | What | How |\n",
    "|------|------|-----|\n",
    "| 1 | **Fine-tune** | ML Job with `submit_directory()` \u2192 model saved to job stage |\n",
    "| 2 | **Log model** | Separate ML Job downloads from stage, wraps in `CustomModel`, logs to Registry |\n",
    "| 3 | **Deploy service** | `mv.create_service()` \u2192 model runs on SPCS (auto-suspends when idle) |\n",
    "| 4 | **Generate embeddings** | `mv.run(df, service_name=...)` \u2192 results direct to DataFrame |\n",
    "| 5 | **Search** | Cortex Search with BYO Embedding clause |\n",
    "\n",
    "### Key Patterns\n",
    "\n",
    "- **`job._stage_path`** - Access training artifacts from completed job\n",
    "- **`SFFileSystem.get()`** - Download from Snowflake stages\n",
    "- **`CustomModel`** - Wrap non-native models with `@inference_api`\n",
    "- **`create_service()`** - Deploy for recurring inference (auto-suspends)\n",
    "- **`run_batch()`** - Use for one-off large batch jobs (outputs to stage)\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "When completely done with the embedder:\n",
    "```python\n",
    "mv.delete_service(\"JIRA_EMBEDDER_SVC\")\n",
    "```\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [ML Jobs Documentation](https://docs.snowflake.com/developer-guide/snowflake-ml/ml-jobs/overview)\n",
    "- [Model Registry Overview](https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview)\n",
    "- [Cortex Search with BYO Embedding](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview)\n",
    "- [Official LLM Fine-Tuning Sample](https://github.com/Snowflake-Labs/sf-samples/tree/main/samples/ml/ml_jobs/llm_finetune)"
   ],
   "id": "6c93a931-b34c-485f-91fd-29f7720c5d33"
  },
  {
   "cell_type": "code",
   "id": "d37a1e1c-54c7-4c85-9c96-b01d3b5c8c41",
   "metadata": {
    "language": "sql",
    "name": "cell28"
   },
   "outputs": [],
   "source": "DROP SERVICE JIRA_EMBEDDER_SVC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8da1950-0e01-4ab1-b988-ffce62b2e94d",
   "metadata": {
    "language": "sql",
    "name": "cell29",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}