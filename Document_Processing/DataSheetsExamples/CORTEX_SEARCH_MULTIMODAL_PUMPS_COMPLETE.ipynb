{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "pyucbpjrzzv5hbys6qwr",
   "authorId": "3366391852320",
   "authorName": "HMASSA",
   "authorEmail": "haley.massa@snowflake.com",
   "sessionId": "449c9d4c-9f79-4a6b-ab59-c05f18ff6656",
   "lastEditTime": 1756771347369
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Implementing multimodal retrieval using Cortex Search Service\n",
    "\n",
    "Welcome! This tutorial shows a lightweight example where a customer has pump datasheet PDFs and wants to search and ask natural questions on them. On a high level, this tutorial demonstrates:\n",
    "\n",
    "- Convert long PDF files to document screenshots (images).\n",
    "- (Optional but highly recommended) Run parse_document on PDFs for auxiliary text retrieval to further improve quality.\n",
    "- Embed document screenshots using EMBED_IMAGE_1024 (PrPr) which runs `voyage-multimodal-3` under the hood\n",
    "- **NEW: Add metadata attributes (vendor, product, section titles) for better targeting**\n",
    "- Create a Cortex Search Service using multimodal embeddings and OCR text.\n",
    "- Retrieve top pages using Cortex Search.\n",
    "- Get natural language answer with multimodal RAG!\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell2",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "%pip install pdfplumber\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell3",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install pypdf2\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell4",
    "language": "sql",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "LS @DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\n",
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": [
    "Now let's run some python code:\n",
    "\n",
    "The purpose is to paginate raw pages into pages -- in image and PDF format. Images are for multimodal retrieval, while PDFs are for better OCR quality (optional). As long as you configure the config correctly, you are good to go!\n",
    "\n",
    "```\n",
    "class Config:\n",
    "    input_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\"\n",
    "    output_stage: str = \"@CORTEX_SEARCH_DB.PYU.MULTIMODAL_DEMO_INTERNAL/\"\n",
    "    input_path: str = \"raw_pdf\"\n",
    "    output_pdf_path: str = \"paged_pdf\"\n",
    "    output_image_path: str = \"paged_image\"\n",
    "    allowed_extensions: List[str] = None\n",
    "    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n",
    "    dpi: int = 300  # Default DPI for image conversion\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.allowed_extensions is None:\n",
    "            self.allowed_extensions = [\".pdf\"]\n",
    "```\n",
    "\n",
    "**Make sure the output_stage is an internal stage**, because `embed_image_1024` only works with internal stages at the moment.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import snowflake.snowpark.session as session\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def print_info(msg: str) -> None:\n",
    "    \"\"\"Print info message\"\"\"\n",
    "    print(f\"INFO: {msg}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def print_error(msg: str) -> None:\n",
    "    \"\"\"Print error message\"\"\"\n",
    "    print(f\"ERROR: {msg}\", file=sys.stderr)\n",
    "    if hasattr(st, \"error\"):\n",
    "        st.error(msg)\n",
    "\n",
    "\n",
    "def print_warning(msg: str) -> None:\n",
    "    \"\"\"Print warning message\"\"\"\n",
    "    print(f\"WARNING: {msg}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_stage: str = \"@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL/raw_pdf/\"\n",
    "    output_stage: str = (\n",
    "        \"@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL/\"  # Base output stage without subdirectories\n",
    "    )\n",
    "    input_path: str = \"raw_pdf\"\n",
    "    output_pdf_path: str = \"paged_pdf\"\n",
    "    output_image_path: str = \"paged_image\"\n",
    "    allowed_extensions: List[str] = None\n",
    "    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n",
    "    dpi: int = 300  # Default DPI for image conversion\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.allowed_extensions is None:\n",
    "            self.allowed_extensions = [\".pdf\"]\n",
    "\n",
    "\n",
    "class PDFProcessingError(Exception):\n",
    "    \"\"\"Base exception for PDF processing errors\"\"\"\n",
    "\n",
    "\n",
    "class FileDownloadError(PDFProcessingError):\n",
    "    \"\"\"Raised when file download fails\"\"\"\n",
    "\n",
    "\n",
    "class PDFConversionError(PDFProcessingError):\n",
    "    \"\"\"Raised when PDF conversion fails\"\"\"\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def managed_temp_file(suffix: str = None) -> str:\n",
    "    \"\"\"Context manager for temporary file handling\"\"\"\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n",
    "    try:\n",
    "        yield temp_file.name\n",
    "    finally:\n",
    "        # Don't delete the file immediately, let the caller handle cleanup\n",
    "        pass\n",
    "\n",
    "\n",
    "def cleanup_temp_file(file_path: str) -> None:\n",
    "    \"\"\"Clean up a temporary file\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            os.unlink(file_path)\n",
    "    except OSError as e:\n",
    "        print_warning(f\"Failed to delete temporary file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def list_pdf_files(session: session.Session, config: Config) -> List[dict]:\n",
    "    \"\"\"List all PDF files in the source stage\"\"\"\n",
    "    try:\n",
    "        # Use LIST command instead of DIRECTORY function\n",
    "        query = f\"\"\"\n",
    "        LIST {config.input_stage}\n",
    "        \"\"\"\n",
    "\n",
    "        file_list = session.sql(query).collect()\n",
    "\n",
    "        # Filter for PDF files\n",
    "        pdf_files = []\n",
    "        for file_info in file_list:\n",
    "            full_path = file_info[\"name\"]\n",
    "            # Extract just the filename from the full path\n",
    "            file_name = os.path.basename(full_path)\n",
    "\n",
    "            if any(\n",
    "                file_name.lower().endswith(ext) for ext in config.allowed_extensions\n",
    "            ):\n",
    "                pdf_files.append(\n",
    "                    {\n",
    "                        \"RELATIVE_PATH\": file_name,  # Use just the filename\n",
    "                        \"FULL_STAGE_PATH\": full_path,  # Use full path for download\n",
    "                        \"SIZE\": file_info[\"size\"] if \"size\" in file_info else 0,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        print_info(f\"Found {len(pdf_files)} PDF files in the stage\")\n",
    "        return pdf_files\n",
    "    except Exception as e:\n",
    "        print_error(f\"Failed to list files: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def download_file_from_stage(\n",
    "    session: session.Session, file_path: str, config: Config\n",
    ") -> str:\n",
    "    \"\"\"Download a file from stage using session.file.get\"\"\"\n",
    "    # Create a temporary directory\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    try:\n",
    "        # Ensure there are no double slashes in the path\n",
    "        stage_path = f\"{config.input_stage.rstrip('/')}/{file_path.lstrip('/')}\"\n",
    "\n",
    "        # Get the file from stage\n",
    "        get_result = session.file.get(stage_path, temp_dir)\n",
    "        if not get_result or get_result[0].status != \"DOWNLOADED\":\n",
    "            raise FileDownloadError(f\"Failed to download file: {file_path}\")\n",
    "\n",
    "        # Construct the local path where the file was downloaded\n",
    "        local_path = os.path.join(temp_dir, os.path.basename(file_path))\n",
    "        if not os.path.exists(local_path):\n",
    "            raise FileDownloadError(f\"Downloaded file not found at: {local_path}\")\n",
    "\n",
    "        return local_path\n",
    "    except Exception as e:\n",
    "        print_error(f\"Error downloading {file_path}: {e}\")\n",
    "        # Clean up the temporary directory\n",
    "        try:\n",
    "            import shutil\n",
    "\n",
    "            shutil.rmtree(temp_dir)\n",
    "        except Exception as cleanup_error:\n",
    "            print_warning(f\"Failed to clean up temporary directory: {cleanup_error}\")\n",
    "        raise FileDownloadError(f\"Failed to download file: {e}\")\n",
    "\n",
    "\n",
    "def upload_file_to_stage(\n",
    "    session: session.Session, file_path: str, output_path: str, config: Config\n",
    ") -> str:\n",
    "    \"\"\"Upload file to the output stage\"\"\"\n",
    "    try:\n",
    "        # Get the directory and filename from the output path\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        base_name = os.path.basename(output_path)\n",
    "\n",
    "        # Create the full stage path with subdirectory\n",
    "        stage_path = f\"{config.output_stage.rstrip('/')}/{output_dir.lstrip('/')}\"\n",
    "\n",
    "        # Read the content of the original file\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        # Create a new file with the correct name\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_file_path = os.path.join(temp_dir, base_name)\n",
    "\n",
    "        # Write the content to the new file\n",
    "        with open(temp_file_path, \"wb\") as f:\n",
    "            f.write(file_content)\n",
    "\n",
    "        # Upload the file using session.file.put with compression disabled\n",
    "        put_result = session.file.put(\n",
    "            temp_file_path, stage_path, auto_compress=False, overwrite=True\n",
    "        )\n",
    "\n",
    "        # Check upload status\n",
    "        if not put_result or len(put_result) == 0:\n",
    "            raise Exception(f\"Failed to upload file: {base_name}\")\n",
    "\n",
    "        if put_result[0].status not in [\"UPLOADED\", \"SKIPPED\"]:\n",
    "            raise Exception(f\"Upload failed with status: {put_result[0].status}\")\n",
    "\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file_path):\n",
    "            os.remove(temp_file_path)\n",
    "\n",
    "        return f\"Successfully uploaded {base_name} to {stage_path}\"\n",
    "    except Exception as e:\n",
    "        print_error(f\"Error uploading file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_pdf_files(config: Config) -> None:\n",
    "    \"\"\"Main process to orchestrate the PDF splitting\"\"\"\n",
    "    try:\n",
    "        session = get_active_session()\n",
    "        pdf_files = list_pdf_files(session, config)\n",
    "\n",
    "        for file_info in pdf_files:\n",
    "            file_path = file_info[\"RELATIVE_PATH\"]\n",
    "            print_info(f\"Processing: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                # Download the PDF file\n",
    "                local_pdf_path = download_file_from_stage(session, file_path, config)\n",
    "\n",
    "                # Get base filename without extension\n",
    "                base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "                # Extract individual PDF pages\n",
    "                with open(local_pdf_path, \"rb\") as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    num_pages = len(pdf_reader.pages)\n",
    "                    print_info(f\"Converting PDF to {num_pages} pages of PDFs\")\n",
    "\n",
    "                    for i in range(num_pages):\n",
    "                        page_num = i + 1\n",
    "                        s3_pdf_output_path = (\n",
    "                            f\"{config.output_pdf_path}/{base_name}_page_{page_num}.pdf\"\n",
    "                        )\n",
    "                        pdf_writer = PyPDF2.PdfWriter()\n",
    "                        pdf_writer.add_page(pdf_reader.pages[i])\n",
    "                        temp_file = tempfile.NamedTemporaryFile(\n",
    "                            delete=False, suffix=\".pdf\"\n",
    "                        )\n",
    "                        local_pdf_tmp_file_name = temp_file.name\n",
    "                        with open(local_pdf_tmp_file_name, \"wb\") as output_file:\n",
    "                            pdf_writer.write(output_file)\n",
    "                        \n",
    "                        upload_file_to_stage(\n",
    "                            session, local_pdf_tmp_file_name, s3_pdf_output_path, config\n",
    "                        )\n",
    "                        cleanup_temp_file(local_pdf_tmp_file_name)\n",
    "                            \n",
    "                # Convert PDF to images                \n",
    "                with pdfplumber.open(local_pdf_path) as pdf:\n",
    "                    print_info(f\"Converting PDF to {len(pdf.pages)} images\")\n",
    "                    for i, page in enumerate(pdf.pages):\n",
    "                        page_num = i + 1\n",
    "                        # Get page dimensions\n",
    "                        width = page.width\n",
    "                        height = page.height\n",
    "\n",
    "                        # Determine if scaling is needed\n",
    "                        max_dim = max(width, height)\n",
    "                        if max_dim > config.max_dimension:\n",
    "                            # Calculate scale factor to fit within max_dimension\n",
    "                            scale_factor = config.max_dimension / max_dim\n",
    "                            width = int(width * scale_factor)\n",
    "                            height = int(height * scale_factor)\n",
    "\n",
    "                        img = page.to_image(resolution=config.dpi)\n",
    "                        temp_file = tempfile.NamedTemporaryFile(\n",
    "                            delete=False, suffix=\".png\"\n",
    "                        )\n",
    "                        local_image_tmp_file_name = temp_file.name\n",
    "                        img.save(local_image_tmp_file_name)\n",
    "\n",
    "                        s3_image_output_path = (\n",
    "                            f\"{config.output_image_path}/{base_name}_page_{page_num}.png\"\n",
    "                        )\n",
    "                        \n",
    "                        upload_file_to_stage(\n",
    "                            session, local_image_tmp_file_name, s3_image_output_path, config\n",
    "                        )\n",
    "                        cleanup_temp_file(local_image_tmp_file_name)\n",
    "                        \n",
    "                # Clean up the original downloaded file\n",
    "                cleanup_temp_file(local_pdf_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print_error(f\"Error processing {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print_error(f\"Fatal error in process_pdf_files: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "config = Config(dpi=200)\n",
    "process_pdf_files(config)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": [
    "Check out one image and see if it's clear. If you can't read clearly, neural models won't either!\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "\n",
    "image=session.file.get_stream(\n",
    "     f\"@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL/paged_image/AHLSTAREndSuctionSingleStage_E10083_page_2.png\",  # change to one image on your stage\n",
    "     decompress=False).read()\n",
    "st.image(image)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "Now let's start the multimodal embedding part! We first create an intermediate table that holds relative file names of images, and then call `SNOWFLAKE.CORTEX.embed_image_1024` to turn them into vectors!\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell10",
    "language": "sql"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DEMODB.DATASHEET_RAG.DATASHEET_IMAGE_CORPUS AS\nSELECT\n    CONCAT('paged_image/', split_part(metadata$filename, '/', -1)) AS FILE_NAME,\n    REGEXP_SUBSTR(metadata$filename, '_page_([0-9]+)', 1, 1, 'e',1)::INTEGER as PAGE_NUMBER,\n    '@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL' AS STAGE_PREFIX\nFROM\n    @DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL/paged_image/\nGROUP BY 1, 2, 3\n;\n\nSELECT * FROM DEMODB.DATASHEET_RAG.DATASHEET_IMAGE_CORPUS LIMIT 5;\n",
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell11",
    "language": "sql",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DEMODB.DATASHEET_RAG.DATASHEET_VM3_VECTORS AS\n",
    "SELECT\n",
    "    FILE_NAME,\n",
    "    STAGE_PREFIX,\n",
    "    AI_EMBED(\n",
    "        'voyage-multimodal-3',\n",
    "        TO_FILE('@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL', FILE_NAME)\n",
    "    ) AS IMAGE_VECTOR\n",
    "FROM DEMODB.DATASHEET_RAG.DATASHEET_IMAGE_CORPUS;\n",
    "\n",
    "\n",
    "SELECT * FROM DEMODB.DATASHEET_RAG.DATASHEET_VM3_VECTORS LIMIT 5;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": [
    "Similarly, we call `SNOWFLAKE.CORTEX.PARSE_DOCUMENT` to extract text from PDF pages. We discover that, although multimodal retrieval is powerful, augmenting it with text retrieval for keyword matching can bring quality improvement on certain types of search tasks/queries.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell13",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DEMODB.DATASHEET_RAG.DATASHEET_PDF_CORPUS AS\n",
    "SELECT\n",
    "    CONCAT('paged_pdf/', split_part(metadata$filename, '/', -1)) AS FILE_NAME,\n",
    "    REGEXP_SUBSTR(metadata$filename, '_page_([0-9]+)', 1, 1, 'e',1)::INTEGER as PAGE_NUMBER,\n",
    "    '@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL' AS STAGE_PREFIX\n",
    "FROM\n",
    "    @DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL/paged_pdf/\n",
    "GROUP BY 1, 2, 3\n",
    ";\n",
    "\n",
    "CREATE OR REPLACE TABLE DEMODB.DATASHEET_RAG.DATASHEET_PARSE_DOC AS\n",
    "    SELECT\n",
    "        FILE_NAME,\n",
    "        PAGE_NUMBER,\n",
    "        STAGE_PREFIX,\n",
    "        PARSE_JSON(TO_VARCHAR(SNOWFLAKE.CORTEX.PARSE_DOCUMENT(\n",
    "            '@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL',\n",
    "            FILE_NAME,\n",
    "            {'mode': 'LAYOUT'}\n",
    "        ))):content AS PARSE_DOC_OUTPUT\n",
    "    FROM DEMODB.DATASHEET_RAG.DATASHEET_PDF_CORPUS\n",
    ";\n",
    "\n",
    "SELECT * FROM DEMODB.DATASHEET_RAG.DATASHEET_PARSE_DOC LIMIT 5;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": [
    "## Adding metadata to Cortex Search Services\n",
    "\n",
    "Before creating the final search service, let's add metadata attributes that will help with better targeting and filtering:\n",
    "\n",
    "- **Directory-level metadata**: Vendor, Product ID, Pump Model, Datasheet Type\n",
    "- **Page-level metadata**: Section Title derived from OCR text\n",
    "\n",
    "This metadata will be projected as attributes in the Cortex Search Service, allowing for better filtering and context in search results.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell15",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- Directory table (one row per datasheet)\n",
    "CREATE OR REPLACE TABLE DEMODB.DATASHEET_RAG.DATASHEET_DIRECTORY (\n",
    "  FILE_NAME STRING PRIMARY KEY,\n",
    "  VENDOR STRING,\n",
    "  PRODUCT_ID STRING,\n",
    "  PUMP_MODEL STRING,\n",
    "  DATASHEET_TYPE STRING\n",
    ");\n",
    "\n",
    "INSERT OVERWRITE INTO DEMODB.DATASHEET_RAG.DATASHEET_DIRECTORY (FILE_NAME, VENDOR, PRODUCT_ID, PUMP_MODEL, DATASHEET_TYPE) VALUES\n",
    "  ('AHLSTAREndSuctionSingleStage_E10083.pdf','Sulzer','AHLSTAR-E10083','AHLSTAR End Suction Single Stage','Performance Datasheet'),\n",
    "  ('B_3196i.pdf','Goulds','3196-iFRAME','3196 i-FRAME','ANSI Process Datasheet'),\n",
    "  ('BEEndSuctionSingleStageCentrifugalPump60HzUS.pdf','Sulzer','BE-60Hz-US','BE End Suction Single Stage','Technical Specification'),\n",
    "  ('Centrifugal-Curvebook-2020-1.pdf','Fristam','Curvebook-2020','Centrifugal Sanitary Pumps','Curve Book'),\n",
    "  ('Pump-Selection-Guide-brochure.pdf','Various','Selection-Guide','General Selection','Selection Guide');\n",
    "\n",
    "SELECT * FROM DEMODB.DATASHEET_RAG.DATASHEET_DIRECTORY;\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell16",
    "language": "sql",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Extract section titles from parsed document content\nCREATE OR REPLACE TABLE DEMODB.DATASHEET_RAG.DATASHEET_PAGE_METADATA AS\nWITH section_extraction AS (\n  SELECT\n    FILE_NAME,\n    PAGE_NUMBER,\n    PARSE_DOC_OUTPUT,\n        -- Extract potential section titles (all caps text lines) - simplified regex\n    TRIM(REGEXP_SUBSTR(TO_VARCHAR(PARSE_DOC_OUTPUT), '[A-Z][A-Z0-9 /\\\\(\\\\)\\\\-]{3,}')) AS SECTION_TITLE\n  FROM DEMODB.DATASHEET_RAG.DATASHEET_PARSE_DOC\n)\nSELECT\n  FILE_NAME,\n  PAGE_NUMBER,\n  PARSE_DOC_OUTPUT,\n  COALESCE(SECTION_TITLE, 'GENERAL') AS SECTION_TITLE,\n  -- Extract base filename for joining with directory\n  REGEXP_SUBSTR(FILE_NAME, 'paged_pdf/(.*)_page_[0-9]+\\\\.pdf$', 1, 1, 'e', 1) || '.pdf' AS BASE_FILE_NAME\nFROM section_extraction;\n\nSELECT * FROM DEMODB.DATASHEET_RAG.DATASHEET_PAGE_METADATA LIMIT 5;\n",
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": [
    "Now we join image vectors and texts into a single table with metadata attributes, and create a Cortex Search service!\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    },
    "name": "cell18",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DEMODB.DATASHEET_RAG.DATASHEET_JOINED_DATA AS\n",
    "SELECT\n",
    "    v.FILE_NAME,\n",
    "    p.PAGE_NUMBER,\n",
    "    v.IMAGE_VECTOR AS VECTOR_MAIN,\n",
    "    p.PARSE_DOC_OUTPUT AS TEXT,\n",
    "    v.FILE_NAME AS IMAGE_FILEPATH,\n",
    "    -- Add metadata attributes\n",
    "    d.VENDOR,\n",
    "    d.PRODUCT_ID,\n",
    "    d.PUMP_MODEL,\n",
    "    d.DATASHEET_TYPE,\n",
    "    p.SECTION_TITLE\n",
    "FROM\n",
    "    DEMODB.DATASHEET_RAG.DATASHEET_VM3_VECTORS v\n",
    "JOIN\n",
    "    DEMODB.DATASHEET_RAG.DATASHEET_PAGE_METADATA p\n",
    "ON\n",
    "    REGEXP_SUBSTR(v.FILE_NAME, 'paged_image/(.*)\\\\.png$', 1, 1, 'e', 1) = REGEXP_SUBSTR(p.FILE_NAME, 'paged_pdf/(.*)\\\\.pdf$', 1, 1, 'e', 1)\n",
    "LEFT JOIN\n",
    "    DEMODB.DATASHEET_RAG.DATASHEET_DIRECTORY d\n",
    "ON\n",
    "    d.FILE_NAME = p.BASE_FILE_NAME;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE DEMODB.DATASHEET_RAG.DATASHEET_CORTEX_SEARCH_SERVICE\n",
    "  TEXT INDEXES TEXT\n",
    "  VECTOR INDEXES VECTOR_MAIN\n",
    "  ATTRIBUTES VENDOR, PRODUCT_ID, PUMP_MODEL, DATASHEET_TYPE, SECTION_TITLE\n",
    "  WAREHOUSE='COMPUTE_WH'\n",
    "  TARGET_LAG='1 day'\n",
    "AS (\n",
    "    SELECT \n",
    "        TO_VARCHAR(TEXT) AS TEXT, \n",
    "        PAGE_NUMBER, \n",
    "        VECTOR_MAIN,\n",
    "        IMAGE_FILEPATH,\n",
    "        VENDOR,\n",
    "        PRODUCT_ID,\n",
    "        PUMP_MODEL,\n",
    "        DATASHEET_TYPE,\n",
    "        SECTION_TITLE\n",
    "    FROM DEMODB.DATASHEET_RAG.DATASHEET_JOINED_DATA\n",
    ");\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": [
    "We have created a multi-index Cortex Search Service with both text and vector indexes, plus metadata attributes. This allows us to perform hybrid search combining keyword matching on text content and semantic similarity on vector embeddings, with the ability to filter by vendor, product, or section type. We'll embed queries directly with `SNOWFLAKE.CORTEX.EMBED_TEXT_1024` and use the new multi-index query syntax to search across both index types.\n",
    "\n",
    "**Note:** The multi-index query syntax requires Snowflake Python API version 1.6.0 or later.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell20",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "demo_query_text = \"What is the NPSH required at 120% flow for Sulzer pump?\"\n",
    "sql_output = session.sql(f\"\"\"SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_1024('voyage-multimodal-3', 'Represent the query for retrieving supporting documents:  {demo_query_text}')\"\"\").collect()\n",
    "query_vector = list(sql_output[0].asDict().values())[0]\n",
    "print(query_vector)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell21",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.core import Root\n\ndef multi_modal_answer(question_text):\n    sql_output = session.sql(f\"\"\"SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_1024('voyage-multimodal-3', 'Represent the query for retrieving supporting documents:  {question_text}')\"\"\").collect()\n    query_vector = list(sql_output[0].asDict().values())[0]\n\n    ## Use Multi-Index Querying \n    root = Root(session)\n    # fetch service\n    my_service = (root\n        .databases[\"DEMODB\"]\n        .schemas[\"DATASHEET_RAG\"]\n        .cortex_search_services[\"DATASHEET_CORTEX_SEARCH_SERVICE\"]\n    )\n    \n    # query service using multi-index query syntax\n    resp = my_service.search(\n    multi_index_query={\n        \"TEXT\": [{\"text\": question_text}],\n        \"VECTOR_MAIN\": [{\"vector\": query_vector}]\n    },\n        columns=[\"TEXT\", \"PAGE_NUMBER\", \"IMAGE_FILEPATH\", \"VENDOR\", \"PRODUCT_ID\", \"PUMP_MODEL\", \"SECTION_TITLE\"],\n        limit=5\n    )\n    \n    # Display search results with metadata\n    print(\"\\n=== SEARCH RESULTS ===\")\n    for i, result in enumerate(resp.to_dict()[\"results\"]):\n        print(f\"\\nRank {i+1}:\")\n        print(f\"  Vendor: {result.get('VENDOR', 'N/A')}\")\n        print(f\"  Product: {result.get('PRODUCT_ID', 'N/A')} - {result.get('PUMP_MODEL', 'N/A')}\")\n        print(f\"  Section: {result.get('SECTION_TITLE', 'N/A')}\")\n        print(f\"  Page: {result.get('PAGE_NUMBER', 'N/A')}\")\n        print(f\"  Image: {result.get('IMAGE_FILEPATH', 'N/A')}\")\n\n    top_page_path = resp.to_dict()[\"results\"][0][\"IMAGE_FILEPATH\"]\n\n    # grab the relative path from search result\n    sql = \"\"\"\n        SELECT AI_COMPLETE(\n        'claude-3-7-sonnet',\n        PROMPT(\n        'Answer the question using the image {0}. Question: {1}',\n        TO_FILE('@DEMODB.DATASHEET_RAG.MULTIMODAL_DEMO_INTERNAL', ?),\n        ?\n        )\n        ) AS answer\n    \"\"\"\n    row = session.sql(sql, params=[top_page_path, question_text]).collect()[0]\n    print(f\"\\n=== TOP RESULT ===\")\n    print(f\"Image: {top_page_path}\")\n    print(f\"\\n=== ANSWER ===\")\n    print(row[\"ANSWER\"])\n\n    return resp\n",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell22",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "multi_modal_answer(\"What is the NPSH required at 120% flow for the Sulzer BE pump?\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell23",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "multi_modal_answer(\"What is the maximum operating temperature specified for the Goulds 3196 pump?\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell24",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "multi_modal_answer(\"What is the casting material listed in the Ahlstar pump datasheet?\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell25",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "multi_modal_answer(\"What is the maximum flow rate capacity of the Fristam FKL pump?\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell27",
    "language": "python"
   },
   "outputs": [],
   "source": "multi_modal_answer(\"What is the maximum liquid temperature with the high-temperature option for the Goulds 3196?\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "code",
   "id": "6704c3ff-1ac9-4ae0-bc7b-4bb7b39a7103",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "multi_modal_answer(\"Using the performance curve, at 50 GPM and 135 ft head, what impeller diameter and NPSH requi\")\n",
   "execution_count": null
  }
 ]
}