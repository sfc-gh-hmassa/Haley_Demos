{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "hg46vsam36tje3tckvik",
   "authorId": "3366391852320",
   "authorName": "HMASSA",
   "authorEmail": "haley.massa@snowflake.com",
   "sessionId": "d8abc015-4017-450b-883e-eed6b8a82e23",
   "lastEditTime": 1765411603591
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# ðŸ“˜ Notebook 2: Deep Learning on GPU (SPCS) â€” CNN vs DNN\n\n## Introduction\n\nThis notebook compares **CNN (Convolutional Neural Network)** vs **DNN (Deep Neural Network)** for wafer yield prediction using GPU compute on SPCS.\n\n| Model | Architecture | Strengths |\n|-------|--------------|-----------|\n| **DNN** | Multi-Layer Perceptron | Universal approximator, good baseline |\n| **CNN** | 1D Convolutional Network | Captures local patterns in sequential features |\n\n### What's Covered\n\n| Section | Topic |\n|---------|-------|\n| 1 | Load ML Dataset via Data Connector â†’ PyTorch |\n| 2 | GPU Setup |\n| 3 | Define DNN & CNN Architectures |\n| 4 | Train Both Models |\n| 5 | Log to Snowflake ML Experiments |\n| 6 | Register Champion Model to Registry |\n\n### References\n\n- [Snowflake ML Data Connector](https://docs.snowflake.com/en/developer-guide/snowflake-ml/load-data)\n- [Snowflake ML Experiments](https://docs.snowflake.com/en/developer-guide/snowflake-ml/experiments)\n- [Snowflake Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview)\n"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "source": "# ============================================================================\n# IMPORTS\n# ============================================================================\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Snowflake\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml import dataset\nfrom snowflake.ml.data import DataConnector\nfrom snowflake.ml.experiment import ExperimentTracking\nfrom snowflake.ml.registry import Registry\n\nprint(\"âœ… Imports complete\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "source": "# ============================================================================\n# SESSION & EXPERIMENT SETUP\n# ============================================================================\n\nsession = get_active_session()\nsession.sql(\"USE DATABASE WAFER_YIELD_DEMO\").collect()\nsession.sql(\"USE SCHEMA RAW_DATA\").collect()  # Same schema where Dataset was created in Notebook 1\n\n# Initialize Snowflake ML Experiments\nexp = ExperimentTracking(session=session)\nEXPERIMENT_NAME = \"WAFER_YIELD_CNN_VS_DNN\"\nexp.set_experiment(EXPERIMENT_NAME)\n\nprint(f\"âœ… Session: {session.get_current_database()}.{session.get_current_schema()}\")\nprint(f\"âœ… Experiment: {EXPERIMENT_NAME}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "28ddff52-c72b-40b5-9f12-cc93bb6ca0d3",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "---\n## Section 1 â€” Load ML Dataset via Data Connector\n\n### Why Use the Snowflake Data Connector?\n\nThe **DataConnector** is Snowflake's optimized API for loading data into ML workflows. Key benefits:\n\n| Benefit | Description |\n|---------|-------------|\n| **Parallel Reads** | Distributes data loading across multiple compute nodes using Ray |\n| **Direct Integration** | Works seamlessly with Snowflake Datasets and Snowpark DataFrames |\n| **Framework Support** | Converts directly to pandas, PyTorch, or TensorFlow datasets |\n| **Streaming** | Data loaded in streaming fashion for memory efficiency |\n\n### Data Loading Options\n\n```\nDataConnector.from_dataset(snowflake_dataset)  â†’ Load from ML Dataset\nDataConnector.from_dataframe(snowpark_df)      â†’ Load from Snowpark DataFrame\n```\n\n### Conversion Methods\n\n```python\ndata_connector.to_pandas()           # â†’ pandas DataFrame\ndata_connector.to_torch_dataset()    # â†’ PyTorch Dataset  \ndata_connector.to_tf_dataset()       # â†’ TensorFlow Dataset\n```\n\n**Reference**: [Snowflake ML Load Data Documentation](https://docs.snowflake.com/en/developer-guide/snowflake-ml/load-data)"
  },
  {
   "cell_type": "code",
   "id": "f27c29fa-cf0c-41af-b8a7-90f6e5883e8e",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# ============================================================================\n# LOAD ML DATASET â†’ DATA CONNECTOR â†’ PYTORCH\n# ============================================================================\n\nDATASET_NAME = \"WAFER_YIELD_TRAINING_DATASET\"\nDATASET_VERSION = \"v1\"\nBATCH_SIZE = 256\n\n# Load ML Dataset (created in Notebook 01)\nwafer_dataset = dataset.load_dataset(session, DATASET_NAME, DATASET_VERSION)\n\n# Create DataConnector and convert directly to PyTorch dataset\ndata_connector = DataConnector.from_dataset(wafer_dataset)\ntorch_dataset = data_connector.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=True)\n\n# Create DataLoader (batch_size=None because batching handled by to_torch_dataset)\ndataloader = DataLoader(torch_dataset, batch_size=None)\n\n# Define columns\nlabel_col = \"YIELD_GOOD\"\nEXCLUDE_COLS = ['WAFER_ID', 'YIELD_GOOD', 'YIELD_SCORE', 'DOMINANT_DEFECT_TYPE']\n\nprint(f\"âœ… ML Dataset loaded: {DATASET_NAME} v{DATASET_VERSION}\")\nprint(f\"âœ… PyTorch DataLoader ready (batch_size={BATCH_SIZE})\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6361f10-cd1c-4e22-9d57-57d1152b0ce3",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# INSPECT FIRST BATCH & DETERMINE FEATURE COLUMNS\n# ============================================================================\n\n# Get first batch to discover columns dynamically\nfirst_batch = next(iter(dataloader))\nall_columns = list(first_batch.keys())\nfeature_cols = [c for c in all_columns if c not in EXCLUDE_COLS]\ninput_dim = len(feature_cols)\n\nprint(f\"ðŸ“Š Batch Structure:\")\nprint(f\"   All columns: {len(all_columns)}\")\nprint(f\"   Feature columns: {input_dim}\")\nprint(f\"   Label column: {label_col}\")\nprint(f\"\\n   Features: {feature_cols[:5]}...\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d8a0a0b-2c61-4858-8168-61f97043c2ba",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2938771-6379-4175-9239-8f906a8b6dc1",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "---\n## Section 2 â€” GPU Setup"
  },
  {
   "cell_type": "code",
   "id": "15f687e5-3bf4-4c1f-a196-d4edad6751f8",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# ============================================================================\n# GPU CONFIGURATION\n# ============================================================================\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ðŸ–¥ï¸ Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88b16a69-ae25-4ce9-bff5-d325d2794939",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "---\n## Section 3 â€” Define DNN & CNN Architectures\n"
  },
  {
   "cell_type": "code",
   "id": "575d8183-728b-4220-aa82-8e421d359f16",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# ============================================================================\n# DNN (DEEP NEURAL NETWORK) ARCHITECTURE\n# ============================================================================\n\nclass WaferYieldDNN(nn.Module):\n    \"\"\"Multi-layer perceptron for wafer yield classification.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dims=[256, 128, 64, 32], dropout=0.3):\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for h in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, h),\n                nn.BatchNorm1d(h),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev_dim = h\n        layers.extend([nn.Linear(prev_dim, 1), nn.Sigmoid()])\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)\n\ndnn_model = WaferYieldDNN(input_dim).to(device)\nprint(f\"ðŸ§  DNN: {sum(p.numel() for p in dnn_model.parameters()):,} parameters\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1c4e8756-7a80-4d95-9c6a-5514d11a46a7",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# CNN (1D CONVOLUTIONAL NEURAL NETWORK) ARCHITECTURE\n# ============================================================================\n\nclass WaferYieldCNN(nn.Module):\n    \"\"\"1D CNN that treats features as a sequence.\"\"\"\n    \n    def __init__(self, input_dim, channels=[64, 128, 256], kernel_size=3, fc_dim=128, dropout=0.3):\n        super().__init__()\n        conv_layers = []\n        in_ch = 1\n        for out_ch in channels:\n            conv_layers.extend([\n                nn.Conv1d(in_ch, out_ch, kernel_size, padding=kernel_size//2),\n                nn.BatchNorm1d(out_ch),\n                nn.ReLU(),\n                nn.MaxPool1d(2)\n            ])\n            in_ch = out_ch\n        self.conv = nn.Sequential(*conv_layers)\n        self.fc = nn.Sequential(\n            nn.AdaptiveAvgPool1d(1),\n            nn.Flatten(),\n            nn.Linear(channels[-1], fc_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(fc_dim, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        x = x.unsqueeze(1)  # (batch, 1, features)\n        return self.fc(self.conv(x))\n\ncnn_model = WaferYieldCNN(input_dim).to(device)\nprint(f\"ðŸ§  CNN: {sum(p.numel() for p in cnn_model.parameters()):,} parameters\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a486b30-1f42-4626-ba26-0204591a4d0c",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "---\n## Section 4 â€” Train Both Models\n"
  },
  {
   "cell_type": "code",
   "id": "109c3dd3-92dd-489a-bbf7-884916fef0b2",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "# ============================================================================\n# TRAINING CONFIGURATION\n# ============================================================================\n\nEPOCHS = 50\nLR = 0.001\ncriterion = nn.BCELoss()\n\n# Store metrics for comparison\ndnn_metrics = {}\ncnn_metrics = {}\n\nprint(f\"âš™ï¸ Training Config: epochs={EPOCHS}, lr={LR}, batch_size={BATCH_SIZE}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c693605e-539d-4e1c-8d95-2443a82eb60f",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "# ============================================================================\n# TRAIN DNN & LOG TO EXPERIMENTS\n# ============================================================================\n\nrun_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\nwith exp.start_run(run_name=f\"DNN_{run_timestamp}\") as dnn_run:\n    # Log hyperparameters\n    exp.log_params({\n        \"model_type\": \"DNN\",\n        \"hidden_dims\": \"[256, 128, 64, 32]\",\n        \"dropout\": 0.3,\n        \"epochs\": EPOCHS,\n        \"learning_rate\": LR,\n        \"batch_size\": BATCH_SIZE\n    })\n    \n    print(\"ðŸš€ Training DNN...\")\n    dnn_start = datetime.now()\n    optimizer = optim.AdamW(dnn_model.parameters(), lr=LR)\n    \n    correct, total = 0, 0\n    for epoch in range(EPOCHS):\n        dnn_model.train()\n        epoch_loss = 0\n        batch_count = 0\n        \n        # Reload for each epoch (streaming dataset)\n        torch_dataset = data_connector.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=True)\n        dataloader = DataLoader(torch_dataset, batch_size=None)\n        \n        for batch in dataloader:\n            y = batch.pop(label_col).float().squeeze().unsqueeze(1).to(device)\n            X = torch.stack([batch[col].float().squeeze() for col in feature_cols], dim=1).to(device)\n            \n            optimizer.zero_grad()\n            pred = dnn_model(X)\n            loss = criterion(pred, y)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            batch_count += 1\n            \n            if epoch == EPOCHS - 1:\n                correct += ((pred >= 0.5).float() == y).sum().item()\n                total += y.size(0)\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\"   Epoch {epoch+1}: loss={epoch_loss/batch_count:.4f}\")\n    \n    dnn_time = (datetime.now() - dnn_start).total_seconds()\n    dnn_accuracy = correct / total if total > 0 else 0\n    dnn_loss = epoch_loss / batch_count\n    \n    # Log final metrics to experiment tracking\n    exp.log_metrics({\n        \"accuracy\": dnn_accuracy,\n        \"final_loss\": dnn_loss,\n        \"training_time_seconds\": dnn_time\n    })\n    \n    print(f\"âœ… DNN logged: accuracy={dnn_accuracy:.4f}, time={dnn_time:.1f}s\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eaee219a-4ac0-4964-a33a-a893cf703ae7",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# TRAIN CNN & LOG TO EXPERIMENTS\n# ============================================================================\n\nwith exp.start_run(run_name=f\"CNN_{run_timestamp}\") as cnn_run:\n    # Log hyperparameters\n    exp.log_params({\n        \"model_type\": \"CNN\",\n        \"channels\": \"[64, 128, 256]\",\n        \"kernel_size\": 3,\n        \"dropout\": 0.3,\n        \"epochs\": EPOCHS,\n        \"learning_rate\": LR,\n        \"batch_size\": BATCH_SIZE\n    })\n    \n    print(\"ðŸš€ Training CNN...\")\n    cnn_start = datetime.now()\n    optimizer = optim.AdamW(cnn_model.parameters(), lr=LR)\n    \n    correct, total = 0, 0\n    for epoch in range(EPOCHS):\n        cnn_model.train()\n        epoch_loss = 0\n        batch_count = 0\n        \n        # Reload for each epoch (streaming dataset)\n        torch_dataset = data_connector.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=True)\n        dataloader = DataLoader(torch_dataset, batch_size=None)\n        \n        for batch in dataloader:\n            y = batch.pop(label_col).float().squeeze().unsqueeze(1).to(device)\n            X = torch.stack([batch[col].float().squeeze() for col in feature_cols], dim=1).to(device)\n            \n            optimizer.zero_grad()\n            pred = cnn_model(X)\n            loss = criterion(pred, y)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            batch_count += 1\n            \n            if epoch == EPOCHS - 1:\n                correct += ((pred >= 0.5).float() == y).sum().item()\n                total += y.size(0)\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\"   Epoch {epoch+1}: loss={epoch_loss/batch_count:.4f}\")\n    \n    cnn_time = (datetime.now() - cnn_start).total_seconds()\n    cnn_accuracy = correct / total if total > 0 else 0\n    cnn_loss = epoch_loss / batch_count\n    \n    # Log final metrics to experiment tracking\n    exp.log_metrics({\n        \"accuracy\": cnn_accuracy,\n        \"final_loss\": cnn_loss,\n        \"training_time_seconds\": cnn_time\n    })\n    \n    print(f\"âœ… CNN logged: accuracy={cnn_accuracy:.4f}, time={cnn_time:.1f}s\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b2dc5e0-bb23-43bf-8272-c0ab48e13b37",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "---\n## Section 4b â€” Distributed Hyperparameter Tuning (DNN)\n\nUse **Snowflake ML Tune** for distributed hyperparameter search across multiple trials.\n\n### Key Concepts\n\n| Component | Description |\n|-----------|-------------|\n| **TuneConfig** | Search space definition (params to tune) |\n| **ScalingConfig** | Compute resources per trial |\n| **RandomSearch** | Search algorithm (random sampling) |\n\n**Reference**: [Snowflake ML Hyperparameter Tuning](https://docs.snowflake.com/en/developer-guide/snowflake-ml/modeling/tune-model-hyperparameters)\n\n\nIn the example below, these are the explicit differences in these trainings vs the ones above. \n\n|Aspect\t| Baseline\t| Tuning |\n|------ | ---------| ------- | \n|Architecture\t| Fixed 4 layers\t|Dynamic 2 layers |\n|Epochs\t| 50\t| 10 (faster trials) |\n|Hyperparameters\t| Hardcoded\t| From search space |\n|Runs\t| 1\t| 8 parallel trials |\n|Purpose\t| Full training\t| Find best config |\n\nThe tuner runs train_dnn_trial() 8 times with different configs, then you'd typically retrain the best config with full epochs (50) for the final model.\n"
  },
  {
   "cell_type": "code",
   "id": "3d6a0928-80c5-42f2-835f-c69c488051d0",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "# ============================================================================\n# DEFINE TRAINABLE FUNCTION FOR TUNING\n# ============================================================================\n\ndef train_dnn_trial(config: dict) -> dict:\n    \"\"\"\n    Training function for a single hyperparameter trial.\n    Called by Snowflake ML Tune with different config values.\n    \"\"\"\n    # Extract hyperparameters from config\n    hidden_dim_1 = config[\"hidden_dim_1\"]\n    hidden_dim_2 = config[\"hidden_dim_2\"]\n    dropout = config[\"dropout\"]\n    lr = config[\"learning_rate\"]\n    \n    # Build model with trial hyperparameters\n    model = nn.Sequential(\n        nn.Linear(input_dim, hidden_dim_1),\n        nn.BatchNorm1d(hidden_dim_1),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim_1, hidden_dim_2),\n        nn.BatchNorm1d(hidden_dim_2),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim_2, 1),\n        nn.Sigmoid()\n    ).to(device)\n    \n    criterion = nn.BCELoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    \n    # Train for fewer epochs during tuning\n    TUNE_EPOCHS = 10\n    \n    for epoch in range(TUNE_EPOCHS):\n        model.train()\n        torch_ds = data_connector.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=True)\n        dl = DataLoader(torch_ds, batch_size=None)\n        \n        for batch in dl:\n            y = batch.pop(label_col).float().squeeze().unsqueeze(1).to(device)\n            X = torch.stack([batch[col].float().squeeze() for col in feature_cols], dim=1).to(device)\n            \n            optimizer.zero_grad()\n            pred = model(X)\n            loss = criterion(pred, y)\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluate\n    model.eval()\n    correct, total = 0, 0\n    torch_ds = data_connector.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=False)\n    dl = DataLoader(torch_ds, batch_size=None)\n    \n    with torch.no_grad():\n        for batch in dl:\n            y = batch.pop(label_col).float().squeeze().unsqueeze(1).to(device)\n            X = torch.stack([batch[col].float().squeeze() for col in feature_cols], dim=1).to(device)\n            pred = model(X)\n            correct += ((pred >= 0.5).float() == y).sum().item()\n            total += y.size(0)\n    \n    accuracy = correct / total if total > 0 else 0\n    \n    return {\"accuracy\": accuracy}\n\nprint(\"âœ… Training function defined for tuning\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2de7455a-1525-4f4e-bb7d-05b728ee8d9b",
   "metadata": {
    "language": "python",
    "name": "cell20",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# DEFINE TRAINABLE FUNCTION FOR TUNING\n# ============================================================================\n\ndef train_dnn_trial():\n    \"\"\"\n    Training function for Snowflake ML Tune.\n    Uses get_tuner_context() to access hyperparameters and datasets.\n    \"\"\"\n    # Get tuner context\n    tuner_context = get_tuner_context()\n    config = tuner_context.get_hyper_params()\n    dataset_map = tuner_context.get_dataset_map()\n    \n    # Extract hyperparameters\n    hidden_dim_1 = int(config[\"hidden_dim_1\"])\n    hidden_dim_2 = int(config[\"hidden_dim_2\"])\n    dropout = config[\"dropout\"]\n    lr = config[\"learning_rate\"]\n    \n    # Load data from dataset_map\n    train_dc = dataset_map[\"train\"]\n    \n    # Build model with trial hyperparameters\n    model = nn.Sequential(\n        nn.Linear(input_dim, hidden_dim_1),\n        nn.BatchNorm1d(hidden_dim_1),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim_1, hidden_dim_2),\n        nn.BatchNorm1d(hidden_dim_2),\n        nn.ReLU(),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_dim_2, 1),\n        nn.Sigmoid()\n    ).to(device)\n    \n    criterion = nn.BCELoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr)\n    \n    # Train for fewer epochs during tuning\n    TUNE_EPOCHS = 10\n    \n    for epoch in range(TUNE_EPOCHS):\n        model.train()\n        torch_ds = train_dc.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=True)\n        dl = DataLoader(torch_ds, batch_size=None)\n        \n        for batch in dl:\n            y = batch.pop(label_col).float().squeeze().unsqueeze(1).to(device)\n            X = torch.stack([batch[col].float().squeeze() for col in feature_cols], dim=1).to(device)\n            \n            optimizer.zero_grad()\n            pred = model(X)\n            loss = criterion(pred, y)\n            loss.backward()\n            optimizer.step()\n    \n    # Evaluate\n    model.eval()\n    correct, total = 0, 0\n    torch_ds = train_dc.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=False)\n    dl = DataLoader(torch_ds, batch_size=None)\n    \n    with torch.no_grad():\n        for batch in dl:\n            y = batch.pop(label_col).float().squeeze().unsqueeze(1).to(device)\n            X = torch.stack([batch[col].float().squeeze() for col in feature_cols], dim=1).to(device)\n            pred = model(X)\n            correct += ((pred >= 0.5).float() == y).sum().item()\n            total += y.size(0)\n    \n    accuracy = correct / total if total > 0 else 0\n    \n    # Report metrics and model back to tuner\n    tuner_context.report(metrics={\"accuracy\": accuracy}, model=model)\n\nprint(\"âœ… Training function defined for Snowflake ML Tune\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77e532e9-bb96-4a7c-b6a5-ecb26fa022e3",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "# ============================================================================\n# RUN DISTRIBUTED HYPERPARAMETER TUNING (Snowflake ML Tune)\n# ============================================================================\n\n# Define search space using Snowflake ML sampling functions\n# Can also use lists for grid search: [128, 256, 512]\nfrom snowflake.ml.modeling.tune import Uniform, LogUniform\n# Import tuning components (in case not already imported)\nfrom snowflake.ml.modeling.tune import Tuner, TunerConfig, get_tuner_context, Uniform, LogUniform\nfrom snowflake.ml.modeling.tune.search import RandomSearch\n\n\nsearch_space = {\n    \"hidden_dim_1\": Uniform(128, 512),\n    \"hidden_dim_2\": Uniform(64, 256),\n    \"dropout\": Uniform(0.2, 0.5),\n    \"learning_rate\": LogUniform(1e-4, 1e-2)\n}\n\n# Configure tuning with GPU resources\ntuner_config = TunerConfig(\n    metric=\"accuracy\",\n    mode=\"max\",\n    search_alg=RandomSearch(random_state=42),\n    num_trials=8,\n    resource_per_trial={\"GPU\": 1, \"CPU\": 2}  # Request GPU for each trial\n)\n\nprint(\"ðŸ”§ Starting distributed hyperparameter tuning...\")\nprint(f\"   Search space: hidden_dim_1, hidden_dim_2, dropout, learning_rate\")\nprint(f\"   Num trials: {tuner_config.num_trials}\")\nprint(f\"   Search algorithm: RandomSearch\")\n\n# Create dataset_map for tuner\ndataset_map = {\"train\": data_connector}\n\n# Create and run tuner\ntuner = Tuner(\n    train_func=train_dnn_trial,\n    search_space=search_space,\n    tuner_config=tuner_config\n)\n\ntune_results = tuner.run(dataset_map=dataset_map)\n\n# Get best result\nbest_config = tune_results.best_result\nbest_accuracy = tune_results.best_result[\"accuracy\"].iloc[0] if hasattr(tune_results.best_result[\"accuracy\"], \"iloc\") else tune_results.best_result[\"accuracy\"]\n\nprint(f\"\\nðŸ† Best Trial Results:\")\nprint(f\"   Accuracy: {best_accuracy:.4f}\")\nprint(f\"   Config: {best_config}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37847072-3a3c-40ed-ae3d-1d2ec626e49a",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "# ============================================================================\n# RETRAIN BEST MODEL WITH FULL EPOCHS\n# ============================================================================\n\nprint(\"ðŸš€ Retraining with best hyperparameters for full epochs...\")\n\n# Debug: print the structure of best_result\nprint(f\"   Best result type: {type(tune_results.best_result)}\")\nprint(f\"   Best result: {tune_results.best_result}\")\n\n# Extract best hyperparameters - handle different result structures\ntry:\n    # If it's a DataFrame row\n    if hasattr(tune_results.best_result, \"iloc\"):\n        row = tune_results.best_result.iloc[0] if len(tune_results.best_result) > 0 else tune_results.best_result\n        best_hidden_1 = int(row[\"hidden_dim_1\"])\n        best_hidden_2 = int(row[\"hidden_dim_2\"])\n        best_dropout = float(row[\"dropout\"])\n        best_lr = float(row[\"learning_rate\"])\n    # If it's a dict\n    elif isinstance(tune_results.best_result, dict):\n        best_hidden_1 = int(tune_results.best_result[\"hidden_dim_1\"])\n        best_hidden_2 = int(tune_results.best_result[\"hidden_dim_2\"])\n        best_dropout = float(tune_results.best_result[\"dropout\"])\n        best_lr = float(tune_results.best_result[\"learning_rate\"])\n    else:\n        # Try accessing as attributes or columns\n        best_hidden_1 = int(tune_results.best_result.hidden_dim_1)\n        best_hidden_2 = int(tune_results.best_result.hidden_dim_2)\n        best_dropout = float(tune_results.best_result.dropout)\n        best_lr = float(tune_results.best_result.learning_rate)\nexcept Exception as e:\n    print(f\"   Error extracting params: {e}\")\n    print(f\"   Using default values from baseline\")\n    best_hidden_1 = 256\n    best_hidden_2 = 128\n    best_dropout = 0.3\n    best_lr = 0.001\n\nprint(f\"   Best config: hidden_dims=[{best_hidden_1}, {best_hidden_2}], dropout={best_dropout:.2f}, lr={best_lr:.6f}\")\n\n# Build tuned model\ntuned_model = nn.Sequential(\n    nn.Linear(input_dim, best_hidden_1),\n    nn.BatchNorm1d(best_hidden_1),\n    nn.ReLU(),\n    nn.Dropout(best_dropout),\n    nn.Linear(best_hidden_1, best_hidden_2),\n    nn.BatchNorm1d(best_hidden_2),\n    nn.ReLU(),\n    nn.Dropout(best_dropout),\n    nn.Linear(best_hidden_2, 1),\n    nn.Sigmoid()\n).to(device)\n\n# Train with full epochs\ntuned_start = datetime.now()\noptimizer = optim.AdamW(tuned_model.parameters(), lr=best_lr)\n\ncorrect, total = 0, 0\nfor epoch in range(EPOCHS):  # Full 50 epochs\n    tuned_model.train()\n    epoch_loss = 0\n    batch_count = 0\n    \n    torch_dataset = data_connector.to_torch_dataset(batch_size=BATCH_SIZE, shuffle=True)\n    dataloader = DataLoader(torch_dataset, batch_size=None)\n    \n    for batch in dataloader:\n        y = batch.pop(label_col).float().squeeze().unsqueeze(1).to(device)\n        X = torch.stack([batch[col].float().squeeze() for col in feature_cols], dim=1).to(device)\n        \n        optimizer.zero_grad()\n        pred = tuned_model(X)\n        loss = criterion(pred, y)\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        batch_count += 1\n        \n        if epoch == EPOCHS - 1:\n            correct += ((pred >= 0.5).float() == y).sum().item()\n            total += y.size(0)\n    \n    if (epoch + 1) % 10 == 0:\n        print(f\"   Epoch {epoch+1}: loss={epoch_loss/batch_count:.4f}\")\n\ntuned_time = (datetime.now() - tuned_start).total_seconds()\ntuned_accuracy = correct / total if total > 0 else 0\ntuned_loss = epoch_loss / batch_count\n\nprint(f\"âœ… Tuned DNN: accuracy={tuned_accuracy:.4f}, time={tuned_time:.1f}s\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d607a8bb-710f-4332-8d29-ccc000791979",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "# ============================================================================\n# LOG TUNED MODEL TO EXPERIMENTS\n# ============================================================================\n\nwith exp.start_run(run_name=f\"DNN_tuned_{run_timestamp}\") as tuned_run:\n    # Log hyperparameters (from tuning)\n    exp.log_params({\n        \"model_type\": \"DNN_tuned\",\n        \"hidden_dim_1\": best_hidden_1,\n        \"hidden_dim_2\": best_hidden_2,\n        \"dropout\": best_dropout,\n        \"learning_rate\": best_lr,\n        \"epochs\": EPOCHS,\n        \"batch_size\": BATCH_SIZE,\n        \"tuning_trials\": tuner_config.num_trials\n    })\n    \n    # Log final metrics (from full training)\n    exp.log_metrics({\n        \"accuracy\": tuned_accuracy,\n        \"final_loss\": tuned_loss,\n        \"training_time_seconds\": tuned_time\n    })\n\nprint(f\"âœ… Tuned DNN logged to experiments: DNN_tuned_{run_timestamp}\")\nprint(f\"   Accuracy: {tuned_accuracy:.4f} (vs baseline DNN: {dnn_accuracy:.4f})\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9c94352-8135-4995-97d6-a85a6491451e",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "codeCollapsed": true,
    "collapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# COMPARE ALL 3 MODELS\n# ============================================================================\n\nprint(\"=\" * 70)\nprint(\"ðŸ“Š EXPERIMENT COMPARISON - All Models\")\nprint(\"=\" * 70)\nprint(f\"{'Model':<15} {'Accuracy':<12} {'Loss':<12} {'Time (s)':<10} {'Params':<12}\")\nprint(\"-\" * 70)\nprint(f\"{'DNN (baseline)':<15} {dnn_accuracy:<12.4f} {dnn_loss:<12.4f} {dnn_time:<10.1f} {sum(p.numel() for p in dnn_model.parameters()):,}\")\nprint(f\"{'CNN (baseline)':<15} {cnn_accuracy:<12.4f} {cnn_loss:<12.4f} {cnn_time:<10.1f} {sum(p.numel() for p in cnn_model.parameters()):,}\")\nprint(f\"{'DNN (tuned)':<15} {tuned_accuracy:<12.4f} {tuned_loss:<12.4f} {tuned_time:<10.1f} {sum(p.numel() for p in tuned_model.parameters()):,}\")\nprint(\"=\" * 70)\n\n# Determine winner from all 3 models\nall_models = [\n    (\"DNN_baseline\", dnn_model, dnn_accuracy, dnn_loss),\n    (\"CNN_baseline\", cnn_model, cnn_accuracy, cnn_loss),\n    (\"DNN_tuned\", tuned_model, tuned_accuracy, tuned_loss)\n]\n\n# Sort by accuracy (descending)\nall_models_sorted = sorted(all_models, key=lambda x: x[2], reverse=True)\n\nprint(\"\\nðŸ† RANKING:\")\nfor i, (name, model, acc, loss) in enumerate(all_models_sorted, 1):\n    medal = \"ðŸ¥‡\" if i == 1 else \"ðŸ¥ˆ\" if i == 2 else \"ðŸ¥‰\"\n    print(f\"   {medal} #{i}: {name} (accuracy={acc:.4f}, loss={loss:.4f})\")\n\n# Set winner for model registration\nwinner_name, winner_model, winner_accuracy, winner_loss = all_models_sorted[0]\n\nprint(f\"\\nðŸ“Š View all runs in Snowsight:\")\nprint(f\"   Database > WAFER_YIELD_DEMO > ML > Experiments > {EXPERIMENT_NAME}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "014e92c9-d1f6-4628-83f7-42424b9da97b",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "---\n## Section 6 â€” Register Champion Model to Model Registry\n\n### What is Snowflake Model Registry?\n\nThe **Snowflake Model Registry** is a centralized repository for managing ML models throughout their lifecycle:\n\n| Capability | Description |\n|------------|-------------|\n| **Model Versioning** | Track multiple versions of the same model |\n| **Metadata Storage** | Store metrics, parameters, and artifacts |\n| **Model Staging** | Manage model lifecycle (dev â†’ staging â†’ production) |\n| **Access Control** | RBAC for model access and deployment |\n| **Lineage Tracking** | Track which data/features were used to train |\n\n### The `registry.log_model()` Process\n\n```python\nregistry.log_model(\n    model=winner_model,           # PyTorch model object\n    model_name=\"WAFER_YIELD_...\", # Unique identifier\n    version_name=\"v1\",            # Version tag\n    comment=\"Description...\",     # Human-readable notes\n    metrics={...},                # Performance metrics\n    sample_input_data={...}       # For signature inference\n)\n```\n\n### What Gets Stored?\n\n| Component | Description |\n|-----------|-------------|\n| **Model Artifact** | Serialized model weights (PyTorch state_dict) |\n| **Model Signature** | Input/output schema inferred from sample_input_data |\n| **Metrics** | accuracy, loss, or any custom metrics |\n| **Metadata** | comment, creation time, user, version |\n| **Dependencies** | Python packages required for inference |\n\n### Model Lifecycle Stages\n\n```\nDevelopment â†’ Staging â†’ Production â†’ Archived\n     â”‚           â”‚           â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        Promote via set_alias() or tags\n```\n\n### Best Practices\n\n1. **Semantic Versioning**: Use `v1`, `v2`, or `v1.0.1` for versions\n2. **Include Metrics**: Always log accuracy, loss for comparison\n3. **Sample Input**: Provide sample data for signature inference\n4. **Descriptive Comments**: Document what changed in each version\n\n**Reference**: [Snowflake Model Registry](https://docs.snowflake.com/en/developer-guide/snowpark-ml/model-registry/overview)"
  },
  {
   "cell_type": "code",
   "id": "1bfb6dde-b403-4569-9b56-516553dab55f",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "# ============================================================================\n# REGISTER CHAMPION MODEL TO SNOWFLAKE MODEL REGISTRY\n# ============================================================================\nimport pandas as pd\n\nregistry = Registry(session=session)\n\n# Move model to CPU for registry (required for signature validation)\nwinner_model_cpu = winner_model.cpu()\nwinner_model_cpu.eval()\n\n# Create sample input as pandas DataFrame for signature inference (float32 to match model)\nimport numpy as np\nsample_input_df = pd.DataFrame({col: np.array([0.0], dtype=np.float32) for col in feature_cols})\n\n# Register the winning model (best from all 3: DNN baseline, CNN baseline, DNN tuned)\nmodel_name = f\"WAFER_YIELD_{winner_name.upper().replace(' ', '_')}\"\nmodel_version = registry.log_model(\n    model=winner_model_cpu,\n    model_name=model_name,\n    version_name=\"v1\",\n    comment=f\"Champion model ({winner_name}) - best from DNN/CNN/Tuned comparison\",\n    metrics={\n        \"accuracy\": float(winner_accuracy),\n        \"final_loss\": float(winner_loss),\n        \"total_params\": int(sum(p.numel() for p in winner_model_cpu.parameters()))\n    },\n    sample_input_data=sample_input_df\n)\n\nprint(f\"âœ… Registered Champion Model: {model_name} v1\")\nprint(f\"   Model: {winner_name}\")\nprint(f\"   Accuracy: {winner_accuracy:.4f}\")\nprint(f\"   Loss: {winner_loss:.4f}\")\nprint(f\"   Parameters: {sum(p.numel() for p in winner_model.parameters()):,}\")\nprint(f\"\\nðŸ“¦ View in Snowsight:\")\nprint(f\"   Database > {session.get_current_database()} > ML > Model Registry > {model_name}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ec10278-5024-4bec-8f0b-8bcbfaa1514c",
   "metadata": {
    "language": "python",
    "name": "cell26",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}