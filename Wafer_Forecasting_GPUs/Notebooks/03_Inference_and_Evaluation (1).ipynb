{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "lastEditStatus": {
   "notebookId": "hy5telo4jzypl6qymn43",
   "authorId": "3366391852320",
   "authorName": "HMASSA",
   "authorEmail": "haley.massa@snowflake.com",
   "sessionId": "d952fcac-6c07-4a43-98e0-171ea528b189",
   "lastEditTime": 1766464360011
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "eb1b7400-17ca-4c26-b3c6-80fc2ebc7d3b",
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": "# üìò Notebook 3: Batch Inference\n\n## Weekly Batch Inference for Wafer Yield Prediction\n\nThis notebook demonstrates **production-ready batch inference** using ML Jobs.\n\n**What this does:**\n1. Loads feature-engineered data from ML Dataset (feature store)\n2. Runs batch inference as an ML Job on GPU compute pool\n3. Saves predictions to table with metadata\n4. Ready to schedule weekly (Tasks, Airflow, etc.)\n\n**Key Pattern:** ML Jobs automatically manage compute pool lifecycle - no manual start/stop needed.\n\n---"
  },
  {
   "id": "7343cdee-913b-44a3-bc6e-1264856ce0e7",
   "cell_type": "code",
   "metadata": {
    "name": "cell2",
    "language": "python"
   },
   "source": "# ============================================================================\n# SETUP\n# ============================================================================\n\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.jobs import remote\nfrom snowflake.ml import dataset\n\nsession = get_active_session()\n\n# Set context\nsession.sql(\"USE DATABASE WAFER_YIELD_DEMO\").collect()\nsession.sql(\"USE SCHEMA RAW_DATA\").collect()\n\nprint(\"‚úÖ Setup complete\")\nprint(f\"   Database: {session.get_current_database()}\")\nprint(f\"   Schema: {session.get_current_schema()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "a2451882-cdf3-45e4-8e2f-fce2a9e26ef7",
   "cell_type": "code",
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "source": "# ============================================================================\n# PREPARE INFERENCE DATA FROM FEATURE STORE\n# ============================================================================\n\n# Load ML Dataset (feature-engineered data from Notebook 01)\nprint(\"üì¶ Loading ML Dataset (feature store)...\")\nds = dataset.load_dataset(\n    session, \n    \"WAFER_YIELD_DEMO.RAW_DATA.WAFER_YIELD_TRAINING_DATASET\", \n    version=\"v1\"\n)\ndf = ds.read.to_snowpark_dataframe()\n\nprint(f\"‚úÖ Loaded dataset: {df.count()} rows, {len(df.columns)} columns\")\n\n# Materialize to table for ML Job access\n# (ML Jobs can't access ML Datasets directly due to permissions)\nprint(\"\\nüìä Creating inference input table...\")\ndf.write.mode(\"overwrite\").save_as_table(\"WAFER_INFERENCE_INPUT\")\n\nprint(f\"‚úÖ Created: WAFER_INFERENCE_INPUT\")\nprint(f\"   Contains feature-engineered data for inference\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "4d1b471a-8444-45da-92db-ddc36ace79f7",
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": "---\n\n## Define Batch Inference Job\n\nThe `@remote()` decorator runs this function as an ML Job on the compute pool.\n\n**Key points:**\n- Compute pool auto-resumes when job starts\n- Compute pool auto-suspends after job completes\n- No manual pool management needed\n\n---"
  },
  {
   "id": "848db789-a49f-4265-b3b3-c7718c10e3db",
   "cell_type": "code",
   "metadata": {
    "name": "cell5",
    "language": "python"
   },
   "source": "# ============================================================================\n# BATCH INFERENCE JOB\n# ============================================================================\n\n@remote(\"WAFER_TRAINING_POOL\", stage_name=\"inference_payload\")\ndef run_weekly_inference(\n    database: str,\n    schema: str,\n    model_name: str,\n    input_table: str,\n    output_table: str\n):\n    \"\"\"\n    Weekly batch inference job.\n    \n    Runs as ML Job - compute pool lifecycle managed automatically.\n    Perfect for scheduling with Snowflake Tasks or Airflow.\n    \"\"\"\n    import torch\n    from snowflake.snowpark import Session\n    from snowflake.ml.registry import Registry\n    \n    session = Session.builder.getOrCreate()\n    \n    session.sql(f\"USE DATABASE {database}\").collect()\n    session.sql(f\"USE SCHEMA {schema}\").collect()\n    print(f\"üìã Context: {database}.{schema}\")\n    \n    registry = Registry(session=session)\n    mv = registry.get_model(model_name).default\n    print(f\"‚úÖ Model: {model_name} v{mv.version_name}\")\n    \n    input_df = session.table(input_table)\n    print(f\"‚úÖ Data: {input_table} ({input_df.count()} rows)\")\n    \n    exclude_cols = ['WAFER_ID', 'YIELD_GOOD', 'YIELD_SCORE', 'DOMINANT_DEFECT_TYPE']\n    feature_cols = [c for c in input_df.columns if c.upper() not in [x.upper() for x in exclude_cols]]\n    \n    print(f\"üìä Running inference on {len(feature_cols)} features...\")\n    \n    model_obj = mv.load()\n    model_obj.eval()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model_obj = model_obj.to(device)\n    print(f\"‚úÖ Model loaded to {device}\")\n    \n    input_pandas = input_df.select(feature_cols).to_pandas()\n    input_tensor = torch.FloatTensor(input_pandas.values).to(device)\n    \n    with torch.no_grad():\n        predictions_tensor = model_obj(input_tensor)\n    \n    predictions = predictions_tensor.cpu().numpy()\n    \n    from snowflake.snowpark.types import DoubleType, TimestampType, StringType\n    from snowflake.snowpark import Row\n    from datetime import datetime\n    \n    timestamp = datetime.now()\n    \n    rows = [\n        Row(\n            OUTPUT_FEATURE_0=float(pred[0]),\n            INFERENCE_TIMESTAMP=timestamp,\n            MODEL_VERSION=mv.version_name\n        )\n        for pred in predictions\n    ]\n    \n    predictions_df = session.create_dataframe(rows)\n    predictions_df.write.mode(\"overwrite\").save_as_table(output_table)\n    \n    result_count = session.table(output_table).count()\n    print(f\"‚úÖ Saved {result_count} predictions to {output_table}\")\n    \n    return result_count\n\nprint(\"‚úÖ Inference job function defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "16d8731a-264f-42cb-b2fb-a67e9c2b7ed9",
   "cell_type": "markdown",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "---\n\n## Run Batch Inference\n\nSubmit the job and wait for completion.\n\n---"
  },
  {
   "id": "e436966c-3be8-4c07-8486-fd1a8ae1b699",
   "cell_type": "code",
   "metadata": {
    "name": "cell7",
    "language": "python"
   },
   "source": "# ============================================================================\n# SUBMIT INFERENCE JOB\n# ============================================================================\n\nprint(\"üöÄ Submitting batch inference job...\")\n\n# Submit the job\njob = run_weekly_inference(\n    database=session.get_current_database(),\n    schema=session.get_current_schema(),\n    model_name=\"WAFER_YIELD_DDP_MODEL\",\n    input_table=\"WAFER_INFERENCE_INPUT\",\n    output_table=\"WAFER_YIELD_PREDICTIONS\"\n)\n\nprint(f\"\\n‚úÖ Job submitted: {job.id}\")\nprint(f\"üìä Status: {job.status}\")\nprint(f\"\\n‚è≥ Waiting for completion (first run may take 3-5 min for pool startup)...\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "6d7b006d-8f6b-4a9a-b9eb-55c8773ce330",
   "cell_type": "code",
   "metadata": {
    "name": "cell8",
    "language": "python",
    "codeCollapsed": false
   },
   "source": "# ============================================================================\n# GET RESULTS\n# ============================================================================\n\n# Wait for job to finish\nresult = job.result()\n\nprint(f\"\\n‚úÖ Job complete!\")\nprint(f\"   Generated {result} predictions\")\nprint(f\"   Saved to: WAFER_YIELD_PREDICTIONS\")\n\n# Show job logs\nprint(f\"\\nüìã Job logs:\")\nprint(\"=\" * 60)\njob.show_logs()\nprint(\"=\" * 60)",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "77c295a3-deab-40fb-8d13-0e13006a6afc",
   "cell_type": "code",
   "metadata": {
    "name": "cell9",
    "language": "python",
    "codeCollapsed": false
   },
   "source": "# ============================================================================\n# ANALYZE PREDICTIONS\n# ============================================================================\n\nprint(\"üìä Prediction Results\")\nprint(\"=\" * 60)\n\npredictions = session.table(\"WAFER_YIELD_PREDICTIONS\")\n\n# Sample predictions\nprint(\"\\n1Ô∏è‚É£ Sample predictions:\")\npredictions.select(\n    \"OUTPUT_FEATURE_0\",\n    \"INFERENCE_TIMESTAMP\",\n    \"MODEL_VERSION\"\n).show(10)\n\n# Statistics\ntotal = predictions.count()\nmodel_version = predictions.select('MODEL_VERSION').first()[0]\ntimestamp = predictions.select('INFERENCE_TIMESTAMP').first()[0]\n\nprint(f\"\\n2Ô∏è‚É£ Summary:\")\nprint(f\"   Total predictions: {total}\")\nprint(f\"   Model version: {model_version}\")\nprint(f\"   Timestamp: {timestamp}\")\n\n# Distribution\nprint(f\"\\n3Ô∏è‚É£ Prediction distribution:\")\npredictions.select(\"OUTPUT_FEATURE_0\").describe().show()\n\nprint(\"\\n‚úÖ Inference complete and verified!\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "ce39ceba-e5b7-404f-bd41-ecc8bc474bc6",
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "---\n\n## How to Schedule Weekly\n\nThis inference job can be scheduled to run automatically:\n\n### Option 1: Snowflake Tasks\n\n```sql\nCREATE TASK WEEKLY_INFERENCE_TASK\n    WAREHOUSE = COMPUTE_WH\n    SCHEDULE = 'USING CRON 0 2 * * 1 UTC'  -- Monday 2 AM\nAS\n    CALL RUN_INFERENCE_PROCEDURE();\n\nALTER TASK WEEKLY_INFERENCE_TASK RESUME;\n```\n\n### Option 2: Python Script + Cron\n\n```python\n# weekly_inference.py\nfrom snowflake.snowpark import Session\nfrom snowflake.ml.jobs import remote\n\nsession = Session.builder.getOrCreate()\n\n@remote(\"WAFER_TRAINING_POOL\", stage_name=\"inference_payload\")\ndef run_weekly_inference(...):\n    # ... (same as above)\n    pass\n\njob = run_weekly_inference(...)\nresult = job.result()\nprint(f\"Completed: {result} predictions\")\n```\n\n```bash\n# Cron: Every Monday at 2 AM\n0 2 * * 1 python weekly_inference.py\n```\n\n### Option 3: Airflow DAG\n\n```python\n# dags/wafer_inference.py\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndef run_inference():\n    # ... ML Job code ...\n    return job.result()\n\nwith DAG(\n    'wafer_weekly_inference',\n    schedule='0 2 * * 1',\n    start_date=datetime(2024, 1, 1),\n    catchup=False\n) as dag:\n    PythonOperator(\n        task_id='batch_inference',\n        python_callable=run_inference\n    )\n```\n\n**See Notebook 04 for detailed orchestration examples.**\n\n---"
  },
  {
   "id": "880e8acd-3c2d-4d10-b6ac-fdd3ebf422d1",
   "cell_type": "markdown",
   "metadata": {
    "name": "cell11"
   },
   "source": "---\n\n## Summary\n\n### What We Did\n\n1. ‚úÖ Loaded feature-engineered data from ML Dataset (feature store)\n2. ‚úÖ Materialized to table for ML Job access\n3. ‚úÖ Defined inference function with `@remote()` decorator\n4. ‚úÖ Submitted job to GPU compute pool\n5. ‚úÖ Compute pool auto-started, ran inference, auto-stopped\n6. ‚úÖ Saved predictions with metadata\n\n### Key Takeaways\n\n| Aspect | Implementation |\n|--------|----------------|\n| **Data Source** | ML Dataset (feature store) from Notebook 01 |\n| **Compute** | GPU compute pool (auto-managed by ML Jobs) |\n| **Pattern** | `@remote()` decorator for ML Jobs |\n| **Scheduling** | Ready for Tasks, Airflow, or cron |\n| **Cost** | Pool only runs during job execution |\n\n### Next Steps\n\n**Notebook 04: Production Orchestration**\n- Schedule weekly inference with Tasks\n- Build Airflow DAGs for complex workflows  \n- Add monitoring and alerting\n- Configure retries and error handling\n\n---"
  }
 ]
}