{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "7jd3k2yfrwgnic5qc5fl",
   "authorId": "3366391852320",
   "authorName": "HMASSA",
   "authorEmail": "haley.massa@snowflake.com",
   "sessionId": "a3fdd6ad-991d-473f-af79-69808ade847f",
   "lastEditTime": 1765404314910
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# ğŸ“˜ Notebook 1: Feature Engineering & ML Datasets\n",
    "\n",
    "## Section 1 â€” Introduction\n",
    "\n",
    "### Business Case: Wafer Yield Forecasting\n",
    "\n",
    "Semiconductor manufacturing is one of the most complex industrial processes, with **yield** being the ultimate measure of success. A single percentage point improvement in yield can translate to millions of dollars in savings for a fab.\n",
    "\n",
    "**Why Predict Yield?**\n",
    "- **Early Warning System**: Identify at-risk wafers before final test\n",
    "- **Process Optimization**: Correlate parameters with yield outcomes\n",
    "- **Equipment Maintenance**: Detect drift before it impacts production\n",
    "- **Cost Reduction**: Reduce scrap, rework, and warranty costs\n",
    "\n",
    "### What's Covered\n",
    "\n",
    "| Section | Topic |\n",
    "|---------|-------|\n",
    "| 2 | Load Data with Snowpark DataFrames |\n",
    "| 3 | Feature Engineering Pipelines |\n",
    "| 4 | Feature Store Registration |\n",
    "| 5 | ML Dataset Creation (handoff to Notebook 2) |\n",
    "| 6 | Best Practices |\n",
    "\n",
    "### Dataset Overview (from Notebook 0)\n",
    "\n",
    "| Table | Description | Key Features |\n",
    "|-------|-------------|--------------|\n",
    "| `WAFER_PROCESS_DATA` | Process telemetry (temp, pressure, gas flow) | Time-series profiles per step |\n",
    "| `WAFER_DEFECT_LOGS` | Defect inspection results | Count, type, severity |\n",
    "| `FINAL_YIELD_LABELS` | Binary yield outcomes | Target variable + root cause |\n",
    "\n",
    "### References\n",
    "\n",
    "- [Snowpark Python DataFrames](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes)\n",
    "- [Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowpark-ml/feature-store/overview)\n",
    "- [Snowflake ML Datasets](https://docs.snowflake.com/en/developer-guide/snowpark-ml/dataset)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell2",
    "language": "python"
   },
   "outputs": [],
   "source": "# ============================================================================\n# IMPORTS & SESSION SETUP\n# ============================================================================\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict\n\n# Snowpark imports\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import (\n    col, lit, avg, sum as sum_, max as max_, min as min_, \n    stddev, count, when, coalesce, array_size, get,\n    datediff, dayofweek, hour, to_variant, parse_json,\n    flatten, lag, lead, row_number\n)\nfrom snowflake.snowpark.types import (\n    StructType, StructField, StringType, IntegerType, \n    FloatType, TimestampType, VariantType\n)\nfrom snowflake.snowpark.window import Window\n\n# Snowflake ML Feature Store\nfrom snowflake.ml.feature_store import (\n    FeatureStore, \n    FeatureView, \n    Entity,\n    CreationMode\n)\n\n# Snowflake ML Dataset (for creating training data handoff)\nfrom snowflake.ml import dataset\n\nprint(\"âœ… Imports complete\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell3",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SNOWFLAKE SESSION (Snowflake Notebook)\n",
    "# ============================================================================\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Get the active session (automatically available in Snowflake Notebooks)\n",
    "session = get_active_session()\n",
    "\n",
    "# Set context\n",
    "session.sql(\"USE DATABASE WAFER_YIELD_DEMO\").collect()\n",
    "session.sql(\"USE SCHEMA RAW_DATA\").collect()\n",
    "\n",
    "print(f\"âœ… Session active\")\n",
    "print(f\"   Database: {session.get_current_database()}\")\n",
    "print(f\"   Schema: {session.get_current_schema()}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "---\n\n## ğŸ“˜ Section 2 â€” Load Data with Snowpark DataFrames\n\nWe use **Snowpark DataFrames** with lazy evaluation for feature engineering. Data stays in Snowflake until an action triggers computation.\n\n### Why Snowpark DataFrames for Feature Engineering?\n\n| Benefit | Description |\n|---------|-------------|\n| **Lazy Evaluation** | Transformations are optimized before execution |\n| **Pushdown** | All computation runs in Snowflake's engine |\n| **SQL + Python** | Seamlessly combine SQL and DataFrame APIs |\n| **Scalability** | Handles large datasets without memory issues |\n\n> **Note**: For ML training data loading, Notebook 2 uses **Data Connector** for efficient pandas/PyTorch conversion.",
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD SOURCE TABLES AS SNOWPARK DATAFRAMES\n",
    "# ============================================================================\n",
    "\n",
    "# Load tables as Snowpark DataFrames - no data is fetched yet (lazy evaluation)\n",
    "process_df = session.table(\"RAW_DATA.WAFER_PROCESS_DATA\")\n",
    "defect_df = session.table(\"RAW_DATA.WAFER_DEFECT_LOGS\")\n",
    "yield_df = session.table(\"RAW_DATA.FINAL_YIELD_LABELS\")\n",
    "\n",
    "print(\"ğŸ“‹ Snowpark DataFrames loaded (lazy evaluation - no data transferred yet)\")\n",
    "print(f\"   WAFER_PROCESS_DATA: {len(process_df.columns)} columns\")\n",
    "print(f\"   WAFER_DEFECT_LOGS: {len(defect_df.columns)} columns\")\n",
    "print(f\"   FINAL_YIELD_LABELS: {len(yield_df.columns)} columns\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA EXPLORATION (triggers query execution)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ“Š Table Row Counts:\")\n",
    "print(f\"   WAFER_PROCESS_DATA: {process_df.count():,} rows\")\n",
    "print(f\"   WAFER_DEFECT_LOGS: {defect_df.count():,} rows\")\n",
    "print(f\"   FINAL_YIELD_LABELS: {yield_df.count():,} rows\")\n",
    "\n",
    "# Preview data (limit reduces data transfer)\n",
    "print(\"\\nğŸ” Sample Yield Labels:\")\n",
    "yield_df.limit(5).show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "---\n\n## ğŸ“˜ Section 3 â€” Feature Engineering Pipelines\n\nWe create production-ready features using **Snowpark DataFrame transformations**. All computations run in Snowflake's compute layer with automatic pushdown optimization.\n\n### Feature Categories\n\n| Category | Features | Description |\n|----------|----------|-------------|\n| **Temperature** | Max, min, mean, std, range | Extracted from temperature profiles |\n| **Pressure** | Max, min, mean, std, delta | Extracted from pressure profiles |\n| **Process** | Gas flow, humidity stats | Process parameter aggregations |\n| **Defect** | Count, severity, density | Defect inspection metrics |\n| **Temporal** | Hour, day-of-week | Time-based patterns |",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell8",
    "language": "python"
   },
   "outputs": [],
   "source": "# ============================================================================\n# FEATURE ENGINEERING: TEMPERATURE FEATURES\n# ============================================================================\n# Snowpark DataFrame API - all operations are LAZY until an action is called\n\nfrom snowflake.snowpark.functions import (\n    col, avg, max as max_, min as min_, stddev, count, parse_json, lit\n)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# LAZY OPERATIONS (No execution yet - just building the query plan)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n# Flatten JSON arrays using table function\ntemp_flattened = (\n    process_df\n    .select(\"WAFER_ID\", \"TEMPERATURE_PROFILE\")                              # Lazy\n    .join_table_function(\"flatten\", parse_json(col(\"TEMPERATURE_PROFILE\"))) # Lazy\n    .select(col(\"WAFER_ID\"), col(\"VALUE\").cast(\"FLOAT\").alias(\"TEMP\"))      # Lazy\n)\n# At this point: NO DATA HAS BEEN PROCESSED - only a query plan exists\n\n# Aggregate temperature statistics per wafer\ntemp_features_df = (\n    temp_flattened\n    .group_by(\"WAFER_ID\")  # Lazy\n    .agg(                   # Lazy\n        max_(\"TEMP\").alias(\"TEMP_MAX\"),\n        min_(\"TEMP\").alias(\"TEMP_MIN\"),\n        avg(\"TEMP\").alias(\"TEMP_MEAN\"),\n        stddev(\"TEMP\").alias(\"TEMP_STD\"),\n        (max_(\"TEMP\") - min_(\"TEMP\")).alias(\"TEMP_RANGE\")\n    )\n)\n# Still no execution - temp_features_df is just a query plan\n\nprint(\"âœ… Temperature features DataFrame created (lazy - not yet executed)\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ACTION (Triggers execution of the entire query plan)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntemp_features_df.limit(5).show()  # NOW the query runs in Snowflake!\n",
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell9",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# FEATURE ENGINEERING: PRESSURE FEATURES\n# ============================================================================\n# All operations below are LAZY - building query plan only\n\n# Flatten and aggregate (LAZY - no execution)\npressure_flattened = (\n    process_df\n    .select(\"WAFER_ID\", \"PRESSURE_PROFILE\")                              # Lazy\n    .join_table_function(\"flatten\", parse_json(col(\"PRESSURE_PROFILE\"))) # Lazy\n    .select(col(\"WAFER_ID\"), col(\"VALUE\").cast(\"FLOAT\").alias(\"PRESSURE\")) # Lazy\n)\n\npressure_features_df = (\n    pressure_flattened\n    .group_by(\"WAFER_ID\")  # Lazy\n    .agg(                   # Lazy\n        max_(\"PRESSURE\").alias(\"PRESSURE_MAX\"),\n        min_(\"PRESSURE\").alias(\"PRESSURE_MIN\"),\n        avg(\"PRESSURE\").alias(\"PRESSURE_MEAN\"),\n        stddev(\"PRESSURE\").alias(\"PRESSURE_STD\"),\n        (max_(\"PRESSURE\") - min_(\"PRESSURE\")).alias(\"PRESSURE_DELTA_MAX\")\n    )\n)\n\nprint(\"âœ… Pressure features DataFrame created (lazy)\")\npressure_features_df.limit(5).show()  # ACTION - executes query\n",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell10",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# FEATURE ENGINEERING: PROCESS FEATURES\n# ============================================================================\n# Aggregate gas flow, humidity, and process step metrics per wafer\n\nprocess_features_df = (\n    process_df\n    .group_by(\"WAFER_ID\")\n    .agg(\n        # Gas flow statistics\n        avg(\"GAS_FLOW_RATE\").alias(\"GAS_FLOW_MEAN\"),\n        stddev(\"GAS_FLOW_RATE\").alias(\"GAS_FLOW_STD\"),\n        max_(\"GAS_FLOW_RATE\").alias(\"GAS_FLOW_MAX\"),\n        min_(\"GAS_FLOW_RATE\").alias(\"GAS_FLOW_MIN\"),\n        \n        # Humidity statistics\n        avg(\"AMBIENT_HUMIDITY\").alias(\"HUMIDITY_MEAN\"),\n        stddev(\"AMBIENT_HUMIDITY\").alias(\"HUMIDITY_STD\"),\n        max_(\"AMBIENT_HUMIDITY\").alias(\"HUMIDITY_MAX\"),\n        \n        # Process metadata\n        count(\"PROCESS_STEP\").alias(\"NUM_PROCESS_STEPS\"),\n        count(\"EQUIPMENT_ID\").alias(\"NUM_EQUIPMENT_USED\")\n    )\n)\n\nprint(\"âœ… Process features created\")\nprocess_features_df.limit(5).show()\n",
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell11",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# FEATURE ENGINEERING: DEFECT FEATURES\n# ============================================================================\n\ndefect_features_df = (\n    defect_df\n    .group_by(\"WAFER_ID\")\n    .agg(\n        # Defect count metrics\n        sum_(\"DEFECT_COUNT\").alias(\"TOTAL_DEFECT_COUNT\"),\n        avg(\"DEFECT_COUNT\").alias(\"AVG_DEFECT_COUNT\"),\n        max_(\"DEFECT_COUNT\").alias(\"MAX_DEFECT_COUNT\"),\n        count(\"*\").alias(\"NUM_INSPECTIONS\"),\n        \n        # Severity metrics\n        max_(\"SEVERITY_SCORE\").alias(\"MAX_SEVERITY\"),\n        avg(\"SEVERITY_SCORE\").alias(\"AVG_SEVERITY\"),\n        \n        # Critical defect count (severity > 7)\n        sum_(when(col(\"SEVERITY_SCORE\") > 7, 1).otherwise(0)).alias(\"CRITICAL_DEFECT_COUNT\")\n    )\n    # Calculate defect density\n    .with_column(\n        \"DEFECT_DENSITY\",\n        col(\"TOTAL_DEFECT_COUNT\") / col(\"NUM_INSPECTIONS\")\n    )\n)\n\nprint(\"âœ… Defect features created\")\ndefect_features_df.limit(5).show()\n",
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell12",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# FEATURE ENGINEERING: DOMINANT DEFECT TYPE\n# ============================================================================\n# Find the most common defect type per wafer using window functions\n\nfrom snowflake.snowpark.functions import sum as sum_, row_number\nfrom snowflake.snowpark.window import Window\n\n# Count defects by type per wafer\ndefect_type_counts = (\n    defect_df\n    .group_by(\"WAFER_ID\", \"DEFECT_TYPE\")\n    .agg(sum_(\"DEFECT_COUNT\").alias(\"TYPE_COUNT\"))\n)\n\n# Rank defect types within each wafer\nwindow_spec = Window.partition_by(\"WAFER_ID\").order_by(col(\"TYPE_COUNT\").desc())\n\ndominant_defect_df = (\n    defect_type_counts\n    .with_column(\"RANK\", row_number().over(window_spec))\n    .filter(col(\"RANK\") == 1)\n    .select(\"WAFER_ID\", col(\"DEFECT_TYPE\").alias(\"DOMINANT_DEFECT_TYPE\"))\n)\n\nprint(\"âœ… Dominant defect type features created\")\ndominant_defect_df.limit(5).show()",
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell13",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# FEATURE ENGINEERING: TEMPORAL FEATURES\n# ============================================================================\n# Extract time-based patterns using Snowpark DataFrame API\n\nfrom snowflake.snowpark.functions import hour, dayofweek, datediff\n\n# Get min/max timestamps per wafer and extract time features\ntemporal_features_df = (\n    process_df\n    .group_by(\"WAFER_ID\")\n    .agg(\n        min_(\"TIMESTAMP\").alias(\"FIRST_TIMESTAMP\"),\n        max_(\"TIMESTAMP\").alias(\"LAST_TIMESTAMP\")\n    )\n    .with_column(\"PROCESS_HOUR\", hour(col(\"FIRST_TIMESTAMP\")))\n    .with_column(\"PROCESS_DAY_OF_WEEK\", dayofweek(col(\"FIRST_TIMESTAMP\")))\n    .select(\"WAFER_ID\", \"PROCESS_HOUR\", \"PROCESS_DAY_OF_WEEK\")\n)\n\nprint(\"âœ… Temporal features created\")\ntemporal_features_df.limit(5).show()\n\n",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell14",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# JOIN ALL FEATURES INTO UNIFIED SNOWPARK DATAFRAME\n# ============================================================================\n# \n# KEY INSIGHT: All the DataFrames we created (temp_features_df, pressure_features_df, \n# process_features_df, etc.) are LAZY - they're just query plans, not materialized data.\n#\n# When we join them together, Snowflake's optimizer combines ALL the query plans\n# into a SINGLE optimized execution plan. This is the power of lazy evaluation!\n#\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n# Combine all lazy DataFrames (still LAZY - just building a bigger query plan)\nfeatures_df = (\n    yield_df\n    .select(\"WAFER_ID\", \"YIELD_GOOD\", \"YIELD_SCORE\")  # Lazy\n    \n    .join(temp_features_df, \"WAFER_ID\", \"left\")       # Lazy - incorporates temp query plan\n    .join(pressure_features_df, \"WAFER_ID\", \"left\")   # Lazy - incorporates pressure query plan\n    .join(process_features_df, \"WAFER_ID\", \"left\")    # Lazy - incorporates process query plan\n    .join(defect_features_df, \"WAFER_ID\", \"left\")     # Lazy - incorporates defect query plan\n    .join(dominant_defect_df, \"WAFER_ID\", \"left\")     # Lazy\n    .join(temporal_features_df, \"WAFER_ID\", \"left\")   # Lazy\n    \n    .na.fill({                                         # Lazy\n        \"TOTAL_DEFECT_COUNT\": 0,\n        \"AVG_DEFECT_COUNT\": 0,\n        \"MAX_SEVERITY\": 0,\n        \"AVG_SEVERITY\": 0,\n        \"CRITICAL_DEFECT_COUNT\": 0,\n        \"DEFECT_DENSITY\": 0\n    })\n)\n\nprint(\"âœ… All feature DataFrames joined (still lazy - no execution yet)\")\nprint(f\"   Total columns in query plan: {len(features_df.columns)}\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ACTION - This triggers execution of the ENTIRE combined query plan!\n# Snowflake optimizes and executes all joins + aggregations in ONE query\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(f\"\\nğŸš€ Executing combined query plan...\")\nprint(f\"   Total rows: {features_df.count():,}\")  # ACTION - triggers execution\nfeatures_df.limit(5).show()                        # ACTION - shows results\n",
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell15",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE STATISTICS (computed on Snowpark DataFrame)\n",
    "# ============================================================================\n",
    "\n",
    "# Compute statistics without materializing to a table\n",
    "stats_df = features_df.select(\n",
    "    count(\"*\").alias(\"TOTAL_WAFERS\"),\n",
    "    avg(\"YIELD_SCORE\").alias(\"AVG_YIELD_SCORE\"),\n",
    "    sum_(\"YIELD_GOOD\").alias(\"GOOD_WAFERS\"),\n",
    "    avg(\"TOTAL_DEFECT_COUNT\").alias(\"AVG_DEFECTS\"),\n",
    "    avg(\"TEMP_MAX\").alias(\"AVG_TEMP_MAX\")\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Feature Statistics:\")\n",
    "stats_df.show()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": "---\n\n## ğŸ“˜ Section 4 â€” Register Feature Pipelines in Snowflake Feature Store\n\nThe **Snowflake Feature Store** provides:\n- **Centralized feature repository** with metadata\n- **Feature versioning** and lineage tracking\n- **Point-in-time correct** feature retrieval\n- **Feature reuse** across models and teams",
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell17",
    "language": "python"
   },
   "outputs": [],
   "source": "# ============================================================================\n# INITIALIZE FEATURE STORE\n# ============================================================================\n\n# Create Feature Store instance\n# CreationMode.CREATE_IF_NOT_EXIST creates the feature store schema if it doesn't exist\nfs = FeatureStore(\n    session=session,\n    database=\"WAFER_YIELD_DEMO\",\n    name=\"FEATURES\",\n    default_warehouse=\"WAFER_DEMO_WH\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)\n\nprint(\"âœ… Feature Store initialized\")\nprint(f\"   Database: WAFER_YIELD_DEMO\")\nprint(f\"   Schema: FEATURES\")\n\n\n",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell18",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# CREATE ENTITY\n# ============================================================================\n# An Entity represents the primary key/join key for features\n\nwafer_entity = Entity(\n    name=\"WAFER_ENTITY\",\n    join_keys=[\"WAFER_ID\"],\n    desc=\"Wafer identifier for semiconductor manufacturing\"\n)\n\n# Register entity (use FAIL_IF_NOT_EXIST for safety, CREATE_IF_NOT_EXIST for convenience)\nfs.register_entity(wafer_entity)\n\nprint(\"âœ… Wafer entity registered\")",
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell19",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# CREATE FEATURE VIEW\n# ============================================================================\n\n# Select features (excluding target and metadata columns) from the features_df\nfeature_columns = [\n    \"WAFER_ID\",\n    # Temperature features\n    \"TEMP_MAX\", \"TEMP_MIN\", \"TEMP_MEAN\", \"TEMP_STD\", \"TEMP_RANGE\",\n    # Pressure features  \n    \"PRESSURE_MAX\", \"PRESSURE_MIN\", \"PRESSURE_MEAN\", \"PRESSURE_STD\", \"PRESSURE_DELTA_MAX\",\n    # Process features\n    \"GAS_FLOW_MEAN\", \"GAS_FLOW_STD\", \"HUMIDITY_MEAN\", \"HUMIDITY_STD\",\n    # Defect features\n    \"TOTAL_DEFECT_COUNT\", \"AVG_DEFECT_COUNT\", \"MAX_SEVERITY\", \"AVG_SEVERITY\",\n    \"CRITICAL_DEFECT_COUNT\", \"DEFECT_DENSITY\",\n    # Temporal features\n    \"PROCESS_HOUR\", \"PROCESS_DAY_OF_WEEK\"\n]\n\n# Filter to available columns from our Snowpark DataFrame\navailable_cols = [c for c in feature_columns if c in features_df.columns]\nfeature_source_df = features_df.select(available_cols)\n\n# Create Feature View\nwafer_fv = FeatureView(\n    name=\"WAFER_YIELD_FEATURES\",\n    entities=[wafer_entity],\n    feature_df=feature_source_df,\n    desc=\"Wafer yield prediction features including process, defect, and temporal metrics\"\n)\n\n# Register with versioning\nregistered_fv = fs.register_feature_view(\n    feature_view=wafer_fv,\n    version=\"1.0\",\n    block=True  # Wait for registration to complete\n)\n\nprint(\"âœ… Feature View registered\")\nprint(f\"   Name: {registered_fv.name}\")\nprint(f\"   Version: {registered_fv.version}\")",
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell20",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# FEATURE METADATA (for documentation)\n# ============================================================================\n\n# Feature definitions for lineage tracking\nfeature_metadata = [\n    (\"TEMP_MAX\", \"Maximum temperature across all process steps\"),\n    (\"TEMP_MEAN\", \"Average temperature across all readings\"),\n    (\"TEMP_STD\", \"Temperature volatility measure\"),\n    (\"PRESSURE_DELTA_MAX\", \"Maximum pressure variation\"),\n    (\"TOTAL_DEFECT_COUNT\", \"Total defects found on wafer\"),\n    (\"MAX_SEVERITY\", \"Highest defect severity score\"),\n    (\"CRITICAL_DEFECT_COUNT\", \"Number of critical defects (severity > 7)\"),\n    (\"PROCESS_HOUR\", \"Hour when processing started\"),\n]\n\nprint(\"ğŸ“‹ Feature Definitions:\")\nfor feature_name, description in feature_metadata:\n    print(f\"   - {feature_name}: {description}\")\n\n",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell21",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# RETRIEVE TRAINING DATA FROM FEATURE STORE\n# ============================================================================\n\n# Get the registered feature view\nfv = fs.get_feature_view(\"WAFER_YIELD_FEATURES\", \"1.0\")\n\n# Get training data with labels\n# Join with yield labels\nlabels_df = session.table(\"RAW_DATA.FINAL_YIELD_LABELS\").select(\"WAFER_ID\", \"YIELD_GOOD\", \"YIELD_SCORE\")\n\ntraining_data = fv.feature_df.join(labels_df, \"WAFER_ID\")\n\nprint(f\"âœ… Training data retrieved: {training_data.count():,} rows\")\nprint(f\"   Features: {len(training_data.columns) - 3}\")  # Exclude WAFER_ID, YIELD_GOOD, YIELD_SCORE\ntraining_data.limit(5).show()\n\n# Store reference for Dataset creation\nTRAINING_DATA_DF = training_data\n\n",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ“˜ Section 5 â€” ML Dataset Creation (Handoff to Notebook 2)\n",
    "\n",
    "Create a **versioned ML Dataset** as the handoff artifact to Notebook 2 for model training.\n",
    "\n",
    "### Why Create an ML Dataset?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Immutable** | Training data is frozen for reproducibility |\n",
    "| **Versioned** | Track which data was used for each experiment |\n",
    "| **Efficient** | Notebook 2 loads via Data Connector |\n",
    "| **Decoupled** | Data engineering â†” Data science separation |\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell23",
    "language": "python"
   },
   "outputs": [],
   "source": "# ============================================================================\n# CREATE ML DATASET\n# ============================================================================\n# Reference: https://docs.snowflake.com/en/developer-guide/snowflake-ml/dataset\n\n# Create a versioned dataset from our training data\n# This creates an immutable snapshot for reproducible ML experiments\n\ndataset_name = \"WAFER_YIELD_TRAINING_DATASET\"\ndataset_version = \"v2\"\n\n# Create dataset using create_from_dataframe\n# This materializes the DataFrame into a Dataset version\nwafer_dataset = dataset.create_from_dataframe(\n    session,\n    dataset_name,\n    dataset_version,\n    input_dataframe=TRAINING_DATA_DF\n)\n\nprint(f\"âœ… ML Dataset created: {dataset_name}\")\nprint(f\"   Version: {dataset_version}\")\nprint(f\"   Rows: {wafer_dataset.read.to_snowpark_dataframe().count():,}\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell24",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# VERIFY DATASET CREATION\n# ============================================================================\n\n# List versions in this dataset\nprint(f\"ğŸ“¦ Dataset: {dataset_name}\")\nprint(f\"   Versions: {wafer_dataset.list_versions()}\")\nprint(f\"   Selected version: {wafer_dataset.selected_version.name}\")\n\n# Verify the dataset can be read\nprint(f\"\\nâœ… Dataset ready for Notebook 2\")\nprint(f\"   Load command: dataset.load_dataset(session, '{dataset_name}', '{dataset_version}')\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell25",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# ============================================================================\n# DATASET SUMMARY\n# ============================================================================\n\n# Quick stats from the Snowpark DataFrame (before dataset creation)\nprint(f\"ğŸ“Š Training Dataset Summary:\")\nprint(f\"   Total rows: {TRAINING_DATA_DF.count():,}\")\nprint(f\"   Total columns: {len(TRAINING_DATA_DF.columns)}\")\nprint(f\"\\nğŸ“‹ Columns included in ML Dataset:\")\nfor i, col in enumerate(TRAINING_DATA_DF.columns):\n    print(f\"   {i+1}. {col}\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell26",
    "collapsed": false
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ“˜ Section 6 â€” Best Practices\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```\n",
    "Notebook 1                                              Notebook 2\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Raw Tables â†’ Snowpark Transformations â†’ Feature Store â†’ ML Dataset â†’ Data Connector â†’ Training\n",
    "                                                              â”‚\n",
    "                                                        (handoff point)\n",
    "```\n",
    "\n",
    "### When to Use Each Component\n",
    "\n",
    "| Component | Where Used | Best For |\n",
    "|-----------|------------|----------|\n",
    "| **Snowpark DataFrame** | Notebook 1 | Feature engineering, transformations |\n",
    "| **Feature Store** | Notebook 1 | Feature reuse across models |\n",
    "| **ML Dataset** | Notebook 1 â†’ 2 | Versioned training data handoff |\n",
    "| **Data Connector** | Notebook 2 | Efficient pandas/PyTorch loading |\n",
    "\n",
    "### Reproducibility Best Practices\n",
    "\n",
    "1. **Version Everything**: Feature views, datasets, and models\n",
    "2. **Use ML Datasets**: Immutable snapshots for experiments\n",
    "3. **Document Transformations**: Store logic in feature metadata\n",
    "4. **Use Point-in-Time Joins**: Prevent data leakage\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell27",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTEBOOK SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ‰ NOTEBOOK 1 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“Š Features Created:\")\n",
    "print(f\"   - Temperature features: 5 (max, min, mean, std, range)\")\n",
    "print(f\"   - Pressure features: 5 (max, min, mean, std, delta)\")\n",
    "print(f\"   - Process features: 4 (gas flow, humidity)\")\n",
    "print(f\"   - Defect features: 6 (count, severity, density)\")\n",
    "print(f\"   - Temporal features: 2 (hour, day-of-week)\")\n",
    "print(f\"   - Total: ~22 features\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Artifacts Created:\")\n",
    "print(f\"   - Feature Table: FEATURES.WAFER_FEATURES_MASTER\")\n",
    "print(f\"   - Feature View: WAFER_YIELD_FEATURES v1.0\")\n",
    "print(f\"   - ML Dataset: {dataset_name} {dataset_version}\")\n",
    "print(f\"   - Entity: WAFER_ENTITY\")\n",
    "\n",
    "print(\"\\nâ¡ï¸ Next: Notebook 2\")\n",
    "print(\"   Loads ML Dataset via Data Connector for model training\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "code",
   "id": "5af6bdd7-423e-474a-85ba-dd0ec3524dbe",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce700b17-3daa-48e0-a679-0d793a2e2c6f",
   "metadata": {
    "language": "sql",
    "name": "cell29"
   },
   "outputs": [],
   "source": "-- List all datasets in the database\nSHOW DATASETS IN DATABASE WAFER_YIELD_DEMO;\n\n-- Or in a specific schema\nSHOW DATASETS IN SCHEMA WAFER_YIELD_DEMO.RAW_DATA;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af2a0334-d9fd-4095-876a-937add64d3ac",
   "metadata": {
    "language": "sql",
    "name": "cell30"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}