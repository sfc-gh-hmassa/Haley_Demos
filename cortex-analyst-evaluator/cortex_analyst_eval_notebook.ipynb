{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Cortex Analyst Semantic Model Evaluator\n",
    "\n",
    "This notebook benchmarks your semantic model by:\n",
    "1. Reading test questions from a table\n",
    "2. Calling Cortex Analyst to generate SQL\n",
    "3. Comparing results against expected SQL\n",
    "4. Storing results for tracking over time\n",
    "\n",
    "**Run this notebook each time you update your semantic model to track accuracy improvements.**\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Approach: Result Set Matching\n",
    "\n",
    "This evaluator uses **Execution Accuracy** â€” we execute both the expected SQL and the generated SQL, then compare the actual result sets.\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "| Approach | How It Works | Pros | Cons |\n",
    "|----------|--------------|------|------|\n",
    "| **Result Set Match** (this notebook) | Execute both SQLs, compare output rows | Ground truth â€” if results match, the answer is correct | Requires executable expected SQL |\n",
    "| **SQL String Match** | Compare SQL text directly | Fast, no execution needed | Different SQL can produce identical results |\n",
    "| **LLM-as-Judge** | Ask an LLM to evaluate if SQL is semantically correct | Handles ambiguity, no expected SQL needed | Subjective, can hallucinate, adds cost |\n",
    "\n",
    "**We chose Result Set Matching because:**\n",
    "- It's the most objective measure for text-to-SQL accuracy\n",
    "- Cortex Analyst may generate different (but equivalent) SQL than your expected answer\n",
    "- The business only cares if they get the right numbers, not how the SQL looks\n",
    "\n",
    "### How Comparison Works\n",
    "\n",
    "```\n",
    "Expected SQL â†’ Execute â†’ Result Set A â”€â”\n",
    "                                        â”œâ”€â†’ Compare Values â†’ PASS/FAIL\n",
    "Generated SQL â†’ Execute â†’ Result Set B â”€â”˜\n",
    "```\n",
    "\n",
    "The comparison:\n",
    "- Ignores column name differences (e.g., `total_revenue` vs `TOTAL_REVENUE`)\n",
    "- Ignores row ordering (unless your expected SQL has `ORDER BY`)\n",
    "- Compares actual values as strings (handles numeric precision)\n",
    "\n",
    "### When to Consider LLM-as-Judge\n",
    "\n",
    "Use TruLens or similar LLM evaluation when:\n",
    "- You want to evaluate natural language explanations (not just SQL)\n",
    "- Expected SQL is hard to define or maintain\n",
    "- You need metrics like \"groundedness\" or \"relevance\"\n",
    "\n",
    "For semantic model development, Result Set Matching is recommended because it directly measures what matters: **does the user get the right answer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these values for your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:50.733285Z",
     "iopub.status.busy": "2026-01-08T20:06:50.733060Z",
     "iopub.status.idle": "2026-01-08T20:06:50.739665Z",
     "shell.execute_reply": "2026-01-08T20:06:50.739156Z"
    }
   },
   "outputs": [],
   "source": [
    "# === CONFIGURATION - UPDATE THESE VALUES ===\n",
    "\n",
    "# Path to your semantic model YAML file\n",
    "SEMANTIC_MODEL_PATH = \"@CORTEX_AGENTS_DEMO.MAIN.SEMANTIC_MODELS/sales_orders.yaml\"\n",
    "\n",
    "# Database and schema where evaluation tables are stored\n",
    "EVAL_DATABASE = \"CORTEX_AGENTS_DEMO\"\n",
    "EVAL_SCHEMA = \"CORTEX_ANALYST_EVAL\"\n",
    "\n",
    "# Optional: filter questions by category\n",
    "QUESTION_CATEGORY = None  # Set to a string like \"Aggregation\" to filter, or None for all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:50.742783Z",
     "iopub.status.busy": "2026-01-08T20:06:50.742579Z",
     "iopub.status.idle": "2026-01-08T20:06:50.906398Z",
     "shell.execute_reply": "2026-01-08T20:06:50.906183Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_snowflake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_snowflake\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get Snowpark session\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_snowflake'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import _snowflake\n",
    "import streamlit as st\n",
    "\n",
    "# Get Snowpark session\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Generate unique run ID\n",
    "RUN_ID = str(uuid.uuid4())[:8]\n",
    "RUN_TIMESTAMP = datetime.now()\n",
    "\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Timestamp: {RUN_TIMESTAMP}\")\n",
    "print(f\"Semantic Model: {SEMANTIC_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "functions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:50.907779Z",
     "iopub.status.busy": "2026-01-08T20:06:50.907709Z",
     "iopub.status.idle": "2026-01-08T20:06:50.910959Z",
     "shell.execute_reply": "2026-01-08T20:06:50.910757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def call_cortex_analyst(question: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Call Cortex Analyst API and return (generated_sql, error).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = _snowflake.send_snow_api_request(\n",
    "            \"POST\",\n",
    "            \"/api/v2/cortex/analyst/message\",\n",
    "            {},  # headers\n",
    "            {},  # params\n",
    "            {\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": question}]}],\n",
    "                \"semantic_model_file\": SEMANTIC_MODEL_PATH\n",
    "            },\n",
    "            {},  # request_guid\n",
    "            60000  # timeout_ms\n",
    "        )\n",
    "        \n",
    "        resp_json = json.loads(response[\"content\"])\n",
    "        \n",
    "        if \"message\" in resp_json and \"content\" in resp_json[\"message\"]:\n",
    "            for item in resp_json[\"message\"][\"content\"]:\n",
    "                if item.get(\"type\") == \"sql\":\n",
    "                    return item.get(\"statement\", \"\"), None\n",
    "        \n",
    "        return \"\", \"No SQL in response\"\n",
    "    except Exception as e:\n",
    "        return \"\", str(e)\n",
    "\n",
    "\n",
    "def execute_sql_safe(sql: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Execute SQL and return (results_as_list, error).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sql = sql.strip().rstrip(';')\n",
    "        df = session.sql(sql).limit(100).collect()\n",
    "        results = [dict(row.asDict()) for row in df]\n",
    "        return results, None\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "\n",
    "def compare_results(result1, result2) -> bool:\n",
    "    \"\"\"\n",
    "    Compare two result sets (ignoring column name differences, checking values).\n",
    "    \"\"\"\n",
    "    if result1 is None or result2 is None:\n",
    "        return False\n",
    "    \n",
    "    if len(result1) != len(result2):\n",
    "        return False\n",
    "    \n",
    "    def to_value_set(results):\n",
    "        value_set = set()\n",
    "        for row in results:\n",
    "            values = tuple(sorted([str(v) for v in row.values()]))\n",
    "            value_set.add(values)\n",
    "        return value_set\n",
    "    \n",
    "    return to_value_set(result1) == to_value_set(result2)\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-questions",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:50.912118Z",
     "iopub.status.busy": "2026-01-08T20:06:50.912050Z",
     "iopub.status.idle": "2026-01-08T20:06:50.918509Z",
     "shell.execute_reply": "2026-01-08T20:06:50.918317Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     query \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AND category = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mQUESTION_CATEGORY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m query \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ORDER BY question_id\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m questions_df \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39msql(query)\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(questions_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m questions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m questions_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'session' is not defined"
     ]
    }
   ],
   "source": [
    "# Load questions from table\n",
    "query = f\"\"\"\n",
    "SELECT question_id, question, expected_sql, description, category\n",
    "FROM {EVAL_DATABASE}.{EVAL_SCHEMA}.EVAL_QUESTIONS\n",
    "WHERE active = TRUE\n",
    "\"\"\"\n",
    "\n",
    "if QUESTION_CATEGORY:\n",
    "    query += f\" AND category = '{QUESTION_CATEGORY}'\"\n",
    "\n",
    "query += \" ORDER BY question_id\"\n",
    "\n",
    "questions_df = session.sql(query).to_pandas()\n",
    "\n",
    "print(f\"Loaded {len(questions_df)} questions\")\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "This cell calls Cortex Analyst for each question and compares results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "run-eval",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:50.919640Z",
     "iopub.status.busy": "2026-01-08T20:06:50.919579Z",
     "iopub.status.idle": "2026-01-08T20:06:50.931252Z",
     "shell.execute_reply": "2026-01-08T20:06:50.931066Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mprogress(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m status_text \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mempty()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m questions_df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "progress_bar = st.progress(0)\n",
    "status_text = st.empty()\n",
    "\n",
    "for idx, row in questions_df.iterrows():\n",
    "    question_id = row['QUESTION_ID']\n",
    "    question = row['QUESTION']\n",
    "    expected_sql = row['EXPECTED_SQL']\n",
    "    description = row['DESCRIPTION']\n",
    "    \n",
    "    status_text.text(f\"[{idx+1}/{len(questions_df)}] {question[:50]}...\")\n",
    "    progress_bar.progress((idx + 1) / len(questions_df))\n",
    "    \n",
    "    # Call Cortex Analyst\n",
    "    generated_sql, gen_error = call_cortex_analyst(question)\n",
    "    \n",
    "    if gen_error:\n",
    "        results.append({\n",
    "            'question_id': question_id,\n",
    "            'question': question,\n",
    "            'description': description,\n",
    "            'expected_sql': expected_sql,\n",
    "            'generated_sql': '',\n",
    "            'expected_result': None,\n",
    "            'generated_result': None,\n",
    "            'results_match': False,\n",
    "            'error': f\"Analyst error: {gen_error}\"\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Execute expected SQL\n",
    "    expected_result, exp_error = execute_sql_safe(expected_sql)\n",
    "    if exp_error:\n",
    "        results.append({\n",
    "            'question_id': question_id,\n",
    "            'question': question,\n",
    "            'description': description,\n",
    "            'expected_sql': expected_sql,\n",
    "            'generated_sql': generated_sql,\n",
    "            'expected_result': None,\n",
    "            'generated_result': None,\n",
    "            'results_match': False,\n",
    "            'error': f\"Expected SQL error: {exp_error}\"\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Execute generated SQL\n",
    "    generated_result, gen_result_error = execute_sql_safe(generated_sql)\n",
    "    if gen_result_error:\n",
    "        results.append({\n",
    "            'question_id': question_id,\n",
    "            'question': question,\n",
    "            'description': description,\n",
    "            'expected_sql': expected_sql,\n",
    "            'generated_sql': generated_sql,\n",
    "            'expected_result': expected_result,\n",
    "            'generated_result': None,\n",
    "            'results_match': False,\n",
    "            'error': f\"Generated SQL error: {gen_result_error}\"\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Compare results\n",
    "    match = compare_results(expected_result, generated_result)\n",
    "    \n",
    "    results.append({\n",
    "        'question_id': question_id,\n",
    "        'question': question,\n",
    "        'description': description,\n",
    "        'expected_sql': expected_sql,\n",
    "        'generated_sql': generated_sql,\n",
    "        'expected_result': expected_result,\n",
    "        'generated_result': generated_result,\n",
    "        'results_match': match,\n",
    "        'error': None\n",
    "    })\n",
    "\n",
    "status_text.text(\"âœ… Evaluation complete!\")\n",
    "print(f\"\\nProcessed {len(results)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "summary",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:50.932355Z",
     "iopub.status.busy": "2026-01-08T20:06:50.932280Z",
     "iopub.status.idle": "2026-01-08T20:06:50.941010Z",
     "shell.execute_reply": "2026-01-08T20:06:50.940843Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m (passed \u001b[38;5;241m/\u001b[39m total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m total \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Display summary metrics\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m col1, col2, col3, col4 \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mcolumns(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      9\u001b[0m col1\u001b[38;5;241m.\u001b[39mmetric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m, total)\n\u001b[1;32m     10\u001b[0m col2\u001b[38;5;241m.\u001b[39mmetric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed\u001b[39m\u001b[38;5;124m\"\u001b[39m, passed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "passed = sum(1 for r in results if r['results_match'])\n",
    "failed = sum(1 for r in results if not r['results_match'] and not r['error'])\n",
    "errors = sum(1 for r in results if r['error'])\n",
    "total = len(results)\n",
    "accuracy = (passed / total * 100) if total > 0 else 0\n",
    "\n",
    "# Display summary metrics\n",
    "col1, col2, col3, col4 = st.columns(4)\n",
    "col1.metric(\"Total\", total)\n",
    "col2.metric(\"Passed\", passed)\n",
    "col3.metric(\"Failed\", failed)\n",
    "col4.metric(\"Accuracy\", f\"{accuracy:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Run ID: {RUN_ID}\")\n",
    "print(f\"Accuracy: {accuracy:.1f}% ({passed}/{total})\")\n",
    "print(f\"Passed: {passed} | Failed: {failed} | Errors: {errors}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "details-header",
   "metadata": {},
   "source": [
    "## Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "details",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:50.942108Z",
     "iopub.status.busy": "2026-01-08T20:06:50.942046Z",
     "iopub.status.idle": "2026-01-08T20:06:51.879387Z",
     "shell.execute_reply": "2026-01-08T20:06:51.879152Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create results dataframe\u001b[39;00m\n\u001b[1;32m      4\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mâœ… PASS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_match\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mâŒ ERROR\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mâŒ FAIL\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m: r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m } \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[0;32m---> 11\u001b[0m st\u001b[38;5;241m.\u001b[39mdataframe(results_df, use_container_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame([{\n",
    "    'Question': r['question'],\n",
    "    'Status': 'âœ… PASS' if r['results_match'] else ('âŒ ERROR' if r['error'] else 'âŒ FAIL'),\n",
    "    'Description': r['description'],\n",
    "    'Error': r['error'] or ''\n",
    "} for r in results])\n",
    "\n",
    "st.dataframe(results_df, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failures-header",
   "metadata": {},
   "source": [
    "## Failed Questions Detail\n",
    "\n",
    "Review the questions that didn't pass to identify improvements needed in your semantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "failures",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:51.880825Z",
     "iopub.status.busy": "2026-01-08T20:06:51.880748Z",
     "iopub.status.idle": "2026-01-08T20:06:51.893947Z",
     "shell.execute_reply": "2026-01-08T20:06:51.893739Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m failures \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_match\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m failures:\n\u001b[0;32m----> 4\u001b[0m     st\u001b[38;5;241m.\u001b[39msuccess(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸŽ‰ All questions passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(failures, \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "failures = [r for r in results if not r['results_match']]\n",
    "\n",
    "if not failures:\n",
    "    st.success(\"ðŸŽ‰ All questions passed!\")\n",
    "else:\n",
    "    for i, f in enumerate(failures, 1):\n",
    "        with st.expander(f\"âŒ {i}. {f['question']}\", expanded=True):\n",
    "            st.write(f\"**Description:** {f['description']}\")\n",
    "            \n",
    "            if f['error']:\n",
    "                st.error(f['error'])\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                st.write(\"**Expected SQL:**\")\n",
    "                st.code(f['expected_sql'], language='sql')\n",
    "                if f['expected_result']:\n",
    "                    st.write(\"**Expected Result:**\")\n",
    "                    st.json(f['expected_result'][:5])  # Show first 5 rows\n",
    "            \n",
    "            with col2:\n",
    "                st.write(\"**Generated SQL:**\")\n",
    "                st.code(f['generated_sql'] or '(none)', language='sql')\n",
    "                if f['generated_result']:\n",
    "                    st.write(\"**Generated Result:**\")\n",
    "                    st.json(f['generated_result'][:5])  # Show first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## Save Results to Table\n",
    "\n",
    "Store this run's results for historical tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "save-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:51.895119Z",
     "iopub.status.busy": "2026-01-08T20:06:51.895053Z",
     "iopub.status.idle": "2026-01-08T20:06:51.923791Z",
     "shell.execute_reply": "2026-01-08T20:06:51.923574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results saved to CORTEX_AGENTS_DEMO.CORTEX_ANALYST_EVAL.EVAL_RESULTS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RUN_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m      3\u001b[0m     session\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m        INSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_DATABASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_SCHEMA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.EVAL_RESULTS \u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m        (run_id, run_timestamp, semantic_model, question_id, question, \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNULL\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Results saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_DATABASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_SCHEMA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.EVAL_RESULTS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Run ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RUN_ID' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "\n",
    "def json_serial(obj):\n",
    "    if isinstance(obj, (datetime, date)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "for r in results:\n",
    "    expected_json = json.dumps(r['expected_result'], default=json_serial) if r['expected_result'] else 'null'\n",
    "    generated_json = json.dumps(r['generated_result'], default=json_serial) if r['generated_result'] else 'null'\n",
    "    error_val = f\"'{r['error']}'\" if r['error'] else 'NULL'\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO {EVAL_DATABASE}.{EVAL_SCHEMA}.EVAL_RESULTS \n",
    "    (run_id, run_timestamp, semantic_model, question_id, question, \n",
    "     expected_sql, generated_sql, expected_result, generated_result, \n",
    "     results_match, execution_error)\n",
    "    SELECT \n",
    "        '{RUN_ID}',\n",
    "        '{RUN_TIMESTAMP}'::TIMESTAMP,\n",
    "        '{SEMANTIC_MODEL_PATH}',\n",
    "        {r['question_id']},\n",
    "        $${r['question']}$$,\n",
    "        $${r['expected_sql']}$$,\n",
    "        $${r['generated_sql']}$$,\n",
    "        PARSE_JSON($${expected_json}$$),\n",
    "        PARSE_JSON($${generated_json}$$),\n",
    "        {r['results_match']},\n",
    "        {error_val}\n",
    "    \"\"\"\n",
    "    session.sql(sql).collect()\n",
    "\n",
    "print(f\"Results saved to {EVAL_DATABASE}.{EVAL_SCHEMA}.EVAL_RESULTS\")\n",
    "print(f\"Run ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-header",
   "metadata": {},
   "source": [
    "## Historical Accuracy Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "history",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T20:06:51.924961Z",
     "iopub.status.busy": "2026-01-08T20:06:51.924895Z",
     "iopub.status.idle": "2026-01-08T20:06:51.932012Z",
     "shell.execute_reply": "2026-01-08T20:06:51.931831Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show historical runs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history_df \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m        run_timestamp,\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m        run_id,\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m        accuracy_pct,\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m        passed,\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m        failed,\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m        errors,\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m        total_questions\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_DATABASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_SCHEMA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.EVAL_RUN_SUMMARY\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    ORDER BY run_timestamp DESC\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    LIMIT 10\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history_df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     17\u001b[0m     st\u001b[38;5;241m.\u001b[39msubheader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy Over Time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'session' is not defined"
     ]
    }
   ],
   "source": [
    "# Show historical runs\n",
    "history_df = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        run_timestamp,\n",
    "        run_id,\n",
    "        accuracy_pct,\n",
    "        passed,\n",
    "        failed,\n",
    "        errors,\n",
    "        total_questions\n",
    "    FROM {EVAL_DATABASE}.{EVAL_SCHEMA}.EVAL_RUN_SUMMARY\n",
    "    ORDER BY run_timestamp DESC\n",
    "    LIMIT 10\n",
    "\"\"\").to_pandas()\n",
    "\n",
    "if len(history_df) > 1:\n",
    "    st.subheader(\"Accuracy Over Time\")\n",
    "    st.line_chart(history_df.set_index('RUN_TIMESTAMP')['ACCURACY_PCT'])\n",
    "\n",
    "st.subheader(\"Recent Runs\")\n",
    "st.dataframe(history_df, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on the failures above, consider:\n",
    "\n",
    "1. **Add column descriptions** - If wrong columns are selected\n",
    "2. **Add synonyms** - If terminology doesn't match\n",
    "3. **Add sample_values** - If filter values aren't recognized\n",
    "4. **Add verified queries (VQRs)** - For complex calculations\n",
    "5. **Add relationships** - If joins are incorrect\n",
    "\n",
    "After updating your semantic model, re-run this notebook to measure improvement!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}