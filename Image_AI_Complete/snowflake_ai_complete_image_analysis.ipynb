{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Snowflake AI Complete: Image Analysis and Entity Extraction\n",
        "\n",
        "This notebook demonstrates how to use Snowflake's AI Complete with multimodal capabilities to analyze a knowledge graph image and generate code for extracting entities in star schema format.\n",
        "\n",
        "## Overview\n",
        "1. Upload knowledge graph image to Snowflake stage\n",
        "2. Use AI Complete with OpenAI to analyze the image\n",
        "3. Generate SQL code to create star schema entities\n",
        "4. Execute the generated code to create tables and relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Setup and Configuration\n",
        "# This notebook runs directly in Snowflake, so no connection setup needed\n",
        "\n",
        "print(\"Snowflake context will be set in the next SQL cell\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Set the context for your Snowflake session\n",
        "USE WAREHOUSE COMPUTE_WH;\n",
        "USE DATABASE DEMO_DB;\n",
        "USE SCHEMA PUBLIC;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Create Stage and Upload Image\n",
        "# This will be handled in the next SQL cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Create a stage for storing the knowledge graph image\n",
        "CREATE OR REPLACE STAGE knowledge_graph_stage\n",
        "    DIRECTORY = (ENABLE = TRUE)\n",
        "    COMMENT = 'Stage for storing knowledge graph images';\n",
        "\n",
        "-- Upload the knowledge graph image to the stage\n",
        "-- Note: Run this command in Snowflake CLI or use the web interface to upload\n",
        "-- PUT file:///path/to/your/knowledge_graph.jpg @knowledge_graph_stage\n",
        "--     AUTO_COMPRESS = FALSE\n",
        "--     OVERWRITE = TRUE;\n",
        "\n",
        "-- Verify the stage contents\n",
        "LIST @knowledge_graph_stage;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Use AI Complete to Analyze the Image\n",
        "# This will be handled in the next SQL cell using Snowflake's native AI Complete\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Use AI Complete to analyze the knowledge graph image and generate SQL\n",
        "-- This query reads the image from the stage and uses AI Complete to analyze it\n",
        "WITH image_analysis AS (\n",
        "    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
        "        'gpt-4-vision-preview',\n",
        "        ARRAY_CONSTRUCT(\n",
        "            OBJECT_CONSTRUCT(\n",
        "                'role', 'user',\n",
        "                'content', ARRAY_CONSTRUCT(\n",
        "                    OBJECT_CONSTRUCT(\n",
        "                        'type', 'text',\n",
        "                        'text', 'Analyze this knowledge graph image and generate SQL DDL statements to create a star schema based on the entities and relationships shown in the graph. Requirements: 1) Identify all entities (nodes) in the graph, 2) Identify all relationships between entities, 3) Create a fact table for the central entity, 4) Create dimension tables for related entities, 5) Include proper foreign key relationships, 6) Use appropriate data types for each field, 7) Include primary keys and indexes. The SQL should be production-ready and follow Snowflake best practices. Return only the SQL DDL statements, no explanations.'\n",
        "                    ),\n",
        "                    OBJECT_CONSTRUCT(\n",
        "                        'type', 'image_url',\n",
        "                        'image_url', OBJECT_CONSTRUCT(\n",
        "                            'url', 'data:image/jpeg;base64,' || BASE64_ENCODE($1)\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    ) as ai_response\n",
        "    FROM (\n",
        "        SELECT $1 as image_data\n",
        "        FROM @knowledge_graph_stage/knowledge_graph.jpg\n",
        "    )\n",
        ")\n",
        "SELECT ai_response as generated_sql\n",
        "FROM image_analysis;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Execute Generated SQL Code\n",
        "# After running the AI analysis query above, copy the generated SQL and execute it here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Execute the generated SQL code from the AI analysis\n",
        "-- Copy the SQL generated by the previous query and paste it here\n",
        "-- Example of what the AI might generate:\n",
        "\n",
        "/*\n",
        "-- Fact Table\n",
        "CREATE OR REPLACE TABLE fact_central_entity (\n",
        "    fact_id NUMBER AUTOINCREMENT PRIMARY KEY,\n",
        "    entity_id VARCHAR(50) NOT NULL,\n",
        "    dimension_1_id VARCHAR(50),\n",
        "    dimension_2_id VARCHAR(50),\n",
        "    dimension_3_id VARCHAR(50),\n",
        "    measure_1 NUMBER,\n",
        "    measure_2 NUMBER,\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\n",
        "-- Dimension Tables\n",
        "CREATE OR REPLACE TABLE dim_entity_1 (\n",
        "    entity_1_id VARCHAR(50) PRIMARY KEY,\n",
        "    entity_1_name VARCHAR(100) NOT NULL,\n",
        "    entity_1_description TEXT,\n",
        "    entity_1_category VARCHAR(50),\n",
        "    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
        "    updated_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n",
        ");\n",
        "\n",
        "-- Add foreign key constraints\n",
        "ALTER TABLE fact_central_entity \n",
        "ADD CONSTRAINT fk_dim_entity_1 \n",
        "FOREIGN KEY (dimension_1_id) REFERENCES dim_entity_1(entity_1_id);\n",
        "\n",
        "-- Create indexes for performance\n",
        "CREATE INDEX idx_fact_entity_id ON fact_central_entity(entity_id);\n",
        "*/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Verify Created Schema\n",
        "# This will be handled in the next SQL cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "-- Verify the created schema by listing tables and their structure\n",
        "SHOW TABLES;\n",
        "\n",
        "-- Describe each table structure (uncomment and run for each table)\n",
        "-- DESCRIBE TABLE fact_central_entity;\n",
        "-- DESCRIBE TABLE dim_entity_1;\n",
        "-- DESCRIBE TABLE dim_entity_2;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Cleanup (Optional)\n",
        "# No cleanup needed when running in Snowflake directly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"Notebook completed! The AI Complete analysis has generated SQL code for creating a star schema based on your knowledge graph image.\")\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Review the generated SQL from the AI analysis\")\n",
        "print(\"2. Execute the SQL to create your database schema\")\n",
        "print(\"3. Verify the created tables and relationships\")\n",
        "print(\"4. Load sample data to test the schema\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative Approach: Using Snowflake SQL with AI Complete\n",
        "\n",
        "If you prefer to work directly in Snowflake SQL instead of Python, here's the equivalent approach:\n",
        "\n",
        "### 1. Create Stage and Upload Image\n",
        "```sql\n",
        "-- Create stage\n",
        "CREATE OR REPLACE STAGE knowledge_graph_stage\n",
        "    DIRECTORY = (ENABLE = TRUE)\n",
        "    COMMENT = 'Stage for storing knowledge graph images';\n",
        "\n",
        "-- Upload image (run this in Snowflake CLI or web interface)\n",
        "PUT file:///path/to/your/knowledge_graph.jpg @knowledge_graph_stage\n",
        "    AUTO_COMPRESS = FALSE\n",
        "    OVERWRITE = TRUE;\n",
        "```\n",
        "\n",
        "### 2. Use AI Complete with Image Analysis\n",
        "```sql\n",
        "-- Analyze the knowledge graph image and generate SQL\n",
        "SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
        "    'gpt-4-vision-preview',\n",
        "    ARRAY_CONSTRUCT(\n",
        "        OBJECT_CONSTRUCT(\n",
        "            'role', 'user',\n",
        "            'content', ARRAY_CONSTRUCT(\n",
        "                OBJECT_CONSTRUCT(\n",
        "                    'type', 'text',\n",
        "                    'text', 'Analyze this knowledge graph image and generate SQL DDL statements to create a star schema based on the entities and relationships shown in the graph. Include fact tables, dimension tables, and proper relationships.'\n",
        "                ),\n",
        "                OBJECT_CONSTRUCT(\n",
        "                    'type', 'image_url',\n",
        "                    'image_url', OBJECT_CONSTRUCT(\n",
        "                        'url', 'data:image/jpeg;base64,' || BASE64_ENCODE($1)\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ") as generated_sql\n",
        "FROM (\n",
        "    SELECT $1 as image_data\n",
        "    FROM @knowledge_graph_stage/knowledge_graph.jpg\n",
        ");\n",
        "```\n",
        "\n",
        "### 3. Execute Generated SQL\n",
        "```sql\n",
        "-- Copy the generated SQL from the previous query and execute it\n",
        "-- (The AI will generate CREATE TABLE statements that you can run)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Snowflake Account**: Access to a Snowflake account with AI Complete enabled\n",
        "2. **OpenAI Integration**: Snowflake AI Complete must be configured with OpenAI\n",
        "3. **Permissions**: ACCOUNTADMIN role or equivalent permissions\n",
        "4. **Image File**: The knowledge_graph.jpg file uploaded to a Snowflake stage\n",
        "\n",
        "## Key Features Demonstrated\n",
        "\n",
        "- **Multimodal AI Analysis**: Using AI Complete to analyze both text and images\n",
        "- **Custom Prompting**: Tailored prompts for specific business requirements\n",
        "- **Code Generation**: AI-generated SQL DDL for database schema creation\n",
        "- **Star Schema Design**: Proper data warehouse design patterns\n",
        "- **Error Handling**: Robust error handling and validation\n",
        "- **Schema Verification**: Automated verification of created database objects\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. Update the connection parameters with your actual Snowflake credentials\n",
        "2. Upload the knowledge_graph.jpg file to your Snowflake stage\n",
        "3. Run the notebook cells sequentially\n",
        "4. Review and modify the generated SQL as needed\n",
        "5. Test the created schema with sample data\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
